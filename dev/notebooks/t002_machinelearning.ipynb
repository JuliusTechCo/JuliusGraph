{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 2: Machine Learning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to use this tutorial\n",
    "\n",
    "  * Select \"run all cells\" on this notebook from the Run menu in Jupyter notebook or Jupyter\n",
    "    lab. This step will produce the intermediate data output and charts. You can also run\n",
    "    individual cells by selecting that cell and pressing `Shift+Enter` key.\n",
    "  * Some cells print out a url, which you can click on and bring up an interactive web UI to\n",
    "    visualize the graph data.\n",
    "  * Time Limits: please be aware of the following time limits to conserve CPU and memory:\n",
    "    * 15 min: Some tutorials use local clusters consisting of multiple processes to mimic\n",
    "      the effects of graph distribution over a remote cluster. By default, these local clusters\n",
    "      automatically stop after idling for 15 minutes.\n",
    "      If you see an 504 gateway timeout error in the web UI, it is most likely your local cluster has stopped.\n",
    "      In that case, you will need to rerun the entire notebook by selecting \"Restart Kernel and Run All Cells\".\n",
    "    * 1 hour: notebook kernels will terminate after no user activity for 1 hour, the kernel\n",
    "      status on the right upper corner will show \"No Kernel\" afterwards. You can select\n",
    "      \"Restart Kernel and Run All Cells\" from the Kernel menu, to restart the kernel and re-run\n",
    "      the notebook.\n",
    "    * 2 hours: you will be logged out if no user activity for 2 hours. Afterwards you need\n",
    "      to login to Julius dev environment again by going to [https://juliusgraph.com](https://juliusgraph.com)\n",
    "  * Additional resources (video demos & blogs) are available at http://juliustech.co.\n",
    "  * To report bugs or request new features, please raise an issue\n",
    "    [here](https://github.com/JuliusTechCo/JuliusGraph/issues).\n",
    "    To schedule a live demo, please go to [http://juliustech.co](http://juliustech.co).\n",
    "    Please email us at info@juliustech.co for other general inquiries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "This Tutorial shows how to use the Julius Graph Engine to set up the training and validation of\n",
    "a machine learning model. We will compare several different ML models to predict (or postdict)\n",
    "the survival of Titanic passengers using the classic Titanic data set.\n",
    "\n",
    "Julius provides a `DataScience` package, which contains a rich set of\n",
    "functionalities for data sourcing, cleansing, and machine learning. In this tutorial,\n",
    "we will show how to use the `DataScience` package to quickly build a transparent and\n",
    "sophisticated ML pipeline. This tutorial broadly follows the steps of a data scientist\n",
    "when building a new ML model.\n",
    "\n",
    "## 1. Data Processing\n",
    "\n",
    "### 1.1 Data Sourcing & Visualization\n",
    "\n",
    "A data scientist usually starts their project by exploring and visualizing data of various\n",
    "sources. Julius provides a rich set of connectors to multiple data sources and formats,\n",
    "such as CSV, web url, relational databases, hadoop or other NoSQL Databases, etc. Julius\n",
    "also offers many data visualization tools in its interactive web UI.\n",
    "\n",
    "We start by including the necessary Julia and Julius packages and set up some basic\n",
    "configurations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# Julia packages\n",
    "using Base.CoreLogging\n",
    "using DataFrames, StatsBase\n",
    "\n",
    "# Julius Packages\n",
    "using GraphEngine: RuleDSL, GraphVM\n",
    "using DataScience, AtomExt, GraphIO\n",
    "\n",
    "# turn off informational logging output\n",
    "disable_logging(CoreLogging.Info)\n",
    "\n",
    "# extend the number of displayed columns in Jupyter notebooks\n",
    "ENV[\"COLUMNS\"] = 100;\n",
    "\n",
    "# the project is used for web UI display\n",
    "config = RuleDSL.Config(:project => \"Titanic\");"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset can be loaded from a url or a local CSV file via rules in the `ds`\n",
    "namespace, which is part of Julius' `DataScience` package. The line commented out is a rule\n",
    "to load the same data from a URL."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rawsrc = RuleDSL.@ref ds.csvsrc(\"../data/titanic.csv\", true; label=\"raw csv\");\n",
    "# rawsrc = RuleDSL.@ref ds.urlsrc(\"https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv\", true; label=\"raw url\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The first thing a data scientist often does is to get a summary of the dataset.\n",
    "The following cell shows how it can be done using the `ds.datasummary` rule in the `DataScience`\n",
    "package."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "rawsummary = RuleDSL.@ref ds.datasummary(rawsrc; label=\"data summary\")\n",
    "\n",
    "gs1 = GraphVM.createlocalgraph(config, RuleDSL.GenericData());\n",
    "GraphVM.calcfwd!(gs1, Set([rawsummary]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data summary results can be retrieved using the `GraphVM.getdata` method. The data\n",
    "cached in individual graph nodes are all vectors. The last argument `1` is optional, as it\n",
    "selects a given element from the data vector of the node. Without it, the entire vector\n",
    "will be returned."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs1, rawsummary, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Data Cleansing & Imputation\n",
    "\n",
    "We observe that some columns in the raw data set have `missing` values. Data imputation and\n",
    "cleansing is the next step of the workflow. Julius' `DataScience` package provides\n",
    "common data imputation methods, which can be easily invoked using the `ds.fillmissing` rule\n",
    "with the desired imputation method for each missing field, i.e., we use median value of Age\n",
    "of all passengers for any missing Ages, and the mode value (which is true) for any missing\n",
    "Embarked. The rule `ds.fillmissing` is generic, it can use any Julia method to fill in\n",
    "missing values, e.g. the `StatsBase.median` and `StatsBase.mode` below.\n",
    "\n",
    "After data imputation, we recompute the data summary, with all the `missing` values\n",
    "for both `Age` and `Embarked` features populated."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cleansrc = RuleDSL.@ref ds.fillmissing(\n",
    "    rawsrc, Dict(:Age => StatsBase.median, :Embarked => StatsBase.mode); label=\"imputation\"\n",
    ");\n",
    "\n",
    "cleansummary = RuleDSL.@ref ds.datasummary(cleansrc; label=\"clean summary\")\n",
    "GraphVM.calcfwd!(gs1, Set([cleansummary]))\n",
    "RuleDSL.getdata(gs1, cleansummary, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Feature Engineering\n",
    "\n",
    "Once the data scientist is happy with the results of the data cleansing and imputation, the next\n",
    "step is feature engineering, which is to add or remove columns from the data set.\n",
    "\n",
    "In the Titanic data set, we want to drop the columns that should have no correlation\n",
    "to a passenger's survival outcome, such as a passenger's ticket id, name and IDs. Including\n",
    "irrelevant data in the training of a ML model may degrade its performance. The\n",
    "Cabin has aslo been dropped because it has too many missing values to be useful.\n",
    "\n",
    "Here we create two additional features: 1) the z value of the ticket fare, which is the\n",
    "difference of a passenger's ticket price from the mean price in the unit of standard deviation\n",
    "of all ticket prices; 2) the total number of relatives onboard for a given passenger, which\n",
    "is the sum of the number of siblings (:SibSp) and parents/children (:Parch) onboard.\n",
    "\n",
    "Feature engineering is supported generically by a rule `ds.coltransform` in the\n",
    "`DataScience` package. The following cell shows its usage. The feature engineering can be\n",
    "easily entered as formulae operating on the columns (named by those variables start with `:`)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "newfeatures = quote\n",
    "    :Zfare = (:Fare .- mean(:Fare)) ./ std(:Fare)\n",
    "    :Relatives = :SibSp .+ :Parch\n",
    "end\n",
    "\n",
    "dropfeatures = [:Cabin, :Ticket, :PassengerId, :Name]\n",
    "features = RuleDSL.@ref ds.coltransform(cleansrc, :feature, newfeatures, dropfeatures; label=\"feature eng\")\n",
    "\n",
    "featuresummary = RuleDSL.@ref ds.datasummary(features; label=\"feature summary\")\n",
    "GraphVM.calcfwd!(gs1, Set([featuresummary]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data summary results after feature engineering is therefore:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs1, featuresummary, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entire data processing steps we performed so far can be visualized interactively\n",
    "in the Julius web UI by clicking the link below. All the intermediate data is\n",
    "accessible from the web UI."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# start data server for web UI\n",
    "gss = Dict{String,RuleDSL.AbstractGraphState}()\n",
    "port = GraphVM.drawdataport()\n",
    "@async GraphVM.startresponder(gss, port)\n",
    "\n",
    "svg = GraphIO.postlocalgraph(gss, gs1, port, true; key=\"data\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Experiment with multiple ML models\n",
    "\n",
    "Once the data scientist is happy with the results of data cleansing, imputation and\n",
    "feature engineering, the next step is to try multiple ML models and see how\n",
    "they perform on the data set.\n",
    "\n",
    "Julius Graph Engine can interop with existing Python, Java, C++ and R libraries via\n",
    "the generic `Atom` interface, making it seamless to access the rich set of ML models in\n",
    "these ecosystems.\n",
    "\n",
    "For example, the following rules leverage the Python ML libraries, such as\n",
    "`sklearn` and `xgboost`, by using the `PyTrain` atom provided in the `DataScience` package. The first\n",
    "parameter of the `PyTrain` atom is the full name of the Python ML class to use. The second\n",
    "parameter is a Dictionary with the corresponding parameters/options/arguments to that Python\n",
    "ML class.\n",
    "\n",
    "```julia\n",
    "@addrules ds begin\n",
    "    classifiertrain(model::Val{:SVC}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.svm.SVC\", options](traindat...)\n",
    "    classifiertrain(model::Val{:DecisionTree}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.tree.DecisionTreeClassifier\", options](traindat...)\n",
    "    classifiertrain(model::Val{:RandomForest}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.ensemble.RandomForestClassifier\", options](traindat...)\n",
    "    classifiertrain(model::Val{:AdaBoost}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.ensemble.AdaBoostClassifier\", options](traindat...)\n",
    "    classifiertrain(model::Val{:MLPC}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.neural_network.MLPClassifier\", options](traindat...)\n",
    "    classifiertrain(model::Val{:GaussianNB}, options::Dict, traindat::NodeRef) = PyTrain[\"sklearn.naive_bayes.GaussianNB\", options](traindat...)\n",
    "    classifiertrain(model::Val{:XGBoost}, options::Dict, traindat::NodeRef) = PyTrain[\"xgboost.XGBClassifier\", options](traindat...)\n",
    "    classifiertrain(model::Symbol, options::Dict, traindat::NodeRef; label = \"$model-train\") = Alias(classifiertrain(val(model), options, traindat))\n",
    "end\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now proceed to train multiple ML models and compare their in-sample and out-of-sample\n",
    "performance using various performance metrics, such as Gini. We first define the list of\n",
    "models we want to compare and their hyperparameters.\n",
    "\n",
    "The ML models are trained to predict the survival probability of Titanic passengers.\n",
    "The target variable name for ML prediction is also given below."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "models = [\n",
    "    :DecisionTree => Dict(:min_samples_leaf => 0.1),\n",
    "    :LogisticRegression => Dict(:solver => \"saga\", :max_iter => 200),\n",
    "    :AdaBoost => Dict(),\n",
    "    :XGBoost => Dict(),\n",
    "    :GradientBoost => Dict(:min_samples_leaf => 0.1),\n",
    "    :RandomForest => Dict(:min_samples_leaf => 0.1),\n",
    "    :GaussianNB => Dict(),\n",
    "];\n",
    "\n",
    "yname = :Survived;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "To divide the input dataset for training and validation, we use the `randrowsel` rule\n",
    "from the `DataScience` package, which randomly selects a portion of the input data as\n",
    "the validation set, while the rest is used for training. The parameter `1/3` is the fraction of\n",
    "rows that are reserved for validation."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "valind = RuleDSL.@ref ds.randrowsel(cleansrc, 1 / 3);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "`DataScience.ClassifierSpec` is a generic `struct` that holds all the configurations for\n",
    "training and validating binary classifiers, such as those we have defined so far. It is more\n",
    "convenient and readable to pass a `ClassifierSpec` object to a rule than\n",
    "having to pass five separate parameters. The `ClassifierSpec`\n",
    "can be used for any binary classifier problems on data sets. The last parameter to the\n",
    "`ClassifierSpec` constructor is a tuple representing the feature engineering."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "cspec = DataScience.ClassifierSpec(models, cleansrc, yname, valind, (:feature, newfeatures, dropfeatures));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can proceed and use the `ds.classifiermetrics` rule, which is also part of `DataScience`,\n",
    "to compute in-sample and out-of-sample metrics for each model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "metrics = [:gini, :roc, :accuracyrate, :accuracygraph]\n",
    "basem = RuleDSL.@ref ds.classifiermetrics(cspec, metrics)\n",
    "gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "@time GraphVM.calcfwd!(gs2, Set([basem]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can retrieve in-sample and out-of-sample performance metrics. For example, the GINIs:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "giniref = RuleDSL.@ref ds.classifiermetric(cspec, :gini)\n",
    "gini = GraphVM.getdata(gs2, hash(giniref), 1)\n",
    "ginidf = DataFrame(model=gini[:InSample][!, :Model], InSample_GINI=gini[:InSample][!, 2], OutSample_GINI=gini[:OutSample][!, 2])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entire data and logic can be visualized by clicking on the URL below."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs2, port, false; key=\"ml\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The entire ML pipeline includes all the steps we have defined so far, including data\n",
    "sourcing, imputation, feature engineering, training of multiple ML models and the computation\n",
    "and reporting of performance metrics. A data scientists only need to invoke a few rules\n",
    "defined in `DataScience` package to construct this realistic ML pipeline, with a total\n",
    "of 83 nodes in the graph, as shown below."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dg = GraphVM.mygraph(gs2)\n",
    "println(length(dg._items))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Hyperparameter Tuning\n",
    "\n",
    "Once a data scientist narrows down the choice of ML models to a few, the next step is\n",
    "to select the optimal hyperparameters for these candidate ML models.\n",
    "\n",
    "The Julius Graph Engine provides a generic rule `hypertune` for hyperparameter tuning\n",
    "of any ML model. This shows the power of high level rules, where a single\n",
    "hypertune rule can perform hyperparameter tuning for any ML model.\n",
    "\n",
    "For example, for a given machine learning model, we can select a range for a set of\n",
    "hyperparameters and easily perform a grid search and report the corresponding\n",
    "metric results:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ht_1 = RuleDSL.@ref ds.hypertune(cspec, :XGBoost,       Dict(), :gini, :n_estimators => 50:50:200);\n",
    "ht_2 = RuleDSL.@ref ds.hypertune(cspec, :AdaBoost,      Dict(), :gini, :n_estimators => 50:50:200);\n",
    "ht_3 = RuleDSL.@ref ds.hypertune(cspec, :GradientBoost, Dict(), :gini, :n_estimators => 50:50:200, :min_samples_leaf => .05:.05:.2);\n",
    "ht_4 = RuleDSL.@ref ds.hypertune(cspec, :RandomForest,  Dict(), :gini, :n_estimators => 50:50:200, :min_samples_leaf => .05:.05:.2);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The hypertune rule support arbitrary number of dimensions in parameters search.\n",
    "Additional search dimensions can be added to the `ds.hypertune` rule by appending extra\n",
    "pairs of hyperparameter => searchgrid to rule parameters. We can then wrap all\n",
    "the hyperparameter searches in a single node for convenience by means of an `alias` rule\n",
    "which uses the `Alias` atom:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "tunings = RuleDSL.@ref ds.alias([ht_1, ht_2, ht_3, ht_4]; label=\"Hyperparameter Tuning\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now proceed with the computation of all the defined hyperparameter tunings:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData());\n",
    "@time GraphVM.calcfwd!(gs3, Set([tunings]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell shows the resulting in-sample and out-of-sample GINI from the different\n",
    "hyperparameters for GradientBoost:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "dat = GraphVM.getdata(gs3, hash(ht_3))\n",
    "df = deepcopy(dat[1][:, 1:2])\n",
    "df[!, :InSampleGINI] = dat[1][!, 3]\n",
    "df[!, :OutSampleGINI] = dat[2][!, 3]\n",
    "df"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "A data scientist has to exercise sound judgment in selecting the optimal\n",
    "hyperparameter set, which may have to balance multiple objectives. The parameter set with\n",
    "the maximum out-of-sample GINI may not be the best choice. Often, it is better to choose the\n",
    "parameter set with similar in-sample and out-of-sample GINI to minimize the chance of\n",
    "overfitting.\n",
    "\n",
    "The details of hyperparameter search can be visualized by clicking the url below."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs3, port, false; key=\"hyper\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Conclusions\n",
    "\n",
    "It only takes a few lines of code in Julius to build a sophisticated Data and ML pipeline, by\n",
    "leveraging the existing rules and atoms provided by the `DataScience` package.  Even though\n",
    "the titanic data set is small, the ML pipeline built in this tutorial is quite\n",
    "representative; it features the essential elements of a real world ML pipepline such as data\n",
    "cleansing, imputation, feature engineering, model performance monitoring and hyper parameter\n",
    "tuning.\n",
    "\n",
    "The ML pipeline built by Julius is fully transparent, allowing data\n",
    "scientists to easily visualize and explore data in every intermediate step, all from Julius'\n",
    "web UI. Julius also offers full data lineage and explainability. A data scientist can easily\n",
    "query and trace how a piece of data is sourced, modified and used throughout the entire ML\n",
    "pipeline, making it easy to explain and audit the ML output.\n",
    "\n",
    "In the next tutorial \"distributed ML pipeline\", we will show how to deal with very large\n",
    "data sets that does not fit into memory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
