{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 3: MapReduce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to use this tutorial\n",
    "\n",
    "  * Select \"run all cells\" on this notebook from the Run menu in Jupyter notebook or Jupyter\n",
    "    lab. This step will produce intermediate data output and charts.\n",
    "  * Some cells print out a url, which you can click on and bring up an interactive web UI to\n",
    "    visualize the graph data.\n",
    "  * In the unlikely event that the notebook becomes irresponsive, you can try \"Restart\n",
    "    Kernel\" from the Kernel menu, then run individual cells one by one using `Shift+Enter`.\n",
    "  * Some tutorials use local clusters consisting of multiple processes to mimic the effects\n",
    "    of graph distribution over a remote cluster. By default, these local clusters\n",
    "    automatically stop after idling for 15 minutes to conserve CPU and memory resources. You\n",
    "    will need to rerun the entire notebook if your local cluster stopped due to inactivity.\n",
    "  * Additional resources (video demos & blogs) are available at http://juliustech.co.\n",
    "  * To report any issues, get help or request features, please raise an issue at\n",
    "    https://github.com/JuliusTechCo/JuliusGraph/issues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Introduction\n",
    "\n",
    "In this tutorial, we use Julius RuleDSL to build a generic MapReduce pipeline, and\n",
    "illustrate the benefits of Julius' high order rules.\n",
    "\n",
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a common pipeline pattern which is\n",
    "often used for processing big data sets in parallel with multiple computers/workers. The\n",
    "MapReduce pipeline is a defining feature in many popular data platforms, such as\n",
    "[Hadoop](https://hadoop.apache.org/).\n",
    "\n",
    "In this tutorial, we explain how to build a generic and re-usable MapReduce pipeline using a\n",
    "few simple rules in Julius' low-code declarative RuleDSL, as oppose to writing tons of code\n",
    "as in traditional programming languages.\n",
    "\n",
    "The MapReduce pipeline is composed of three main steps:\n",
    "- **map**: a common `map` function is applied to every input data batch.\n",
    "- **shuffle**: workers redistribute the `map` output based on certain key values such that\n",
    "  all data with the same key value is shipped to the same worker. After the shuffle, each\n",
    "  worker has the complete mapping results for its sub set of keys.\n",
    "- **reduce**: each worker then process all the mapping results for a subset of keys by\n",
    "  applying a `reduce` function. These reduced results are then collated as the final result.\n",
    "\n",
    "The following image shows the generic MapReduce steps for the problem of counting the number\n",
    "of occurrences of each word in a large collection of documents, which is given in the original\n",
    "paper of Hadoop. This word count example is often used to illustrate the MapReduce pipeline.\n",
    "We will try to replicate this example while building a generic MapReduce pipeline in Julius\n",
    "RuleDSL from scratch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../assets/mapreduce.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MapReduce input data is a collection of data batches. The creation of the data batches\n",
    "has to be done before the MapReduce pipeline (as in the Splitting stage in the diagram\n",
    "above). For example, if the original data is a single large data file, it has to be split\n",
    "into multiple batches of smaller files before feeding into the MapReduce process.\n",
    "\n",
    "The goal of the tutorial is to construct a generic `mapreduce` rule whose `mapper`,\n",
    "`shuffler` and `reducer` operators can be customized by the user. Even though the word\n",
    "count problem itself is trivial, we will implement the pipeline in a generic fashion so that\n",
    "it can be re-used without change for any MapReduce problems.\n",
    "\n",
    "The readers are referred to the quick start tutorial for basic concepts and syntax of\n",
    "`RuleDSL` and `Atom`. But for completeness, we give a brief explanations of the rule syntax\n",
    "and graph execution here.\n",
    "\n",
    "A rule in `RuleDSL` has the following syntax:\n",
    "\n",
    "```julia\n",
    "RuleDSL.@addrules namespace begin\n",
    "    rulename(rulearg1::Type1, rulearg2::Type2, ...) = begin\n",
    "        # additional code here transforming ruleargs to atom args and dependent args\n",
    "        AtomName[atomarg1, atomarg2...](deprule1(depargs1...), deprule2(depargs2...), ...)\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "The `RuleDSL.@addrules` is a macro used for processing RuleDSL, it takes a namespace\n",
    "parameter and a set of rule declarations.  The rule namespace helps organize the rules into\n",
    "related groups, and avoid name clashes. A rule in RuleDSL is an instruction to create\n",
    "certain nodes in the computational graph. When Julius GraphEngine process a rule, it creates\n",
    "a node with name `rulename`in the graph. The `AtomName[atomarg1, atomarg2...]` syntax\n",
    "defines an `Atom` object, which is used to process the data from the node's dependency.\n",
    "The dependent nodes are captured by the `deprules1, deprules` etc, they are added\n",
    "recursively to the graph, if they do not already exist.\n",
    "\n",
    "As you can appreciate, graph programming is quite different from traditional programming.\n",
    "Instead of writing imperative functions, we declare the logic and dependencies using rules,\n",
    "then let the Graph Engine to create the application or systems as computational DAGs for us.\n",
    "The benefit of graph programming will become more aparent when we go through this tutorial.\n",
    "\n",
    "The generic `mapreduce` rule should include three stages, mapper, shuffler and reducer,\n",
    "therefore it should look like:\n",
    "\n",
    "```julia\n",
    "RuleDSL.@addrules mr begin\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "        # ... rule definition goes here ...\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "The `RuleDSL.NodeRef` is a data structure that refers to another node in the graph. In Julius,\n",
    "every node is created by a specific rule, therefore a dependency on another node can also be\n",
    "understood as a dependency on another rule in the `RuleDSL`. A rule with a\n",
    "`RuleDSL.NodeRef` parameter, like the `mapreduce` rule above, is called a high order rule,\n",
    "as it defines a generic pattern whose behavior depends on other nodes (or rules). Each\n",
    "`RuleDSL.NodeRef` parameter can be a node/rule that has a complex sub-graph, thus the high\n",
    "order rule is extremely powerful in defining complex logic and behaviors.\n",
    "Furthermore, a high order rule can be passed as parameter to another rule, creating even\n",
    "higher order rules. The ability to nest high order rules is one of the reasons why the\n",
    "`RuleDSL` is both low-code and expressive. The high order rule is similar in\n",
    "spirit to the high order functions in functional programming, which we will compare to at\n",
    "the end of this tutorial.\n",
    "\n",
    "We now proceed to the implementation of the MapReduce pipeline as depicted in the diagram\n",
    "above using Julius RuleDSL.\n",
    "\n",
    "## 2. Generic Map/Reduce\n",
    "\n",
    "### 2.1 Mapping\n",
    "\n",
    "In the mapping step of the word count example, a batch of data is just a `String` such as\n",
    "`\"Mary has a lamb\"`, which is converted into a `Vector` of `Pairs`: `[\"Mary\" => 1, \"has\" =>\n",
    "1, \"a\" => 1, \"lamb\" => 1]`, where each entry represents one occurrence of a given word. At\n",
    "the shuffle stage, the vector is split by a key value, which is the word itself in the above\n",
    "diagram; then all the pairs for the same key word are sent to a single node; where they are\n",
    "concatenated to form a single vector. Finally at the reducer stage, the total occurrence of\n",
    "each key word is deduced by simply counting their occurrences in the vector.\n",
    "\n",
    "Given that the logic in the word count example is simple, we use a generic `ApplyFn` atom\n",
    "that is provided as part of the `DataScience` package, which can take any Julia function as\n",
    "argument, so that we don't have to define many Atom types for every stage of the\n",
    "MapReduce process. ApplyFn source code is listed below, which inherits from the abstract\n",
    "base type `Datom` and implements a generic method `fwddata!`, which will be called by Julius\n",
    "Graph Engine at run time to process data at individual node.\n",
    "\n",
    "```Julia\n",
    "import GraphEngine.RuleDSL: fwddata!\n",
    "\n",
    "struct ApplyFn <: RuleDSL.Datom\n",
    "    fn::Any # can be a function or any object with a callable method defined\n",
    "    params::Tuple # the first few arguments of `fn`\n",
    "\n",
    "    ## this inner constructor captures `params` as a Tuple\n",
    "    ApplyFn(fn::Any, params::Any...) = new(fn, params)\n",
    "end\n",
    "\n",
    "fwddata!(self::ApplyFn, xs::Any...) = [self.fn(self.params..., xs...)]\n",
    "```\n",
    "\n",
    "Using the generic `DataScience.ApplyFn` Atom, the `mapper` rule for word count example can\n",
    "be written as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "using DataScience: ApplyFn\n",
    "using AtomExt\n",
    "\n",
    "wordmap(words::String) = [k => 1 for k in split(words)]\n",
    "\n",
    "RuleDSL.@addrules mr begin\n",
    "    mapper(batch::RuleDSL.NodeRef, mapfun::Function) = ApplyFn[mapfun](batch...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `GraphEngine.RuleDSL` and `GraphEngine.GraphVM` modules has to be included in order to\n",
    "use the RuleDSL to create and run computational graphs.\n",
    "\n",
    "The dependency of this rule is simply given as `batch...`, which specifies that the node\n",
    "represented by the `batch` parameter is a dependency. The three dot syntax `...` is used to\n",
    "signal dynamic dependencies from a `NodeRef` parameter or variable. At run time, Julius\n",
    "GraphEngine first converts the `ApplyFn[mapfun]` specification to a call to the\n",
    "constructor of `ApplyAtom(mapfun)`. Then, the generic `fwddata!` method of the `ApplyFn`\n",
    "atom object is used to process the data from its input node specified by the `batch`\n",
    "parameter.\n",
    "\n",
    "The `mapper` rule above takes a single `RuleDSL.NodeRef` as argument, as it only applies to an\n",
    "individual batch. However, the `mapreduce` rule needs to process all the mapper\n",
    "results from all batches. So, how do we make that information available to the `mapreduce`\n",
    "rule? We could create a collection of mapper rules as `Vector{NodeRef}` then pass it into\n",
    "the `mapreduce` rule:\n",
    "\n",
    "```julia\n",
    "mappers = RuleDSL.@ref(mr.mapper(batch, mapfun) for batch in batches)\n",
    "mr = RuleDSL.@ref mr.mapreduce(batches, mappers, shufflers, reducers)\n",
    "```\n",
    "\n",
    "where the `batches` is a `Vector{RuleDSL.NodeRef}` representing the collection of input\n",
    "batches. However, this approach would require us to also create vectors of `shufflers` and\n",
    "`reducers`, thus putting too much burden on the user to ensure their consistencies. By\n",
    "observing that the first argument of the `mapper` rule is its input data batch, and the same\n",
    "mapper rule should be applied to all batch inputs, we instead choose to drop the first\n",
    "argument in the mapper rule before passing it as an argument to the `mapreduce` rule, such\n",
    "that:\n",
    "\n",
    "```julia\n",
    "mapper = RuleDSL.@ref mr.mapper(mapfun)\n",
    "mr = RuleDSL.@ref mr.mapreduce(batches, mapper, shuffler, reducer)\n",
    "```\n",
    "\n",
    "Inside the `mapreduce` rule, the first argument is added back for every data batch using the\n",
    "following `prepend` function, to recover the full form of the `mapper` rule:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prepend(ref::RuleDSL.NodeRef, firstarg::Any) = RuleDSL.NodeRef(ref.ns, ref.name, (firstarg, ref.params...), ref.meta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The advantages of dynamically inserting the first parameter in the `mapreduce` rule are the\n",
    "following:\n",
    "- First it is more readable and clear in that we only need the overall rule logic, but not\n",
    "  its first argument that specifies its batch input.\n",
    "- Secondly it is less error prone, as the mappers are created inside the `mapreduce` rule to\n",
    "  be fully consistent with the batches. We will do the same for `shuffler` and `reducer`\n",
    "  later.\n",
    "\n",
    "Let's test our mapping rule to see how it works. We have to define the input data batches\n",
    "first. For this word count example, we can simply use the `ApplyFn` atom with the\n",
    "`identity` function to return a rule argument, such that:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "    batch(s::Any) = ApplyFn[identity, s]()\n",
    "end\n",
    "\n",
    "# some input data\n",
    "_sentences = (\"Deer Bear River\", \"Car Car River\", \"Deer Car Bear\")\n",
    "_batches   = RuleDSL.@ref(mr.batch(s) for s in _sentences)\n",
    "_mapper    = RuleDSL.@ref mr.mapper(wordmap)\n",
    "\n",
    "# prepend returns a new `NodeRef` such that `mappers` is of `Vector{NodeRef}` type\n",
    "_mappers = [prepend(_mapper, batch) for batch in _batches]\n",
    "\n",
    "# create a local graph, provide the node references and calculate\n",
    "config = RuleDSL.Config()\n",
    "gs1 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs1, Set(_mappers))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now created a computational graph for the mapper and executed it. How do we see the\n",
    "results? Julius provides an easy-to-use web UI for users to navigate and visualize the\n",
    "resulting data and logic in the graph. The following code block starts a local server so\n",
    "that the web UI can retrieve the resulting graph data, it also overrides the\n",
    "`RuleDSL.nodelabel` method to customize the information displayed on the graph node."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphIO\n",
    "\n",
    "# a container of graphs\n",
    "gss = Dict{String,RuleDSL.AbstractGraphState}()\n",
    "\n",
    "# used for WebUI display purposes\n",
    "port = GraphVM.drawdataport()\n",
    "@async GraphVM.startresponder(gss, port);\n",
    "\n",
    "# override node label display\n",
    "import GraphEngine.RuleDSL: nodelabel\n",
    "function nodelabel(gs::RuleDSL.AbstractGraphState, ref::RuleDSL.NodeRef)\n",
    "    shortrepr(x::Vector; sep=\", \") = \"[\"*join(shortrepr.(x), sep)*\"]\"\n",
    "    shortrepr(p::Pair; sep=\"=>\") = shortrepr(p.first) * sep * shortrepr(p.second)\n",
    "    shortrepr(p::Dict; sep=\", \") = \"{\" * join(shortrepr.(collect(p)), sep) * \"}\"\n",
    "    shortrepr(x::Any; sep=\"\") = repr(x)\n",
    "\n",
    "    label = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "\n",
    "    try\n",
    "        data = RuleDSL.getdata(gs, ref)\n",
    "        if isone(length(data))\n",
    "            data = first(data)\n",
    "        end\n",
    "        label *= \"\\n\" * shortrepr(data; sep = \"\\n\")\n",
    "    catch\n",
    "        label *= \": n/a\"\n",
    "    end\n",
    "\n",
    "    return label\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "User can view the resulting data from exeucting the graph interactively by clicking on the\n",
    "url below to bring up the full web UI. As expected, the output of mapper is a vector of\n",
    "entries like `\"word\" => 1`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs1, port; key=\"map\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Shuffling\n",
    "\n",
    "The shuffling step consists of three sub steps:\n",
    "1. take the outputs from the `mappers` and split them into multiple chunks by certain key\n",
    "   values computed from the mapped data.\n",
    "2. move these chunks around so that all data with the same key value is shipped to the same\n",
    "   node.\n",
    "3. concatenate all the chunks associated with a given key at the containing node to recover\n",
    "   the full collection of data for the key.\n",
    "\n",
    "To implement the first sub step of the shuffling, we define a generic split function that\n",
    "takes a key function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# given a collection of elements `xs` and a key function that computes the key of each of\n",
    "# these elements, return a Dictionary of `key => x`\n",
    "function splitbykey(keyfunc::Function, xs::Any)\n",
    "    splits = Dict()\n",
    "    for x in xs\n",
    "        key = keyfunc(x)\n",
    "        splits[key] = push!(get(splits, key, []), x)\n",
    "    end\n",
    "    return splits\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this split function, we define three rules that corresponds to the three sub steps of\n",
    "shuffling, then combine them together in the generic shuffler rule:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "\n",
    "    # use `splitbykey` function defined above\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function) = ApplyFn[splitbykey, keyfunc](mapper...)\n",
    "\n",
    "    # select an element of a dictionary ir exists or return an empty `Vector{Any}`\n",
    "    selectkey(dict::RuleDSL.NodeRef, key::Any; label=\"selectby $(key)\") = ApplyFn[dict -> get(dict, key, [])](dict...)\n",
    "\n",
    "    # merge\n",
    "    mergebykey(vecs::Vector{RuleDSL.NodeRef}) = ApplyFn[vcat](vecs...)\n",
    "\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, keys::Set) = begin\n",
    "\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc) for mapper in mappers)\n",
    "\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in keys\n",
    "            # a `Vector{NodeRef}` that encompasses nodes with a given key\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "\n",
    "            # merge the previously selected nodes outputs\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "\n",
    "            # add merged element to the shuffled `Vector`\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These rules are quite self explanatory as all of them use the generic atom `ApplyFn`.\n",
    "It is worth mentioning that the `selectkey` rule uses a function closure when constructing\n",
    "the `ApplyFn` atom; and in the `mergebykey` rule, the  `...` follows a `Vector{NodeRef}`\n",
    "to specify dynamic dependency on multiple rules in the vector. The `label` keyword in the\n",
    " `selectkey` rule is to customize the display information of individual nodes in the graph\n",
    " web UI, please refer to the `nodelabel` function defined at the beginning.\n",
    "\n",
    "We can test the shuffler using the words in the text as the split key, the `first` in the\n",
    "shuffler rule is a function that returns the first element of the `\"word\"=>1` pair, which is\n",
    "the word itself."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# _mappers were created before\n",
    "_shuffler = RuleDSL.@ref mr.shuffler(_mappers, first, Set([\"Bear\", \"Car\", \"Deer\", \"River\"]))\n",
    "\n",
    "gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs2, Set([_shuffler]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs2, port; key=\"mappers\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Reducing\n",
    "\n",
    "Finally, we get to the *reduce* part of the MapReduce pipeline. In the word count example,\n",
    "the `reducer` simply counts the occurrences of word. The `reducer` rule is applied to the\n",
    "result of `mergebykey`, i.e. a vector of entries like `\"word\" => 1`, which are all\n",
    "associated with the same key value from shuffling stage."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules mr begin\n",
    "    reducer(shuffled::RuleDSL.NodeRef, reducefun::Function) = ApplyFn[reducefun](shuffled...)\n",
    "end\n",
    "\n",
    "# the reducer function\n",
    "function wordreduce(xs::Vector)\n",
    "    count = Dict()\n",
    "    for (key, _) in xs\n",
    "        count[key] = get(count, key, 0) + 1\n",
    "    end\n",
    "    return count\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Map/Reduce Rule\n",
    "\n",
    "We now put everything together and write a generic `mapreduce` rule. Note that we will use\n",
    "the same `prepend` function to dynamically insert the first argument the for `shuffler` and\n",
    "`mapper` rules:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules mr begin\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "\n",
    "        # create one mapper node per batch\n",
    "        mappers = [prepend(mapper, batch) for batch in batches]\n",
    "\n",
    "        # create the shuffler\n",
    "        shuffler = prepend(shuffler, mappers)\n",
    "\n",
    "        # this gives the inputs to the shuffled nodes, which is where reducer must be applied\n",
    "        shuffled = RuleDSL.calcdeps(RuleDSL.@config, shuffler)\n",
    "        reducers = [prepend(reducer, m) for m in shuffled]\n",
    "\n",
    "        # finally the results (i.e. a Dict per reducer) are merged to a single Dictionary\n",
    "        ApplyFn[merge](reducers...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the MapReduce rule using our word count example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# no need for the first argument as it will be populated at `mapreduce`\n",
    "_shuffler = RuleDSL.@ref mr.shuffler(first, Set([\"Bear\", \"Car\", \"Deer\", \"River\"]))\n",
    "\n",
    "_mapper  = RuleDSL.@ref mr.mapper(wordmap)\n",
    "_reducer = RuleDSL.@ref mr.reducer(wordreduce)\n",
    "\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(_batches, _mapper, _shuffler, _reducer)\n",
    "\n",
    "gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs3, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs3, port; key=\"mapred\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting diagram from Julius web UI is quite self explanatory, it matches exactly the\n",
    "diagram provided by the Hadoop paper. A side benefit of Julius is that it frees developers\n",
    "from the pain of having to manually draw system diagram or UMLs ever again. The graph\n",
    "diagram above is an output from the Julius Graph Engine, which shows great details of both\n",
    "the data and logic. Julius' convenient Web UI allows users to easily navigate and access the\n",
    "entire graph data and logic, which can be accessed by clicking the link above if you are\n",
    "running this example in Jupyter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Split by Hashed Keys\n",
    "\n",
    "So far, our MapReduce implementation works as expected. However, there is a serious\n",
    "shortcoming in that we have to specify all the possible words in the shuffler, which is not\n",
    "known before we process all the input batches. In practice, we don't want to scan all the\n",
    "input batches just to find out all the possible words, which can be very time consuming when\n",
    "the inputs are large. Also, in live streaming applications, such pre-scan is not possible at\n",
    "all.\n",
    "\n",
    "It would be much more convenient if we don't have to specify all the possible words in the\n",
    "shuffler. We can easily achieve this by supplying a different key function whose number of\n",
    "possible outputs are known, for example, by making use of the `hash` and the remainder `%`\n",
    "functions:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_shuffler = RuleDSL.@ref mr.shuffler(x -> Int(hash(first(x)) % 3), Set(collect(0:2)))\n",
    "\n",
    "# reuse the same _mapper and _reducer declared earlier\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(_batches, _mapper, _shuffler, _reducer)\n",
    "\n",
    "gs4 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs4, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs4, port; key=\"hash\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the shuffler splits the mapper data into 3 pipes, each of which is identified by\n",
    "an index number. In this implementation, multiple words can go to the same pipe. This\n",
    "implementation removes the need of pre-scans for obtaining all the words; it also works for\n",
    "live streaming use cases. Since the splitting by hash key is a much better implementation,\n",
    "we declare a couple convenience rules to encourage its use:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function, N::Int) = begin\n",
    "        ApplyFn[splitbykey, x -> Int(hash(keyfunc(x)) % N)](mapper...)\n",
    "    end\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, N::Int) = begin\n",
    "\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc, N) for mapper in mappers)\n",
    "\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in 0:N-1\n",
    "\n",
    "            # a `Vector{NodeRef}` that encompasses nodes with a given key\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "\n",
    "            # merge the previously selected nodes outputs\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "\n",
    "            # add merged element to the shuffled `Vector`\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the shuffler declaration can be simply given as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_shuffler = RuleDSL.@ref mr.shuffler(first, 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "which becomes much easier to read and define than its equivalent earlier version of\n",
    "`_shuffler`. Note that since rules support polymorphism, the hash version of `splitbykey`\n",
    "rule will be used if an interger is supplied as its 3rd argument.\n",
    "\n",
    "So far we have demonstrated the MapReduce pipeline can be implemented using RuleDSL by\n",
    "simply declaring a few high order rules. The resulting MapReduce rule is generic, powerful\n",
    "and reusable. Next, we will use it to solve a few common MapReduce problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Examples of MapReduce\n",
    "\n",
    "### 3.1 Finding Friends\n",
    "\n",
    "We can use the MapReduce pipeline to compute the common friends among hundred of millions or\n",
    "more users in a social network. This feature can be used to populate the *You and Joe have N\n",
    "friends in common* displayed in many social networks. Given the list of friends for each\n",
    "user, we proceed to define both a `mapper` and a `reducer` functions and make use of our\n",
    "previously defined `mapreduce` rule to compute common friends for every user pair $\\left(\n",
    "u_i, u_j \\right)$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function friends_mapfun(batch::String)\n",
    "    dict = Dict{NTuple{2,Char},Vector}()\n",
    "    handler = strip.(split(batch, \"=>\"))\n",
    "\n",
    "    # no friends\n",
    "    if isone(length(handler))\n",
    "        return [dict]\n",
    "    elseif length(handler) > 2\n",
    "        return error(\"Unexpected data format.\")\n",
    "    end\n",
    "\n",
    "    user, friends = handler\n",
    "\n",
    "    # no friends\n",
    "    if isempty(friends)\n",
    "        return dict\n",
    "    end\n",
    "\n",
    "    uid = only(user)\n",
    "    fids = only.(split(friends, ','))\n",
    "    for fid in fids\n",
    "        if isequal(uid, fid)\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        key = tuple(sort!([uid, fid])...)\n",
    "        push!(dict, key => fids)\n",
    "    end\n",
    "\n",
    "    return dict\n",
    "end\n",
    "\n",
    "function friends_reducefun(shuffler::Vector)\n",
    "    out = Dict{NTuple{2,Char},Vector{Char}}()\n",
    "    for (k, v) in shuffler\n",
    "        if !haskey(out, k)\n",
    "            out[k] = v\n",
    "        else\n",
    "            out[k] = intersect(out[k], v)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# each user is represented by a `Char`\n",
    "_friends = IOBuffer(\"\n",
    "    A => B,C,D\n",
    "    B => A,C,D,E\n",
    "    C => A,B,D,E\n",
    "    D => A,B,C,E\n",
    "    E => B,C,D\n",
    "\")\n",
    "\n",
    "_batches = RuleDSL.@ref(mr.batch(line) for line in eachline(_friends) if !isempty(line))\n",
    "\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(friends_mapfun)),\n",
    "    RuleDSL.@ref(mr.shuffler(first, 4)),\n",
    "    RuleDSL.@ref(mr.reducer(friends_reducefun))\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs5 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs5, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs5, port; key=\"ff\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 GroupBy\n",
    "\n",
    "When dealing with large data set, we often need to split it to smaller batches, then apply\n",
    "the MapReduce pipeline to perform certain operations on individual batches then group\n",
    "them together. In this section, we will show how to implement the `groupby` operation\n",
    "on a large data set using the MapReduce pipeline.\n",
    "\n",
    "In order to split the data in multiple batches, we make use of our `DDataFrame` (which stands\n",
    "for Distributed DataFrames) provided in the `DataScience` package. The following `mapper`\n",
    "and `reducer` rules implements the group by using any number of features within the\n",
    "`MapReduce` pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DataFrames\n",
    "using DataScience: DDataFrame\n",
    "\n",
    "# `cols` can be anything accepted by `DataFrames.groupby` method\n",
    "function groupby_mapfun(batch::AbstractDataFrame, cols)\n",
    "    dict = Dict()\n",
    "    gdf = groupby(batch, cols)\n",
    "    for (key, df) in zip(keys(gdf), gdf)\n",
    "        push!(dict, NamedTuple(key) => DataFrame(df; copycols=false))\n",
    "    end\n",
    "    return dict\n",
    "end\n",
    "\n",
    "function groupby_reducefun(shuffler::Vector)\n",
    "    out = Dict()\n",
    "    for (k, v) in shuffler\n",
    "        out[k] = append!(get(out, k, DataFrame()), v)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "filepath = joinpath(@__DIR__, \"../data/iris.csv\")\n",
    "ddf = DDataFrame(filepath, nrows=25)\n",
    "_batches = ddf.chunks\n",
    "\n",
    "# use 3 reducing nodes for the reducing step\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(x -> groupby_mapfun(x, [:Species]))),\n",
    "    RuleDSL.@ref(mr.shuffler(first, 3)),\n",
    "    RuleDSL.@ref(mr.reducer(groupby_reducefun))\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs6 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs6, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nodelabel(::AbstractGraphState, ref::NodeRef) = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "svg = GraphIO.postlocalgraph(gss, gs6, port; key=\"groupby\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result is a `DataFrame` per group, such that, the first 10 rows look like:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_reducers = calcdeps(config, _mapreduce)\n",
    "for reducer in _reducers\n",
    "    dict = RuleDSL.getdata(gs6, reducer)[]\n",
    "    for (k, v) in dict\n",
    "      println(\"$k => $(first(v, 10))\")\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These previous examples are relatively straight forward in their logic. However, the\n",
    "`mapper` and `reducer` rules can encapsulate complicated logic, both of them can be an\n",
    "entire graph of great complexity. For example, the mapper can be the training and validation\n",
    "of an entire ML model, and the reducer can be a bagging algorithm that joins multiple models\n",
    "trained on different batches of data. We will show an example of a more complex use case in\n",
    "the next tutorial.\n",
    "\n",
    "## 4. Advantages of Julius Graph\n",
    "\n",
    "### 4.1 Graph Composition vs Function Composition\n",
    "\n",
    "You may find the high level rules in `RuleDSL` have a lot similarity to high order functions\n",
    "in functional languages like Haskell, where a function can take another function as parameter.\n",
    "So what are the main benefits of high order rule over the high order functions in a functional\n",
    "language?\n",
    "\n",
    "The key difference is that high level rules are for composing graphs, while high level\n",
    "functions are for composing functions. The graph composition has a number of advantages over\n",
    "function composition:\n",
    "\n",
    "1. It does not create deep call stacks, the results of graph composition is nothing but\n",
    "   another graph. Therefore, it is much easier for a developer to visualize and debug. With\n",
    "   function composition, one has to use a debugger to access the intermediate results and\n",
    "   call sequences, deep among the call stack of a program's runtime.\n",
    "2. The resulting graph composition can be automatically distributed without code change. A\n",
    "   generic graph distributor can analyze any graph and distribute it effectively to multiple\n",
    "   worker computers. In contrast, the traditional functional code is permeated with loops\n",
    "   and branches, making their runtime behavior unpredictable, thus cannot be distributed\n",
    "   automatically and efficiently.\n",
    "3. The graph composition is much more flexible. Once the graph is constructed, it can run in\n",
    "   different mode. For example, the same graph can support both batch and streaming use\n",
    "   cases without code changes, which is not possible in traditional functional programming.\n",
    "4. Lastly, graph composition can mimic function composition, but the reverse is not true.\n",
    "   The `mapreduce` rule is a good example how function composition can be replicated using\n",
    "   graph composition. However, it is almost impossible to create the equivalent graph\n",
    "   composition from function composition in traditional functional languages.\n",
    "\n",
    "You have seen some of the benefits graph composition in this and previous tutorials. Next,\n",
    "we will illustrate the 2nd benefit of automatically distributing the MapReduce pipeline to\n",
    "multiple computers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Distributed Map/Reduce\n",
    "\n",
    "In order to show the automatic distribution, we set up a local cluster with 3 worker processes\n",
    "managed by a master process running at a port of the local computer, which mimic a remote\n",
    "master and worker processes running on multiple physical computers. Please note that the\n",
    "local cluster automatically terminates after 15min of inactivity, so if you noticed the\n",
    "local cluster no longer accessible after 15min, please re-run this entire tutorial notebook.\n",
    "\n",
    "The following few lines of code starts the local cluster then connects to the master process,\n",
    "through which we gain control to all the worker processes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "\n",
    "config = RuleDSL.newconfig(RuleDSL.Config(), :project => \"MapReduce\")\n",
    "balancer = GraphVM.GlobalUnique()\n",
    "my_domain = GraphVM.mydomain()\n",
    "\n",
    "# draw a port number to start the local cluster esrvice\n",
    "remoteport = GraphVM.drawdataport()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# start a local master service at the given port\n",
    "\n",
    "GraphVM.startlocalmasterservice(remoteport, 3)\n",
    "\n",
    "gs = GraphVM.RemoteGraphProxy(config, my_domain => remoteport, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is the complete definition of the generic `mapreduce` rule and corresponding\n",
    "functions for the word count example. Now we instantiate them in the remote cluster so that\n",
    "we can run the distributed word count with distribution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs begin\n",
    "    using GraphEngine: RuleDSL, GraphVM\n",
    "    using DataScience: ApplyFn\n",
    "    using AtomExt, GraphIO\n",
    "end\n",
    "\n",
    "# wait for the server to complete the task before proceeding\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs mr begin\n",
    "    echo(x::Any) = ApplyFn[identity, x]()\n",
    "\n",
    "    mapper(batch::RuleDSL.NodeRef, mapfun::Function) = ApplyFn[mapfun](batch...)\n",
    "\n",
    "    reducer(shuffled::RuleDSL.NodeRef, reducefun::Function) = ApplyFn[reducefun](shuffled...)\n",
    "\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function) = ApplyFn[splitbykey, keyfunc](mapper...)\n",
    "\n",
    "    selectkey(dict::RuleDSL.NodeRef, key::Any; label=\"selectby $(key)\") = ApplyFn[dict -> get(dict, key, [])](dict...)\n",
    "\n",
    "    mergebykey(vecs::Vector{RuleDSL.NodeRef}) = ApplyFn[vcat](vecs...)\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, keys::Set) = begin\n",
    "        merged = Vector{NodeRef}()\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc) for mapper in mappers)\n",
    "        for key in keys\n",
    "            values = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "            mergebykey = RuleDSL.@ref mergebykey(values; label=\"mergeby $key\")\n",
    "            push!(merged, mergebykey)\n",
    "        end\n",
    "        Alias(merged...)\n",
    "    end\n",
    "\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "        mappers = [prepend(mapper, batch) for batch in batches]\n",
    "        shuffler = prepend(shuffler, mappers)\n",
    "        shuffled = RuleDSL.calcdeps(RuleDSL.@config, shuffler)\n",
    "        reducers = [prepend(reducer, m) for m in shuffled]\n",
    "        ApplyFn[merge](reducers...)\n",
    "    end\n",
    "\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function, N::Int) = begin\n",
    "        ApplyFn[splitbykey, x -> Int(hash(keyfunc(x)) % N)](mapper...)\n",
    "    end\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, N::Int) = begin\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc, N) for mapper in mappers)\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in 0:N-1\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "end\n",
    "\n",
    "GraphVM.@remote_eval gs begin\n",
    "    import GraphEngine.RuleDSL: fwddata!\n",
    "\n",
    "    prepend(ref::RuleDSL.NodeRef, firstarg::Any) = RuleDSL.NodeRef(ref.ns, ref.name, (firstarg, ref.params...), ref.meta)\n",
    "\n",
    "    function splitbykey(keyfunc::Function, xs::Any)\n",
    "        splits = Dict()\n",
    "        for x in xs\n",
    "            key = keyfunc(x)\n",
    "            splits[key] = push!(get(splits, key, []), x)\n",
    "        end\n",
    "        return splits\n",
    "    end\n",
    "\n",
    "    wordmap(words::String) = [k => 1 for k in split(words)]\n",
    "\n",
    "    function wordreduce(xs::Vector)\n",
    "        count = Dict()\n",
    "        for (key, _) in xs\n",
    "            count[key] = get(count, key, 0) + 1\n",
    "        end\n",
    "        return count\n",
    "    end\n",
    "\n",
    "    import GraphEngine.RuleDSL: nodelabel\n",
    "    function nodelabel(gs::RuleDSL.AbstractGraphState, ref::RuleDSL.NodeRef)\n",
    "        shortrepr(x::Vector; sep=\", \") = \"[\"*join(shortrepr.(x), sep)*\"]\"\n",
    "        shortrepr(p::Pair; sep=\"=>\") = shortrepr(p.first) * sep * shortrepr(p.second)\n",
    "        shortrepr(p::Dict; sep=\", \") = \"{\" * join(shortrepr.(collect(p)), sep) * \"}\"\n",
    "        shortrepr(x::Any; sep=\"\") = repr(x)\n",
    "\n",
    "        label = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "\n",
    "        try\n",
    "            data = RuleDSL.getdata(gs, ref)\n",
    "            if isone(length(data))\n",
    "                data = first(data)\n",
    "            end\n",
    "            label *= \"\\n\" * shortrepr(data; sep = \"\\n\")\n",
    "        catch\n",
    "            label *= \": n/a\"\n",
    "        end\n",
    "\n",
    "        return label\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, there is no change in the RuleDSL and Julia functions at all, we simply sent\n",
    "them to the remote cluster to instantiate. Afterwards, we can execute the MapReduce pipeline\n",
    "with distribution. The distribution API, `RuleDSL.jobdeps` and `GraphVM.dispatchjobs!` are\n",
    "explained in more detail in the next tutorial on Distributed ML pipeline, we won't\n",
    "repeat them here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_sentences = (\"Deer Bear River\", \"Car Car River\", \"Deer Car Bear\")\n",
    "_batches   = RuleDSL.@ref(mr.echo(s) for s in _sentences)\n",
    "\n",
    "N = 3\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(Main.wordmap)),\n",
    "    RuleDSL.@ref(mr.shuffler(first, N)),\n",
    "    RuleDSL.@ref(mr.reducer(Main.wordreduce))\n",
    ")\n",
    "\n",
    "alljobs, ds = RuleDSL.jobdeps(config, [_mapreduce], Set([:splitbykey, :reducer]));\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "GraphVM.initgraph!(gs)\n",
    "GraphVM.dispatchjobs!(gs, alljobs, 1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "svg = GraphIO.postremotegraph(gs, port);\n",
    "\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting graph use different colors to highlight the placement of nodes to individual\n",
    "remote workers. Nodes with the same color are placed and computed on the same worker\n",
    "computer.\n",
    "\n",
    "Upon close examination, we observe that the resulting graph distribution is optimal in\n",
    "that the work load is evenly distributed amongst 3 workers, and the only shipment of data\n",
    "happens during the shuffling stage, and the collation of final reducer results, when an\n",
    "arrow connects two nodes with different colors. There is no unnecessary data transfer at all\n",
    "in the resulting graph distribution.\n",
    "\n",
    "The ability to automatically and optimally distribute graph without code change is a\n",
    "powerful feature. Julius can handle the distribute of graphs as large as\n",
    "hundreds millions nodes across hundreds of computers. Using Julius, the same code run\n",
    "efficiently on one worker instance or hundreds of worker instances, without the need for any\n",
    "manual tweaking and optimizations. Auto-scaling allows developers to quickly build and test\n",
    "their rules and functions on the local computer, then immediately scale it up to run large\n",
    "jobs and heavy workload in parallel without the need for any code changes. Julius'\n",
    "autoscaling automates away one of the most time-consuming and expensive aspect in enterprise\n",
    "systems, which is the constant needs for manual tweaking and optimization of the system for\n",
    "better performance and scalability.\n",
    "\n",
    "In a next toturial Distributed ML pipeline, we will dive into Julius distribution\n",
    "and auto-scaling capability in much more depth, and compare to existing tools like Dask and\n",
    "Dagger.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The result speaks for itself: we built a generic MapReduce pipeline from scratch using 10\n",
    "rules in RuleDSL and 20 lines of additional Julia code. The resulting MapReduce pipeline\n",
    "implementation is generic, transparent and auto-scaling. Every intermediate calculation\n",
    "result is fully visible to the user in our web UI, it automatically distributes to multiple\n",
    "computers without any code changes for extreme scalability and performance.\n",
    "\n",
    "Intrigued? If you are a developer, you should. To hear more about Julius Graph Engine,\n",
    "contact us at `info@juliustech.co`, or go to our [website](http://juliustech.co) to schedule\n",
    "a demo or sign up for free access for developers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix: Additional Technical Tips & Notes\n",
    "\n",
    "Here we explain some additional technical tips and points. We refer to the general structure\n",
    "of a rule in RuleDSL as:\n",
    "\n",
    "```julia\n",
    "rulename(ruleargs::Any...) = Atom[atomargs...](dependentrule(depargs...))\n",
    "```\n",
    "\n",
    "* The `ApplyFn` atom is used extensively in this tutorial. Though it is convenient, it was\n",
    "  only intended for simple analytical logic. For complex analytical algorithms, it is better\n",
    "  to define individual Atoms for reusability and readability.\n",
    "* There is an important difference between the `ruleargs` and `atomargs` in the rule syntax.\n",
    "  The `ruleargs` is serialized and sent to remote worker during distribution, while atomargs\n",
    "  are only created and used locally by individual workers. Therefore to enable distribution,\n",
    "  every `ruleargs` has to be serializable with a stable hash, i.e. same object shall\n",
    "  retrieve the same hash regardless which worker runs it. This requirement does not apply to\n",
    "  atomargs. Julius use a customized hash function `RuleDSL.hashparam` for rule parameters,\n",
    "  that support stable hashes for a wide variety of object types. However, the following are\n",
    "  some instances where the serialization can fail or the hash becomes unstable:\n",
    "  * If a rule parameter is a function closure with reference to any local variable, the\n",
    "    serialization will fail.  A workaround is to move the function closure inside the body\n",
    "    of the rule. For example, the first hash key shuffler will fail:\n",
    "  ```julia\n",
    "  # fail to serialize: local variable used in function closure\n",
    "  N = 3\n",
    "  _shuffler = RuleDSL.@ref mr.shuffler(x -> Int(hash(first(x)) % N), Set(collect(0:N-1)))\n",
    "  ```\n",
    "  but the second version of the shuffler will work fine:\n",
    "  ```julia\n",
    "  # closure moved inside the rule declaration\n",
    "  _shuffler = RuleDSL.@ref mr.shuffle(first, N)\n",
    "  ```\n",
    "  * A complex struct is more likely to have unstable hashes, you can either make it inherit\n",
    "    from `RuleDSL.ValueType` thus using the more stable `RuleDSL.hashparam`, or you can\n",
    "    provide your own stable hash function by overriding the `RuleDSL.hashparam` method for\n",
    "    the type in question. To help detect the potential serialization and hash stability\n",
    "    issues in rules, we provide a convenient macro `RuleDSL.@isdistributable`, which will\n",
    "    flag any node in a graph that cannot be safely distributed.\n",
    "* You may be tempted to define a `mapreduce` rule that takes a mapper function and a reducer\n",
    "  function, and create the `RuleDSL.@ref mr.mapper(func)` and `RuleDSL.@ref\n",
    "  mr.reducer(func)` inside the `mapreduce` rule. As discussed before, this is less generic\n",
    "  as the `mapper` and `reducer` rule shall not be restricted to simple wrappers like\n",
    "  `RuleDSL.@ref mr.mapper(func)`. Instead, any rule can be used as the `mapper` and\n",
    "  `reducer`, for example it could  represent a complex graph with sophisticated logic. As a\n",
    "  matter of fact, they could very well be high order rules themselves."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
