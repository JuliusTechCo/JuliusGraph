{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 3: MapReduce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to use this tutorial\n",
    "\n",
    "  * Select \"run all cells\" on this notebook from the Run menu in Jupyter notebook or Jupyter\n",
    "    lab. This step will produce the intermediate data output and charts. You can also run\n",
    "    individual cells by selecting that cell and pressing `Shift+Enter` key.\n",
    "  * Some cells print out a url, which you can click on and bring up an interactive web UI to\n",
    "    visualize the graph data.\n",
    "  * Time Limits: please be aware of the following time limits to conserve CPU and memory:\n",
    "    * 15 min: Some tutorials use local clusters consisting of multiple processes to mimic\n",
    "      the effects of graph distribution over a remote cluster. By default, these local clusters\n",
    "      automatically stop after idling for 15 minutes.\n",
    "      If you see an 504 gateway timeout error in the web UI, it is most likely your local cluster has stopped.\n",
    "      In that case, you will need to rerun the entire notebook by selecting \"Restart Kernel and Run All Cells\".\n",
    "    * 1 hour: notebook kernels will terminate after no user activity for 1 hour, the kernel\n",
    "      status on the right upper corner will show \"No Kernel\" afterwards. You can select\n",
    "      \"Restart Kernel and Run All Cells\" from the Kernel menu, to restart the kernel and re-run\n",
    "      the notebook.\n",
    "    * 2 hours: you will be logged out if no user activity for 2 hours. Afterwards you need\n",
    "      to login to Julius dev environment again by going to [https://juliusgraph.com](https://juliusgraph.com)\n",
    "  * Additional resources (video demos & blogs) are available at http://juliustech.co.\n",
    "  * To report bugs or request new features, please raise an issue\n",
    "    [here](https://github.com/JuliusTechCo/JuliusGraph/issues).\n",
    "    To schedule a live demo, please go to [http://juliustech.co](http://juliustech.co).\n",
    "    Please email us at info@juliustech.co for other general inquiries."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we use Julius RuleDSL to build a generic MapReduce pipeline, and\n",
    "illustrate the benefits of Julius' high order rules.\n",
    "\n",
    "[MapReduce](https://en.wikipedia.org/wiki/MapReduce) is a common pipeline pattern which is\n",
    "often used for processing big data sets in parallel with multiple computers/workers. The\n",
    "MapReduce pipeline is a defining feature in some of the most popular data platforms, such as\n",
    "[Hadoop](https://hadoop.apache.org/).\n",
    "\n",
    "In this tutorial, we explain how to build a generic and re-usable MapReduce pipeline using a\n",
    "few simple rules in Julius' low-code declarative RuleDSL, as opposed to writing excessive\n",
    "amounts of code as in traditional programming languages.\n",
    "\n",
    "The MapReduce pipeline is composed of three main steps:\n",
    "- **map**: a common `map` function is applied to every input data batch.\n",
    "- **shuffle**: workers redistribute the `map` output based on certain key values such that\n",
    "  all data with the same key value is shipped to the same worker.\n",
    "- **reduce**: each worker then processes the results for its subset of keys by\n",
    "  applying a `reduce` function. These reduced results are then collated as the final result.\n",
    "\n",
    "The following image shows the generic MapReduce steps for the problem of counting the number\n",
    "of occurrences of words in a large collection of documents, which is given in the original\n",
    "paper of Hadoop. This word count example is often used to illustrate the MapReduce pipeline.\n",
    "We will replicate this example while building a generic MapReduce pipeline in the Julius\n",
    "RuleDSL from scratch."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](../assets/mapreduce.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The MapReduce input data is a collection of data batches. The creation of data batches\n",
    "has to be done before the MapReduce pipeline (as in the Splitting stage in the diagram\n",
    "above). For example, if the original data is a single large data file, it has to be split\n",
    "into multiple batches of smaller files before feeding into the MapReduce process.\n",
    "\n",
    "The goal of this tutorial is to construct a generic `mapreduce` rule whose `mapper`,\n",
    "`shuffler` and `reducer` operators can be customized by the user. Even though the word\n",
    "count problem itself is trivial, we will implement the pipeline in a generic fashion so that\n",
    "it can be re-used for any MapReduce problems.\n",
    "\n",
    "The readers are referred to the quick start tutorial for the basic concepts and syntax of\n",
    "the `RuleDSL` and `Atom`. But for completeness, we give a brief explanation of the rule syntax\n",
    "and graph execution here.\n",
    "\n",
    "A rule in `RuleDSL` has the following syntax:\n",
    "\n",
    "```julia\n",
    "RuleDSL.@addrules namespace begin\n",
    "    rulename(rulearg1::Type1, rulearg2::Type2, ...) = begin\n",
    "        # additional code here transforming ruleargs to atom args and dependent args\n",
    "        AtomName[atomarg1, atomarg2...](deprule1(depargs1...), deprule2(depargs2...), ...)\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "The `RuleDSL.@addrules` is a macro used for processing the RuleDSL. It takes a namespace\n",
    "parameter and a set of rule declarations.  The rule namespace helps organize the rules into\n",
    "related groups, and avoid name clashes. A rule in the RuleDSL is an instruction to create\n",
    "certain nodes in the computational graph. When the Julius GraphEngine processes a rule, it\n",
    "creates a node from the rule in the computational graph, and then recursively adds the dependent\n",
    "nodes to the graph according to the dependent rules specified in `deprule1, deprule2` etc.\n",
    "The `AtomName[atomarg1, atomarg2...]` syntax defines an `Atom` object, which is used\n",
    "to process the data from the node's dependency.\n",
    "\n",
    "As you can now appreciate, graph programming is quite different from traditional programming.\n",
    "Instead of writing imperative functions, we declare the logic and dependencies using rules,\n",
    "then let the Graph Engine create the application or systems as computational DAGs for us.\n",
    "That is why the amount of code required in graph programming is far less than\n",
    "traditional programming languages, since most of the boilerplate code for the program's\n",
    "flow control is automated away.\n",
    "\n",
    "The generic `mapreduce` rule should include three stages: mapper, shuffler and reducer,\n",
    "thus it should look like:\n",
    "\n",
    "```julia\n",
    "RuleDSL.@addrules mr begin\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "        # ... rule definition goes here ...\n",
    "    end\n",
    "end\n",
    "```\n",
    "\n",
    "The `RuleDSL.NodeRef` is a data structure that refers to another node in the graph. In Julius,\n",
    "every node is created by a specific rule, so that a dependency on another node can also be\n",
    "understood as a dependency on its underlying rule. A rule with a\n",
    "`RuleDSL.NodeRef` parameter, like the `mapreduce` rule above, is called a high order rule,\n",
    "as it defines a generic pattern whose behavior depends on other rules. The high\n",
    "order rule is extremely powerful in defining abstract and high level logic and behaviors.\n",
    "Furthermore, a high order rule can be passed as parameter to another rule, creating even\n",
    "higher order rules. The ability to nest high order rules is one of the reasons why the\n",
    "`RuleDSL` is both low-code and expressive. The high order rule is similar in\n",
    "spirit to the high order functions in functional programming, which we will discuss in more\n",
    "detail at the end of this tutorial.\n",
    "\n",
    "We now proceed to implement the MapReduce pipeline as depicted in the diagram\n",
    "above using the Julius RuleDSL.\n",
    "\n",
    "## 2. Generic Map/Reduce\n",
    "\n",
    "### 2.1 Mapping\n",
    "\n",
    "In the mapping step of the word count example, a batch of data is just a `String` such as\n",
    "`\"Mary has a lamb\"`, which is converted into a `Vector` of `Pairs`: `[\"Mary\" => 1, \"has\" =>\n",
    "1, \"a\" => 1, \"lamb\" => 1]`, where each entry represents one occurrence of a given word. At\n",
    "the shuffle stage, the vector is split by a key value, which is the word itself in the above\n",
    "diagram. Then all the pairs for the same keyword are sent to a single node where they are\n",
    "concatenated to form a single vector. Finally at the reducer stage, the total occurrence of\n",
    "each key word is deduced by simply counting their occurrences in the vector.\n",
    "\n",
    "Given that the logic in the word count example is simple, we use a generic `ApplyFn` atom\n",
    "that is provided as part of the `DataScience` package, which can take any Julia function as\n",
    "an argument, so that we don't have to define many Atom types for every stage of the\n",
    "MapReduce process. The `ApplyFn` source code is listed below, which inherits from the abstract\n",
    "base type `Datom` and implements a generic method `fwddata!`, which will be called by the Julius\n",
    "Graph Engine at runtime to process data at individual nodes.\n",
    "\n",
    "```Julia\n",
    "import GraphEngine.RuleDSL: fwddata!\n",
    "\n",
    "struct ApplyFn <: RuleDSL.Datom\n",
    "    fn::Any # can be a function or any object with a callable method defined\n",
    "    params::Tuple # the first few arguments of `fn`\n",
    "\n",
    "    ## this inner constructor captures `params` as a Tuple\n",
    "    ApplyFn(fn::Any, params::Any...) = new(fn, params)\n",
    "end\n",
    "\n",
    "fwddata!(self::ApplyFn, xs::Any...) = [self.fn(self.params..., xs...)]\n",
    "```\n",
    "\n",
    "Using the generic `DataScience.ApplyFn` Atom, the `mapper` rule for word count example can\n",
    "be written as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "using DataScience: ApplyFn\n",
    "using AtomExt\n",
    "\n",
    "wordmap(words::String) = [k => 1 for k in split(words)]\n",
    "\n",
    "RuleDSL.@addrules mr begin\n",
    "    mapper(batch::RuleDSL.NodeRef, mapfun::Function) = ApplyFn[mapfun](batch...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `GraphEngine.RuleDSL` and `GraphEngine.GraphVM` modules have to be included in order to\n",
    "use the RuleDSL to create and run computational graphs.\n",
    "\n",
    "The dependency of this rule is simply given as `batch...`, which specifies that the node\n",
    "represented by the `batch` parameter is a dependency. The three dot syntax `...` is used to\n",
    "signal dynamic dependencies from a `NodeRef` parameter or variable. At runtime, the Julius\n",
    "GraphEngine first converts the `ApplyFn[mapfun]` specification to a call to the\n",
    "constructor of `ApplyFn(mapfun)`. Then, the `fwddata!` method of the `ApplyFn`\n",
    "atom object is called to process the data from its input node specified by the `batch`\n",
    "parameter, which in turn calls the underlying `mapfun` function.\n",
    "\n",
    "The `mapper` rule above takes a single `RuleDSL.NodeRef` as an argument, as it only applies\n",
    "to an individual batch. However, the `mapreduce` rule needs to process all the mapper\n",
    "results from all the batches. So, how do we make that information available to the `mapreduce`\n",
    "rule? We could create a collection of mapper rules as `Vector{NodeRef}` then pass it into\n",
    "the `mapreduce` rule:\n",
    "\n",
    "```julia\n",
    "mappers = RuleDSL.@ref(mr.mapper(batch, mapfun) for batch in batches)\n",
    "mr = RuleDSL.@ref mr.mapreduce(batches, mappers, shufflers, reducers)\n",
    "```\n",
    "\n",
    "where the `batches` is a `Vector{RuleDSL.NodeRef}` representing the collection of input\n",
    "batches. However, this approach would require us to also create vectors of `shufflers` and\n",
    "`reducers`, thus putting too much burden on the user to ensure their consistency. By\n",
    "observing that the first argument of the `mapper` rule is its input data batch and that the\n",
    "same mapper rule should be applied to all batch inputs, we instead choose to drop the first\n",
    "argument in the mapper rule before passing it as an argument to the `mapreduce` rule, such\n",
    "that:\n",
    "\n",
    "```julia\n",
    "mapper = RuleDSL.@ref mr.mapper(mapfun)\n",
    "mr = RuleDSL.@ref mr.mapreduce(batches, mapper, shuffler, reducer)\n",
    "```\n",
    "\n",
    "Inside the `mapreduce` rule, the first argument is added back for every data batch using the\n",
    "following `prepend` function, to recover the full form of the `mapper` rule:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prepend(ref::RuleDSL.NodeRef, firstarg::Any) = RuleDSL.NodeRef(ref.ns, ref.name, (firstarg, ref.params...), ref.meta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The advantages of dynamically inserting the first parameter in the `mapreduce` rule are the\n",
    "following:\n",
    "- First it is more readable and clear in that we only need the overall rule logic, but not\n",
    "  its first argument that specifies a particular batch input.\n",
    "- Secondly it is less error prone, as the mappers are created inside the `mapreduce` rule by\n",
    "  inserting the right batch as its first parameter, making it fully consistent with the\n",
    "  batch input parameter. We will apply the same trick for `shuffler` and `reducer` later.\n",
    "\n",
    "Let's test our mapping rule to see how it works. We have to define the input data batches\n",
    "first. For this word count example, we can simply use the `ApplyFn` atom with the\n",
    "`identity` function to return a rule argument, such that:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "    batch(s::Any) = ApplyFn[identity, s]()\n",
    "end\n",
    "\n",
    "# some input data\n",
    "_sentences = (\"Deer Bear River\", \"Car Car River\", \"Deer Car Bear\")\n",
    "_batches   = RuleDSL.@ref(mr.batch(s) for s in _sentences)\n",
    "_mapper    = RuleDSL.@ref mr.mapper(wordmap)\n",
    "\n",
    "# prepend returns a new `NodeRef` such that `mappers` is of `Vector{NodeRef}` type\n",
    "_mappers = [prepend(_mapper, batch) for batch in _batches]\n",
    "\n",
    "# create a local graph, provide the node references and calculate\n",
    "config = RuleDSL.Config()\n",
    "gs1 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs1, Set(_mappers));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now created a computational graph for the mapper and executed it. How do we see the\n",
    "results? Julius provides an easy-to-use web UI for users to navigate and visualize the\n",
    "resulting data and logic in the graph. The following code block starts a local server so\n",
    "that the web UI can retrieve the resulting graph data, and it also overrides the\n",
    "`RuleDSL.nodelabel` method to customize the information displayed on the graph node."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphIO\n",
    "\n",
    "# a container of graphs\n",
    "gss = Dict{String,RuleDSL.AbstractGraphState}()\n",
    "\n",
    "# used for WebUI display purposes\n",
    "port = GraphVM.drawdataport()\n",
    "@async GraphVM.startresponder(gss, port);\n",
    "\n",
    "# override node label display\n",
    "import GraphEngine.RuleDSL: nodelabel\n",
    "function nodelabel(gs::RuleDSL.AbstractGraphState, ref::RuleDSL.NodeRef)\n",
    "    shortrepr(x::Vector; sep=\", \") = \"[\"*join(shortrepr.(x), sep)*\"]\"\n",
    "    shortrepr(p::Pair; sep=\"=>\") = shortrepr(p.first) * sep * shortrepr(p.second)\n",
    "    shortrepr(p::Dict; sep=\", \") = \"{\" * join(shortrepr.(collect(p)), sep) * \"}\"\n",
    "    shortrepr(x::Any; sep=\"\") = repr(x)\n",
    "\n",
    "    label = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "\n",
    "    try\n",
    "        data = RuleDSL.getdata(gs, ref)\n",
    "        if isone(length(data))\n",
    "            data = first(data)\n",
    "        end\n",
    "        label *= \"\\n\" * shortrepr(data; sep = \"\\n\")\n",
    "    catch\n",
    "        label *= \": n/a\"\n",
    "    end\n",
    "\n",
    "    return label\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Users can interact with the resulting data from executing the graph by clicking on the\n",
    "url below to bring up the full web UI. As expected, the output of the mapper is a vector of\n",
    "entries like `\"word\" => 1`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs1, port; key=\"map\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Shuffling\n",
    "\n",
    "The shuffling step consists of three substeps:\n",
    "1. take the outputs from the `mappers` and split them into multiple chunks by certain key\n",
    "   values computed from the mapped data.\n",
    "2. move these chunks around so that all data with the same key value is gathered at the same\n",
    "   node.\n",
    "3. concatenate all the chunks at the gathering node to recover the full collection of data\n",
    "   for the subset of keys at the node.\n",
    "\n",
    "To implement the first substep of the shuffling, we define a generic split function that\n",
    "takes a key function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# given a collection of elements `xs` and a key function that computes the key of each of\n",
    "# these elements, return a Dictionary of `key => x`\n",
    "function splitbykey(keyfunc::Function, xs::Any)\n",
    "    splits = Dict()\n",
    "    for x in xs\n",
    "        key = keyfunc(x)\n",
    "        splits[key] = push!(get(splits, key, []), x)\n",
    "    end\n",
    "    return splits\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "With this split function, we define three rules that corresponds to the three substeps of\n",
    "shuffling, and then combine them together in the generic shuffler rule:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "\n",
    "    # use `splitbykey` function defined above\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function) = ApplyFn[splitbykey, keyfunc](mapper...)\n",
    "\n",
    "    # select an element of a dictionary ir exists or return an empty `Vector{Any}`\n",
    "    selectkey(dict::RuleDSL.NodeRef, key::Any; label=\"selectby $(key)\") = ApplyFn[dict -> get(dict, key, [])](dict...)\n",
    "\n",
    "    # merge\n",
    "    mergebykey(vecs::Vector{RuleDSL.NodeRef}) = ApplyFn[vcat](vecs...)\n",
    "\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, keys::Set) = begin\n",
    "\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc) for mapper in mappers)\n",
    "\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in keys\n",
    "            # a `Vector{NodeRef}` that encompasses nodes with a given key\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "\n",
    "            # merge the previously selected nodes outputs\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "\n",
    "            # add merged element to the shuffled `Vector`\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These rules are self explanatory.\n",
    "It is worth mentioning that the `selectkey` rule uses a function closure when constructing\n",
    "the `ApplyFn` atom; and in the `mergebykey` rule, the  `...` follows a `Vector{NodeRef}`\n",
    "to specify dynamic dependencies on multiple rules in the vector. The `label` keyword in the\n",
    " `selectkey` rule is to customize the display information of the individual nodes in the graph\n",
    " web UI. To see how the `label` keyword is used for node display, please refer to the\n",
    " `nodelabel` function defined earlier.\n",
    "\n",
    "We can test the shuffler using the words in the text as the split key. The `first` in the\n",
    "shuffler rule is a function that returns the first element of the `\"word\"=>1` pair, which is\n",
    "the word itself."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# _mappers were created before\n",
    "_shuffler = RuleDSL.@ref mr.shuffler(_mappers, first, Set([\"Bear\", \"Car\", \"Deer\", \"River\"]))\n",
    "\n",
    "gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs2, Set([_shuffler]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs2, port; key=\"mappers\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Reducing\n",
    "\n",
    "Finally, we get to the *reduce* part of the MapReduce pipeline. In the word count example,\n",
    "the `reducer` simply counts the occurrences of a word. The `reducer` rule is applied to the\n",
    "result of `mergebykey`, i.e. a vector of entries like `\"word\" => 1`. Even though all entries\n",
    "have the same key word in this example, we implemented the wordreduce in a generic way that\n",
    "it also works for a vector with multiple key values."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules mr begin\n",
    "    reducer(shuffled::RuleDSL.NodeRef, reducefun::Function) = ApplyFn[reducefun](shuffled...)\n",
    "end\n",
    "\n",
    "# the reducer function\n",
    "function wordreduce(xs::Vector)\n",
    "    count = Dict()\n",
    "    for (key, _) in xs\n",
    "        count[key] = get(count, key, 0) + 1\n",
    "    end\n",
    "    return count\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4 Map/Reduce Rule\n",
    "\n",
    "We now put everything together and write a generic `mapreduce` rule. Note that we use\n",
    "the same `prepend` function to dynamically insert the first argument the for `shuffler` and\n",
    "`mapper` rules:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules mr begin\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "\n",
    "        # create one mapper node per batch\n",
    "        mappers = [prepend(mapper, batch) for batch in batches]\n",
    "\n",
    "        # create the shuffler\n",
    "        shuffler = prepend(shuffler, mappers)\n",
    "\n",
    "        # this gives the inputs to the shuffled nodes, which is where reducer must be applied\n",
    "        shuffled = RuleDSL.calcdeps(RuleDSL.@config, shuffler)\n",
    "        reducers = [prepend(reducer, m) for m in shuffled]\n",
    "\n",
    "        # finally the results (i.e. a Dict per reducer) are merged to a single Dictionary\n",
    "        ApplyFn[merge](reducers...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the MapReduce rule using our word count example:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# no need for the first argument as it will be populated at `mapreduce`\n",
    "_shuffler = RuleDSL.@ref mr.shuffler(first, Set([\"Bear\", \"Car\", \"Deer\", \"River\"]))\n",
    "\n",
    "_mapper  = RuleDSL.@ref mr.mapper(wordmap)\n",
    "_reducer = RuleDSL.@ref mr.reducer(wordreduce)\n",
    "\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(_batches, _mapper, _shuffler, _reducer)\n",
    "\n",
    "gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs3, Set([_mapreduce]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs3, port; key=\"mapred\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting diagram from the Julius web UI is self explanatory, it matches exactly the\n",
    "diagram provided by the Hadoop paper. A side benefit of Julius is that it frees developers\n",
    "from the pain of having to manually draw the system diagram or UMLs ever again. The graph\n",
    "diagram above is an output from the Julius Graph Engine, which shows in great detail both\n",
    "the data and logic. Julius' convenient Web UI allows users to easily navigate and access the\n",
    "entire graph data and logic, which can be accessed by clicking the link above if you are\n",
    "running this example in Jupyter."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 Split by Hashed Keys\n",
    "\n",
    "So far our MapReduce implementation works as expected. However, there is a serious\n",
    "shortcoming in that we have to specify all the possible words in the shuffler, which is not\n",
    "known before we process all the input batches. In practice, we don't want to scan all the\n",
    "input batches just to find out all the possible words, which can be very time consuming when\n",
    "the inputs are large. Also, in live streaming applications such a pre-scan is not possible at\n",
    "all.\n",
    "\n",
    "It would be much more convenient if we don't have to specify all the possible words in the\n",
    "shuffler. We can easily achieve this by supplying a different key function whose number of\n",
    "possible outputs are known, for example, by making use of the `hash` and the remainder `%`\n",
    "functions:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_shuffler = RuleDSL.@ref mr.shuffler(x -> Int(hash(first(x)) % 3), Set(collect(0:2)))\n",
    "\n",
    "# reuse the same _mapper and _reducer declared earlier\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(_batches, _mapper, _shuffler, _reducer)\n",
    "\n",
    "gs4 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs4, Set([_mapreduce]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs4, port; key=\"hash\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the shuffler splits the mapper data into 3 pipes, each of which is identified by\n",
    "an index number. In this implementation, multiple words can go to the same pipe. This\n",
    "implementation removes the need of pre-scans for obtaining all the words; it also works for\n",
    "live streaming use cases. Since the splitting by hash key is a much better implementation,\n",
    "we declare a couple convenience rules to encourage its use:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "@addrules mr begin\n",
    "\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function, N::Int) = begin\n",
    "        ApplyFn[splitbykey, x -> Int(hash(keyfunc(x)) % N)](mapper...)\n",
    "    end\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, N::Int) = begin\n",
    "\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc, N) for mapper in mappers)\n",
    "\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in 0:N-1\n",
    "\n",
    "            # a `Vector{NodeRef}` that encompasses nodes with a given key\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "\n",
    "            # merge the previously selected nodes outputs\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "\n",
    "            # add merged element to the shuffled `Vector`\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the shuffler declaration can be simply given as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_shuffler = RuleDSL.@ref mr.shuffler(first, 3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "which becomes much easier to read and define than its equivalent earlier version of\n",
    "`_shuffler`. Note that since the rules support polymorphism, the hash version of `splitbykey`\n",
    "rule will be used if an integer is supplied as its 3rd argument.\n",
    "\n",
    "So far we have demonstrated the MapReduce pipeline can be implemented using the RuleDSL by\n",
    "simply declaring a few high order rules. The resulting MapReduce rule is generic, powerful\n",
    "and reusable. Next, we will use it to solve a few common MapReduce problems."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Examples of MapReduce\n",
    "\n",
    "### 3.1 Finding Friends\n",
    "\n",
    "We can use the MapReduce pipeline to compute the common friends among hundreds of millions\n",
    " users in a social network. This feature can be applied to populate the *You and Joe have N\n",
    "friends in common* displayed in many social networks. Given the list of friends for each\n",
    "user, we proceed to define both a `mapper` and a `reducer` functions and make use of our\n",
    "previously defined `mapreduce` rule to compute common friends for every user pair $\\left(\n",
    "u_i, u_j \\right)$:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function friends_mapfun(batch::String)\n",
    "    dict = Dict{NTuple{2,Char},Vector}()\n",
    "    handler = strip.(split(batch, \"=>\"))\n",
    "\n",
    "    # no friends\n",
    "    if isone(length(handler))\n",
    "        return [dict]\n",
    "    elseif length(handler) > 2\n",
    "        return error(\"Unexpected data format.\")\n",
    "    end\n",
    "\n",
    "    user, friends = handler\n",
    "\n",
    "    # no friends\n",
    "    if isempty(friends)\n",
    "        return dict\n",
    "    end\n",
    "\n",
    "    uid = only(user)\n",
    "    fids = only.(split(friends, ','))\n",
    "    for fid in fids\n",
    "        if isequal(uid, fid)\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        key = tuple(sort!([uid, fid])...)\n",
    "        push!(dict, key => fids)\n",
    "    end\n",
    "\n",
    "    return dict\n",
    "end\n",
    "\n",
    "function friends_reducefun(shuffler::Vector)\n",
    "    out = Dict{NTuple{2,Char},Vector{Char}}()\n",
    "    for (k, v) in shuffler\n",
    "        if !haskey(out, k)\n",
    "            out[k] = v\n",
    "        else\n",
    "            out[k] = intersect(out[k], v)\n",
    "        end\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "# each user is represented by a `Char`\n",
    "_friends = IOBuffer(\"\n",
    "    A => B,C,D\n",
    "    B => A,C,D,E\n",
    "    C => A,B,D,E\n",
    "    D => A,B,C,E\n",
    "    E => B,C,D\n",
    "\")\n",
    "\n",
    "_batches = RuleDSL.@ref(mr.batch(line) for line in eachline(_friends) if !isempty(line))\n",
    "\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(friends_mapfun)),\n",
    "    RuleDSL.@ref(mr.shuffler(first, 4)),\n",
    "    RuleDSL.@ref(mr.reducer(friends_reducefun))\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs5 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs5, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs5, port; key=\"ff\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 GroupBy\n",
    "\n",
    "When dealing with large data sets, we often need to split them into smaller batches, and then\n",
    "apply the MapReduce pipeline to perform certain operations on individual batches to be then\n",
    "grouped together later. In this section, we will show how to implement the `groupby` operation\n",
    "on a large data set using the MapReduce pipeline.\n",
    "\n",
    "In order to split the data in multiple batches, we make use of our `DDataFrame` (which stands\n",
    "for Distributed DataFrames) provided in the `DataScience` package. The following `mapper`\n",
    "and `reducer` rules implements the group by using any number of features within the\n",
    "`MapReduce` pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using DataFrames\n",
    "using DataScience: DDataFrame\n",
    "\n",
    "# `cols` can be anything accepted by `DataFrames.groupby` method\n",
    "function groupby_mapfun(batch::AbstractDataFrame, cols)\n",
    "    dict = Dict()\n",
    "    gdf = groupby(batch, cols)\n",
    "    for (key, df) in zip(keys(gdf), gdf)\n",
    "        push!(dict, NamedTuple(key) => DataFrame(df; copycols=false))\n",
    "    end\n",
    "    return dict\n",
    "end\n",
    "\n",
    "function groupby_reducefun(shuffler::Vector)\n",
    "    out = Dict()\n",
    "    for (k, v) in shuffler\n",
    "        out[k] = append!(get(out, k, DataFrame()), v)\n",
    "    end\n",
    "    return out\n",
    "end\n",
    "\n",
    "filepath = joinpath(@__DIR__, \"../data/iris.csv\")\n",
    "ddf = DDataFrame(filepath, nrows=25)\n",
    "_batches = ddf.chunks\n",
    "\n",
    "# use 3 reducing nodes for the reducing step\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(x -> groupby_mapfun(x, [:Species]))),\n",
    "    RuleDSL.@ref(mr.shuffler(first, 3)),\n",
    "    RuleDSL.@ref(mr.reducer(groupby_reducefun))\n",
    ")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs6 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs6, Set([_mapreduce]))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "nodelabel(::AbstractGraphState, ref::NodeRef) = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "svg = GraphIO.postlocalgraph(gss, gs6, port; key=\"groupby\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The result is a `DataFrame` per group, such that, the first 10 rows look like:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_reducers = calcdeps(config, _mapreduce)\n",
    "for reducer in _reducers\n",
    "    dict = RuleDSL.getdata(gs6, reducer)[]\n",
    "    for (k, v) in dict\n",
    "      println(\"$k => $(first(v, 10))\")\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "These previous examples are relatively straightforward in their logic. However, the\n",
    "`mapper` and `reducer` rules can encapsulate complicated logic, where both can represent\n",
    "entire graphs of great complexity. For example, the mapper can be the training and validation\n",
    "of an entire ML model, and the reducer can be a bagging algorithm that joins multiple models\n",
    "trained on different batches of data. We will show an example of a more complex use case in\n",
    "the next tutorial.\n",
    "\n",
    "## 4. Advantages of Julius Graph\n",
    "\n",
    "### 4.1 Graph Composition vs Function Composition\n",
    "\n",
    "You may find the high level rules in `RuleDSL` have a lot similarities to high order functions\n",
    "in languages like Haskell, where a function can take another function as a parameter.\n",
    "So what are the main benefits of high order rules over the high order functions in a functional\n",
    "language?\n",
    "\n",
    "The key difference is that high level rules are for composing graphs, while high level\n",
    "functions are for composing functions. The graph composition has a number of advantages over\n",
    "function compositions:\n",
    "\n",
    "1. It does not create deep call stacks. The results of a graph composition is nothing but\n",
    "   another graph. Therefore it is much easier for a developer to visualize and debug. With\n",
    "   function compositions, one has to use a debugger to access the intermediate results and\n",
    "   call sequences, deep among the call stack of a program's runtime.\n",
    "2. The resulting graph composition can be automatically distributed without code changes. A\n",
    "   clever graph distributor can analyze any graph and distribute it effectively to multiple\n",
    "   worker computers. In contrast, the traditional functional code is permeated with loops\n",
    "   and branches, making their runtime behavior unpredictable, and thus cannot be distributed\n",
    "   automatically or efficiently.\n",
    "3. The graph composition is much more flexible. Once the graph is constructed, it can run in\n",
    "   different modes. For example, the same graph can support both batch and streaming use\n",
    "   cases without code changes, which is not possible in traditional functional programming.\n",
    "4. Lastly, graph compositions can mimic function compositions, but the reverse is not true.\n",
    "   The `mapreduce` rule is a good example of how function compositions can be replicated using\n",
    "   graph composition. However, it is not possible to create the equivalent graph\n",
    "   compositions from function compositions in traditional functional languages.\n",
    "\n",
    "You have seen some of the benefits of graph compositions in this and previous tutorials. Next,\n",
    "we will illustrate the second benefit of automatically distributing the MapReduce pipeline to\n",
    "multiple computers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2 Distributed Map/Reduce\n",
    "\n",
    "In order to demonstrate the automatic distribution, we set up a local cluster with 3 worker processes\n",
    "managed by a master process running at a port of the local computer. This setup mimics a remote\n",
    "master and worker process running on multiple physical computers. Please note that the\n",
    "local cluster automatically terminates after 15min of inactivity, so if the\n",
    "local cluster is no longer accessible after 15min, please re-run this entire tutorial notebook.\n",
    "\n",
    "The following few lines of code starts the local cluster then connects to the master process,\n",
    "through which we gain control to all the worker processes:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "\n",
    "config = RuleDSL.newconfig(RuleDSL.Config(), :project => \"MapReduce\")\n",
    "balancer = GraphVM.GlobalUnique()\n",
    "my_domain = GraphVM.mydomain()\n",
    "\n",
    "# draw a port number to start the local cluster esrvice\n",
    "remoteport = GraphVM.drawdataport()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# start a local master service at the given port\n",
    "GraphVM.startlocalmasterservice(remoteport, 3)\n",
    "\n",
    "gs = GraphVM.RemoteGraphProxy(config, my_domain => remoteport, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is the complete definition of the generic `mapreduce` rule and corresponding\n",
    "functions for the word count example. Now we instantiate them in the remote cluster so that\n",
    "we can run the distributed word count with distribution."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs begin\n",
    "    using GraphEngine: RuleDSL, GraphVM\n",
    "    using DataScience: ApplyFn\n",
    "    using AtomExt, GraphIO\n",
    "end\n",
    "\n",
    "# wait for the server to complete the task before proceeding\n",
    "# wait is needed after every @remote_eval\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs mr begin\n",
    "    echo(x::Any) = ApplyFn[identity, x]()\n",
    "\n",
    "    mapper(batch::RuleDSL.NodeRef, mapfun::Function) = ApplyFn[mapfun](batch...)\n",
    "\n",
    "    reducer(shuffled::RuleDSL.NodeRef, reducefun::Function) = ApplyFn[reducefun](shuffled...)\n",
    "\n",
    "    selectkey(dict::RuleDSL.NodeRef, key::Any; label=\"selectby $(key)\") = ApplyFn[dict -> get(dict, key, [])](dict...)\n",
    "\n",
    "    mergebykey(vecs::Vector{RuleDSL.NodeRef}) = ApplyFn[vcat](vecs...)\n",
    "\n",
    "    splitbykey(mapper::RuleDSL.NodeRef, keyfunc::Function, N::Int) = begin\n",
    "        ApplyFn[splitbykey, x -> Int(hash(keyfunc(x)) % N)](mapper...)\n",
    "    end\n",
    "\n",
    "    shuffler(mappers::Vector{RuleDSL.NodeRef}, keyfunc::Function, N::Int) = begin\n",
    "        splits = RuleDSL.@ref(splitbykey(mapper, keyfunc, N) for mapper in mappers)\n",
    "        shuffled = Vector{NodeRef}()\n",
    "        for key in 0:N-1\n",
    "            selected = RuleDSL.@ref(selectkey(s, key) for s in splits)\n",
    "            merged = RuleDSL.@ref mergebykey(selected; label=\"mergeby $key\")\n",
    "            push!(shuffled, merged)\n",
    "        end\n",
    "        Alias(shuffled...)\n",
    "    end\n",
    "\n",
    "    mapreduce(\n",
    "        batches::Vector{RuleDSL.NodeRef},\n",
    "        mapper::RuleDSL.NodeRef,\n",
    "        shuffler::RuleDSL.NodeRef,\n",
    "        reducer::RuleDSL.NodeRef\n",
    "    ) = begin\n",
    "        mappers = [prepend(mapper, batch) for batch in batches]\n",
    "        shuffler = prepend(shuffler, mappers)\n",
    "        shuffled = RuleDSL.calcdeps(RuleDSL.@config, shuffler)\n",
    "        reducers = [prepend(reducer, m) for m in shuffled]\n",
    "        ApplyFn[merge](reducers...)\n",
    "    end\n",
    "end\n",
    "\n",
    "GraphVM.@remote_eval gs begin\n",
    "    prepend(ref::RuleDSL.NodeRef, firstarg::Any) = RuleDSL.NodeRef(ref.ns, ref.name, (firstarg, ref.params...), ref.meta)\n",
    "\n",
    "    function splitbykey(keyfunc::Function, xs::Any)\n",
    "        splits = Dict()\n",
    "        for x in xs\n",
    "            key = keyfunc(x)\n",
    "            splits[key] = push!(get(splits, key, []), x)\n",
    "        end\n",
    "        return splits\n",
    "    end\n",
    "\n",
    "    wordmap(words::String) = [k => 1 for k in split(words)]\n",
    "\n",
    "    function wordreduce(xs::Vector)\n",
    "        count = Dict()\n",
    "        for (key, _) in xs\n",
    "            count[key] = get(count, key, 0) + 1\n",
    "        end\n",
    "        return count\n",
    "    end\n",
    "\n",
    "    import GraphEngine.RuleDSL: nodelabel\n",
    "    function nodelabel(gs::RuleDSL.AbstractGraphState, ref::RuleDSL.NodeRef)\n",
    "        shortrepr(x::Vector; sep=\", \") = \"[\"*join(shortrepr.(x), sep)*\"]\"\n",
    "        shortrepr(p::Pair; sep=\"=>\") = shortrepr(p.first) * sep * shortrepr(p.second)\n",
    "        shortrepr(p::Dict; sep=\", \") = \"{\" * join(shortrepr.(collect(p)), sep) * \"}\"\n",
    "        shortrepr(x::Any; sep=\"\") = repr(x)\n",
    "\n",
    "        label = haskey(ref.meta, :label) ? ref.meta[:label] : \"$(ref.ns).$(ref.name)\"\n",
    "\n",
    "        try\n",
    "            data = RuleDSL.getdata(gs, ref)\n",
    "            if isone(length(data))\n",
    "                data = first(data)\n",
    "            end\n",
    "            label *= \"\\n\" * shortrepr(data; sep = \"\\n\")\n",
    "        catch\n",
    "            label *= \": n/a\"\n",
    "        end\n",
    "\n",
    "        return label\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, there is no change in the RuleDSL and Julia functions at all, we simply sent\n",
    "them to the remote cluster to instantiate. Afterwards, we can execute the MapReduce pipeline\n",
    "with distribution. The distribution API, `RuleDSL.jobdeps` and `GraphVM.dispatchjobs!` are\n",
    "explained in more detail in the next tutorial on Distributed ML pipeline, so we won't\n",
    "repeat them here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "_sentences = (\"Deer Bear River\", \"Car Car River\", \"Deer Car Bear\")\n",
    "_batches   = RuleDSL.@ref(mr.echo(s) for s in _sentences)\n",
    "\n",
    "N = 3\n",
    "_mapreduce = RuleDSL.@ref mr.mapreduce(\n",
    "    _batches,\n",
    "    RuleDSL.@ref(mr.mapper(Main.wordmap)),\n",
    "    RuleDSL.@ref(mr.shuffler(first, N)),\n",
    "    RuleDSL.@ref(mr.reducer(Main.wordreduce))\n",
    ")\n",
    "\n",
    "alljobs, ds = RuleDSL.jobdeps(config, [_mapreduce], [:splitbykey, :reducer]);\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "GraphVM.initgraph!(gs)\n",
    "GraphVM.dispatchjobs!(gs, alljobs; nocopy=Set([:splitbykey]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "svg = GraphIO.postremotegraph(gs, port);\n",
    "\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting graph uses different colors to highlight the placement of nodes to individual\n",
    "remote workers. Nodes with the same color are placed and computed on the same worker\n",
    "computer.\n",
    "\n",
    "Upon closer examination, we observe that the resulting graph distribution is optimal in\n",
    "that the work load is evenly distributed amongst 3 workers. The only shipment of data\n",
    "happens during the shuffling stage and the collation of final reducer results, when an\n",
    "arrow connects two nodes with different colors. There is no unnecessary data transfer\n",
    "in the resulting graph distribution.\n",
    "\n",
    "The ability to automatically and optimally distribute graphs without code change is a\n",
    "powerful feature. Julius can handle the distribution of graphs as large as\n",
    "hundreds millions nodes across hundreds of computers. Using Julius, the same code runs\n",
    "efficiently on one worker instance or hundreds of worker instances, without the need for any\n",
    "manual tweaking or optimizations. Auto-scaling allows developers to quickly build and test\n",
    "their rules and functions on the local computer, then immediately scale it to run large\n",
    "jobs and heavy workloads in parallel without the need for any code changes. Julius'\n",
    "autoscaling automates away one of the most time-consuming and expensive aspects of enterprise\n",
    "systems, which is the constant need to manually optimize a system for\n",
    "better performance and scalability.\n",
    "\n",
    "In a next toturial, \"Distributed ML pipeline\", we will dive into the Julius distribution\n",
    "and auto-scaling capabilities in much more depth, and compare them to existing tools like\n",
    "Dask and Dagger.jl."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "The results speak for themselves: we built a generic MapReduce pipeline from scratch using 10\n",
    "rules in the RuleDSL and 20 lines of additional Julia code. The resulting MapReduce pipeline\n",
    "implementation is generic, transparent and auto-scaling. Every intermediate calculation\n",
    "result is fully visible to the user in our web UI, it automatically distributes to multiple\n",
    "computers without the need for code changes for extreme scalability and performance.\n",
    "\n",
    "Intrigued? If you are a developer, you should be. To hear more about the Julius Graph Engine,\n",
    "contact us at `info@juliustech.co`, or go to our [website](http://juliustech.co) to schedule\n",
    "a demo or sign up for free access for developers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Appendix: Additional Technical Tips & Notes\n",
    "\n",
    "Here we explain some additional technical tips and points. We refer to the general structure\n",
    "of a rule in RuleDSL as:\n",
    "\n",
    "```julia\n",
    "rulename(ruleargs::Any...) = Atom[atomargs...](dependentrule(depargs...))\n",
    "```\n",
    "\n",
    "* The `ApplyFn` atom is used extensively in this tutorial. Though it is convenient, it was\n",
    "  only intended for simple analytical logic. For complex analytical algorithms, it is better\n",
    "  to define individual Atoms for reusability and readability.\n",
    "* There is an important difference between the `ruleargs` and `atomargs` in the rule syntax.\n",
    "  The `ruleargs` is serialized and sent to a remote worker during distribution, while atomargs\n",
    "  are only created and used locally by individual workers. Therefore to enable distribution,\n",
    "  every `ruleargs` has to be serializable with a stable hash, i.e. the same object shall\n",
    "  retrieve the same hash regardless of which worker runs it. This requirement does not apply to\n",
    "  atomargs. Julius uses a customized hash function `RuleDSL.hashparam` for rule parameters,\n",
    "  that supports stable hashes for a wide variety of object types. However, the following are\n",
    "  some instances where the serialization can fail or the hash becomes unstable:\n",
    "  * If a rule parameter is a function closure with reference to any local variable, the\n",
    "    serialization will fail.  A workaround is to move the function closure inside the body\n",
    "    of the rule. For example, the first hash key shuffler will fail:\n",
    "    ```julia\n",
    "    # fail to serialize: local variable used in function closure\n",
    "    N = 3\n",
    "    _shuffler = RuleDSL.@ref mr.shuffler(x -> Int(hash(first(x)) % N), Set(collect(0:N-1)))\n",
    "    ```\n",
    "    but the second version of the shuffler will work fine:\n",
    "    ```julia\n",
    "    # closure moved inside the rule declaration\n",
    "    _shuffler = RuleDSL.@ref mr.shuffle(first, N)\n",
    "    ```\n",
    "  * A complex struct is more likely to have unstable hashes, so you can either make it inherit\n",
    "    from `RuleDSL.ValueType` using the more stable `RuleDSL.hashparam`, or you can\n",
    "    provide your own stable hash function by overriding the `RuleDSL.hashparam` method for\n",
    "    the type in question. To help detect the potential serialization and hash stability\n",
    "    issues in rules, we provide a convenient macro `RuleDSL.@isdistributable`, which will\n",
    "    flag any node in a graph that cannot be safely distributed.\n",
    "* You may be tempted to define a `mapreduce` rule that takes a mapper function and a reducer\n",
    "  function, and create the `RuleDSL.@ref mr.mapper(func)` and `RuleDSL.@ref\n",
    "  mr.reducer(func)` inside the `mapreduce` rule. As discussed before, this is less generic\n",
    "  as the `mapper` and `reducer` rule is not restricted to simple wrappers like\n",
    "  `RuleDSL.@ref mr.mapper(func)`. Instead, any rule can be used as the `mapper` and\n",
    "  `reducer`. For example it could  represent a complex graph with sophisticated logic. Or,\n",
    "  they could in fact be high order rules themselves."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
