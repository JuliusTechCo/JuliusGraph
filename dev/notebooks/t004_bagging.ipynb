{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 4: Distributed Machine Learning Pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this tutorial, we use Julius GraphEngine and its Domain Specific Language (RuleDSL) to\n",
    "build a distributed Machine Learning (ML) pipeline that can process a large volume of\n",
    "data in parallel for model training and inference.\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "In real world ML problems, the training or inference data can be too large to fit into the\n",
    "memory of a single computer. It is a common challenge that ML engineers often face when\n",
    "productionizing a ML model. There is a popular blog post, [Training models when data doesn't\n",
    "fit in memory](https://gdmarmerola.github.io/big-data-ml-training/), describing how to use\n",
    "[Dask](https://dask.org/), a popular python distribution package, to build distributed ML\n",
    "pipelines that can process large training and inference data in batches, in order to handle\n",
    "data size greater than a computer's memory.\n",
    "\n",
    "In this tutorial, we are going to replicate the functionality described in the blog post\n",
    "using Julius GraphEngine instead of using Dask or Dagger.jl (which is a Julia package inspired\n",
    "by Dask). We will show how to achieve similar or better results as in the original Dask\n",
    "blog with much fewer lines of code. Here, we will re-use the generic MapReduce pipeline\n",
    "developed in a previous tutorial. At the end of this tutorial, we will compare Julius with\n",
    "Dask, and summarize their key differences.\n",
    "\n",
    "We use the same fraud detection dataset as the Dask blog, which is originated from\n",
    "[Kaggle](https://www.kaggle.com/ealaxi/paysim1/version/2). The realization of fraud is\n",
    "indicated by the column `isFraud`, which is the outcome the ML model tries to predict."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Simple ML Pipeline\n",
    "\n",
    "As a baseline solution, we first build a simple ML pipeline in Julius without batching or\n",
    "distribution. We will then describe step by step how to adapt it to support batching and\n",
    "distribution. This is enlightening as it matches a typical ML model development workflow,\n",
    "where a data scientist first build a ML model using a simple data pipeline, then a data\n",
    "engineer parallelizes the implementation to handle a large volume of data. To\n",
    "productionize a ML model, it often takes multiple iterations between data scientists and\n",
    "data engineers, which is one of the most time-consuming and costly step of a ML model's life\n",
    "cycle. By repeating the exact process in this tutorial, we illustrate how easy it is to move\n",
    "a ML pipeline from development to production using Julius.\n",
    "\n",
    "In order to match the numbers in the original Dask blog, we chose to use the same Python ML\n",
    "models in sklearn. Julius can interop and integrate with any major programming languages\n",
    "such as Python, C/C++, Java, R, .Net, Julia, etc.\n",
    "\n",
    "The fraud detection ML pipeline, as described in the Dask blog, consists of the following\n",
    "steps:\n",
    "* read datasets,\n",
    "* separate the features from target variable from the data,\n",
    "* train a ML model (`ExtraTreesClassifier` in the Dask blog),\n",
    "* infer using test data,\n",
    "* compute AUC score.\n",
    "\n",
    "We now proceed to the implementation. First, we import the required Julia and Julius\n",
    "packages:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "using DataFrames, DataScience, StatsBase\n",
    "using AtomExt"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following are the few rules to define the entire ML pipeline, which are quite\n",
    "self-explanatory and roughly follow the steps mentioned above."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules ml begin\n",
    "\n",
    "    # selects columns `cols` from a DataFrame\n",
    "    select(\n",
    "        ref::RuleDSL.NodeRef, cols::Any;\n",
    "        label=\"$(isa(cols, InvertedIndex) ? \"col != $(cols.skip)\" : \"col == $(cols)\")\"\n",
    "    ) = begin\n",
    "        DataScience.ApplyFn[x::DataFrame -> DataFrames.select(x, cols; copycols=false)](ref...)\n",
    "    end\n",
    "\n",
    "    # any `sklearn` ML model can be easily handled by overloading the `classifiertrain`\n",
    "    # rule, as follows\n",
    "    classifiertrain(\n",
    "        model::Val{:ExtraTreesClassifier},\n",
    "        options::Dict,\n",
    "        trainxs::RuleDSL.NodeRef,\n",
    "        trainy::RuleDSL.NodeRef;\n",
    "        label=\"$model train\"\n",
    "    ) = begin\n",
    "        DataScience.PyTrain[\"sklearn.ensemble.ExtraTreesClassifier\", options](trainxs..., trainy...)\n",
    "    end\n",
    "\n",
    "    # this rule makes the predictions\n",
    "    classify(\n",
    "        train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, testx::RuleDSL.NodeRef;\n",
    "        label=\"$model inference\"\n",
    "    ) = begin\n",
    "        train_data_X = RuleDSL.@ref ml.select(train_data, Not(target))\n",
    "        train_data_y = RuleDSL.@ref ml.select(train_data, target)\n",
    "        trained = RuleDSL.@ref ml.classifiertrain(model, options,  train_data_X, train_data_y )\n",
    "        DataScience.PyPredict(trained..., testx...)\n",
    "    end\n",
    "\n",
    "    # makes predictions and selects the `:proba` column for the resulting DataFrame\n",
    "    classifyprob(\n",
    "        train_data::RuleDSL.NodeRef,\n",
    "        target::Symbol,\n",
    "        model::Val,\n",
    "        options::Dict,\n",
    "        test_data::RuleDSL.NodeRef;\n",
    "        label=\"prob\"\n",
    "    ) = begin\n",
    "        testx = RuleDSL.@ref ml.select(test_data, Not(target))\n",
    "        DataScience.ApplyFn[\n",
    "            x::DataFrame -> DataFrames.select(x, :proba; copycols=false)\n",
    "        ](classify(train_data, target, model, options, testx))\n",
    "    end\n",
    "\n",
    "    # computes the AUC score\n",
    "    score(realized::RuleDSL.NodeRef, probs::RuleDSL.NodeRef) = begin\n",
    "        DataScience.PyScore(realized..., probs...)\n",
    "    end\n",
    " end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `PyTrain`, `PyPredict`, `PyScore` are three useful atoms provided by the `DataScience`\n",
    "package that wraps up the training, inference and `roc_auc_score` method from the sklearn\n",
    "Python package. The `PyTrain` atom is generic, it can instantiate any Python ML model using\n",
    "its name and parameters, as shown at the `ml.classifiertrain` rule. Using this set of\n",
    "rules, we can create a simple ML pipeline as:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# we use existing rules in ds namespace to read CSV files from a shared drive\n",
    "train_data_file = joinpath(@__DIR__, \"..\", \"data/train_fraud.csv\")\n",
    "test_data_file = joinpath(@__DIR__, \"..\", \"data/test_fraud.csv\")\n",
    "train_data = RuleDSL.@ref ds.csvsrc(train_data_file, true; label=\"train data\")\n",
    "test_data  = RuleDSL.@ref ds.csvsrc(test_data_file, true; label=\"test data\")\n",
    "\n",
    "target = :isFraud\n",
    "model = Val(:ExtraTreesClassifier)\n",
    "options = Dict(:n_estimators => 10, :min_samples_leaf => 10)\n",
    "\n",
    "pred = RuleDSL.@ref ml.classifyprob(train_data, target, model, options, test_data)\n",
    "test_data_y = RuleDSL.@ref ml.select(test_data, target)\n",
    "mlscore = RuleDSL.@ref ml.score(test_data_y, pred);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "config = RuleDSL.Config()\n",
    "gs1 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs1, Set([mlscore]));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have now created a simple ML training and inference pipeline without using batching or\n",
    "distribution. Julius provides an easy-to-use web UI for users to navigate and visualize the\n",
    "resulting data and logic in the computation graph. The following code block starts a local\n",
    "server for the web UI so that we can retrieve the resulting data from the graph."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphIO\n",
    "\n",
    "# a container of graphs\n",
    "gss = Dict{String,RuleDSL.AbstractGraphState}()\n",
    "\n",
    "# used for WebUI display purposes\n",
    "port = GraphVM.drawdataport()\n",
    "@async GraphVM.startresponder(gss, port);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Julius package `GraphIO` provides several convenience functions for retrieving and\n",
    "displaying graphs in SVG format. User can also view the graph data interactively by clicking\n",
    "on the url below to bring up the full web UI."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs1, port, true; key=\"single\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, the AUC score value obtained using the complete dataset is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs1, mlscore, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 Down Sampling\n",
    "\n",
    "Downsampling is a common technique to reduce the training data size so that the training can\n",
    "run faster with a large amount of data. The Dask blog implemented a 5% downsampling while\n",
    "maintaining a constant fraction of real fraud. We replicate the same downsampling scheme\n",
    "using a single Julia function and a single rule in Julius:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Random\n",
    "using StatsBase\n",
    "\n",
    "function downsample(ycol::Symbol, frac::Float64, df::DataFrame)\n",
    "    # get filtered DataFrames with true/false cases for isFraud\n",
    "    positive = DataFrames.filter(row -> isequal(row[ycol], true), df)\n",
    "    negative = DataFrames.filter(row -> isequal(row[ycol], false), df)\n",
    "\n",
    "    # sample without replacement each DataFrame\n",
    "    dspositive = positive[sample(1:nrow(positive), round(Int, frac * nrow(positive)), replace=false), :]\n",
    "    dsnegative = negative[sample(1:nrow(negative), round(Int, frac * nrow(negative)), replace=false), :]\n",
    "\n",
    "    # concatenate both sampled DataFrames\n",
    "    merged = vcat(dspositive, dsnegative)\n",
    "\n",
    "    # shuffle rows before returning\n",
    "    return merged[shuffle(1:nrow(merged)), :]\n",
    "end\n",
    "\n",
    "@addrules ml begin\n",
    "    downsample(\n",
    "        raw::RuleDSL.NodeRef, ycol::Symbol, frac::Float64\n",
    "    ) = begin\n",
    "        DataScience.ApplyFn[downsample, ycol, frac](raw...)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's test the downsampling:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sampleratio = 0.05\n",
    "downsamples = RuleDSL.@ref ml.downsample(train_data, target, sampleratio)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs2, Set([downsamples]))\n",
    "\n",
    "svg = GraphIO.postlocalgraph(gss, gs2, port; key=\"downsample\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can verify that the fraud frequency remains unchanged, the minor remaining difference is\n",
    "due to rounding."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "sample_df = RuleDSL.getdata(gs2, downsamples, 1)\n",
    "sum(sample_df.isFraud) / size(sample_df, 1) * 100"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# use full data set to verify\n",
    "using CSV\n",
    "df = CSV.read(train_data_file, DataFrames.DataFrame)\n",
    "sum(df.isFraud) / size(df, 1) * 100"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is easy to modify the existing ML pipeline to include downsampling, we just replace the\n",
    "`train_data` with `downsamples`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "downproba = RuleDSL.@ref ml.classifyprob(downsamples, target, model, options, test_data)\n",
    "downscore = RuleDSL.@ref ml.score(test_data_y, downproba)\n",
    "\n",
    "gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs3, Set([downscore]))\n",
    "\n",
    "svg = GraphIO.postlocalgraph(gss, gs3, port, true; key=\"downscore\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, the AUC score obtained using downsampling is slightly less than training with the\n",
    "full data, as expected."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs3, downscore, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have built the baseline ML pipeline where the entire training and inference data is\n",
    "processed all at once. A obvious downside of this implementation is that it can't handle large\n",
    "training or inference data, if they don't fit into the computer's memory. Now let's proceed\n",
    "to productionize the pipeline by adding batching and distribution.\n",
    "\n",
    "## 2. ML Pipeline with Batching\n",
    "\n",
    "It is a common strategy to break the training data into multiple batches and train a\n",
    "separate ML model for each batch. Once we have multiple trained ML models, we can average\n",
    "their inferences for better accuracy. This strategy of boosting accuracy from multiple\n",
    "trained models is commonly called \"bagging\". Batching and bagging are often used together to\n",
    "allow large training data to be split across multiple machines, and be processed in\n",
    "parallel.\n",
    "\n",
    "### 2.1 Training Data Batching\n",
    "\n",
    "We use a convenience type `DataScience.DDataFrame` provided by Julius `DataScience` package\n",
    "to create a vector of `RuleDSL.NodeRef` that represents roughly equal-sized chunks from\n",
    "the large input CSV file."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train_ddf = DataScience.DDataFrame(train_data_file, blocksize=\"5 MB\")\n",
    "train_batches = train_ddf.chunks\n",
    "down_batches = RuleDSL.@ref(ml.downsample(b, target, sampleratio) for b in train_batches)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training data batching and bagging can be easily implemented using the following single\n",
    "rule, which just re-uses the previous `ml.classifyprob` for each input batch and average\n",
    "their output."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# compute the average of multiple dataframes\n",
    "function dfmean(dfs::DataFrame...)\n",
    "    df = reduce(.+, dfs)\n",
    "    df ./ (length(dfs))\n",
    "end\n",
    "\n",
    "RuleDSL.@addrules ml begin\n",
    "    bagpred(\n",
    "        test::RuleDSL.NodeRef,\n",
    "        model::Val,\n",
    "        options::Dict,\n",
    "        train_batches::Vector{RuleDSL.NodeRef},\n",
    "        target::Symbol\n",
    "    ) = begin\n",
    "        refs = RuleDSL.@ref((ml.classifyprob(b, target, model, options, test) for b in train_batches))\n",
    "        DataScience.ApplyFn[dfmean](refs...)\n",
    "    end\n",
    "end\n",
    "\n",
    "bagpred = RuleDSL.@ref ml.bagpred(test_data, model, options, down_batches, target)\n",
    "bagscore = RuleDSL.@ref ml.score(test_data_y, bagpred)\n",
    "\n",
    "gs4 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs4, Set([bagscore]))\n",
    "\n",
    "svg = GraphIO.postlocalgraph(gss, gs4, port; key=\"ml\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The AUC score obtained using multiple samples of the data is:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs4, bagscore, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Batching both Training and Prediction Data\n",
    "\n",
    "In the previous implementation, the training data is batched but not the inference data. In\n",
    "practice, the inference data could also be too large to fit into the memory of a single\n",
    "machine. In that case, we will also need to batch the inference data. It is a much more\n",
    "complex pipeline to batch both training and inference data. However, in Julius, we can\n",
    "leverage the generic MapReduce pattern to easily define this complicated pipeline with very\n",
    "little coding.\n",
    "\n",
    "We first use `DataScience.DDataFrame` to create a vector of `NodeRef.RuleDSL` for the\n",
    "inference data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "test_ddf = DataScience.DDataFrame(test_data_file, blocksize=\"3.5 MB\")\n",
    "test_batches = test_ddf.chunks"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The doubly batched training and inference ML pipeline naturally maps to a MapReduce pipeline\n",
    "as following:\n",
    "* mapper: compute the bagged inference of a single test batch from multiple trained models,\n",
    "  this stage already includes training data batching,\n",
    "* shuffler/reducer: move the individual batch inference and concatenate them to form the\n",
    "  entire inference.\n",
    "\n",
    "The following `batchpred` rule extracts the realization and inference from the same test\n",
    "batch file in a DataFrame, and assign a unique key using\n",
    "the hash of the batch file's `NodeRef`."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules ml begin\n",
    "    # extract both realization and prob predictions\n",
    "    batchpred(\n",
    "        test::RuleDSL.NodeRef,\n",
    "        model::Val,\n",
    "        options::Dict,\n",
    "        train_batches::Vector{RuleDSL.NodeRef},\n",
    "        target::Symbol\n",
    "    ) = begin\n",
    "        DataScience.ApplyFn[\n",
    "            (ind, prob)->[hash(test) => hcat(ind, prob)]\n",
    "        ](select(test, target), bagpred(test, model, options, train_batches, target))\n",
    "    end\n",
    "end\n",
    "\n",
    "# extracts the DataFrames from `batchpred` from all batches and concatenates them\n",
    "function valcat(xs::Vector...)\n",
    "    agg = DataFrame()\n",
    "    for (_, v) in vcat(xs...)\n",
    "        agg = vcat(agg, v)\n",
    "    end\n",
    "    return agg\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following is the entire definintion of the doubly batched ML pipeline using the\n",
    "MapReduce pattern:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mapper = RuleDSL.@ref ml.batchpred(model, options, down_batches, target)\n",
    "\n",
    "# map all batches to 3 pipelines before reducing\n",
    "shuffler = RuleDSL.@ref mr.shuffler(first, 3)\n",
    "\n",
    "# simply concatenates all the vectors in a given pipeline\n",
    "reducer = RuleDSL.@ref mr.reducer(vcat)\n",
    "\n",
    "# valcat extracts the DataFrame from each batch, and concatenate them together\n",
    "mrpred = RuleDSL.@ref mr.mapreduce(test_batches, mapper, shuffler, reducer, valcat)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last step in MapReduce is to aggregate the results from the mapper/shuffler results of\n",
    "individual test batches. The generic `mr.mapreduce` rule can take an optional function as\n",
    "the last parameter to customize this aggregation. The function `valcat` is used for\n",
    "the aggregation, which concatenates individual batchs' fraud realization and inference\n",
    "into a single DataFrame."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "mrscore = RuleDSL.@ref ml.score(RuleDSL.@ref(ml.select(mrpred, :isFraud)), RuleDSL.@ref(ml.select(mrpred, :proba)))\n",
    "\n",
    "gs5 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())\n",
    "GraphVM.calcfwd!(gs5, Set([mrscore]))\n",
    "\n",
    "svg = GraphIO.postlocalgraph(gss, gs5, port; key=\"mapred\");\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The corresponding AUC score from the doubly batched pipeline is similar:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.getdata(gs5, mrscore, 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Distributed ML Pipeline\n",
    "\n",
    "We have now adapted the simple ML pipeline to support doubly data batching in both training and\n",
    "inference. However, the real benefits of batching only comes from distributing the data and\n",
    "computation to multiple computers, so that we can process large volume of data and\n",
    "computation in parallel.\n",
    "\n",
    "Using Julius, it is effortless to distribute the batched pipeline to multiple computers and\n",
    "run it in parallel. Let's use the doubly batched ML pipeline as an example to show how easy\n",
    "it is to distribute. We first connect to a remote cluster with 3 worker instances, and\n",
    "import necessary packages on the remote cluster:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "\n",
    "config = RuleDSL.newconfig(RuleDSL.Config(), :project => \"MapReduce\")\n",
    "balancer = GraphVM.GlobalUnique()\n",
    "my_domain = GraphVM.mydomain()\n",
    "\n",
    "# draw a port number to start the local cluster esrvice\n",
    "remoteport = GraphVM.drawdataport()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# start a local master service at the given port, which mimic the effects of a remote cluster\n",
    "\n",
    "GraphVM.startlocalmasterservice(remoteport, 3)\n",
    "gs = GraphVM.RemoteGraphProxy(config, my_domain => remoteport, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs begin\n",
    "    using GraphEngine: RuleDSL, GraphVM\n",
    "    using DataScience, Random, AtomExt, GraphIO\n",
    "    using StatsBase, DataFrames\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now load the entire doubly batched ML pipeline to the remote cluster, by sending the\n",
    "entire code we have written so far to the remote cluster. The following is the full list of\n",
    "code to replicate the Dask blog in Julius. There are only 8 rules, and about 50 lines of\n",
    "code in total."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs  begin\n",
    "    function downsample(ycol::Symbol, frac::Float64, df::DataFrame)\n",
    "        positive = DataFrames.filter(row -> isequal(row[ycol], true), df)\n",
    "        negative =  DataFrames.filter(row -> isequal(row[ycol], false), df)\n",
    "        dspositive = positive[sample(1:nrow(positive), round(Int, frac * nrow(positive)), replace=false), :]\n",
    "        dsnegative = negative[sample(1:nrow(negative), round(Int, frac * nrow(negative)), replace=false), :]\n",
    "        merged = vcat(dspositive, dsnegative)\n",
    "        return merged[shuffle(1:nrow(merged)), :]\n",
    "    end\n",
    "\n",
    "    function dfmean(dfs::DataFrame...)\n",
    "        df = reduce(.+, dfs)\n",
    "        return df ./ (length(dfs))\n",
    "    end\n",
    "\n",
    "    function valcat(xs::Vector...)\n",
    "        agg = DataFrame()\n",
    "        for (_, v) in vcat(xs...)\n",
    "            agg = vcat(agg, v)\n",
    "        end\n",
    "        return agg\n",
    "    end\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs ml begin\n",
    "    select(ref::RuleDSL.NodeRef, cols::Any; label=\"$(isa(cols, InvertedIndex) ? \"col != $(cols.skip)\" : \"col == $(cols)\")\") = DataScience.ApplyFn[x::DataFrame->DataFrames.select(x, cols; copycols=false)](ref...)\n",
    "    classifiertrain(model::Val{:ExtraTreesClassifier}, options::Dict, trainxs::RuleDSL.NodeRef, trainy::RuleDSL.NodeRef; label=\"$model train\") = DataScience.PyTrain[\"sklearn.ensemble.ExtraTreesClassifier\", options](trainxs..., trainy...)\n",
    "    classify(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, testx::RuleDSL.NodeRef; label=\"$model inference\") = begin\n",
    "        train_data_X = RuleDSL.@ref ml.select(train_data, Not(target))\n",
    "        train_data_y = RuleDSL.@ref ml.select(train_data, target)\n",
    "        trained = RuleDSL.@ref ml.classifiertrain(model, options,  train_data_X, train_data_y )\n",
    "        DataScience.PyPredict(trained..., testx...)\n",
    "    end\n",
    "    classifyprob(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, test_data::RuleDSL.NodeRef; label=\"prob\") = begin\n",
    "        testx = RuleDSL.@ref ml.select(test_data, Not(target))\n",
    "        DataScience.ApplyFn[x::DataFrame->DataFrames.select(x, :proba; copycols=false)](classify(train_data, target, model, options, testx))\n",
    "    end\n",
    "    score(realized::RuleDSL.NodeRef, probs::RuleDSL.NodeRef)=DataScience.PyScore(realized..., probs...)\n",
    "    downsample(raw::RuleDSL.NodeRef, ycol::Symbol, frac::Float64)=DataScience.ApplyFn[Main.downsample, ycol, frac](raw...)\n",
    "    bagpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) = DataScience.ApplyFn[Main.dfmean](RuleDSL.@ref((ml.classifyprob(b, target, model, options, test) for b = train_batches))...)\n",
    "    batchpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) = DataScience.ApplyFn[(ind, prob)->[hash(test) => hcat(ind, prob)]](select(test, target), bagpred(test, model, options, train_batches, target))\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Afterwards, we can create and run the doubly batched pipeline on the cluster."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# select all the nodes with rule name classifiertrain, splitbykey and reducer in alljobs\n",
    "keyjobs, ds = RuleDSL.jobdeps(config, [mrscore], Set([:classifiertrain, :splitbykey, :reducer]));\n",
    "GraphVM.initgraph!(gs)\n",
    "\n",
    "# distribute the nodes in alljobs to workers\n",
    "GraphVM.dispatchjobs!(gs, keyjobs, 1)\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the code block above, we first find the `keyjobs`, which is a vector of `NodeRef` that\n",
    "holds all the nodes in the graph that are associated with rule name classifertrain,\n",
    "splitbykey and reducer. These nodes are the main points of computation and data\n",
    "processing. A developer can use his domain knowledge about the specific workload in\n",
    "selecting the best set of rule names for the key jobs. The last key job is always the final\n",
    "node that user want to compute, which is `mrscore` in the cell above. Once these key jobs\n",
    "(or key nodes) are determined, we send them to the workers by calling `GraphVM.dispatchjobs!`,\n",
    "whose 3rd parameter specifies the number of threads worker shall run to process these jobs\n",
    "concurrently.\n",
    "\n",
    "The way Julius distribute the jobs to workers is fundamentally different from\n",
    "Dask or Dagger.jl distribution. In Dask or Dagger.jl, every node in their graph represents\n",
    "a separate task that must be sent to a worker for execution individually. In Julius, since\n",
    "every worker holds the entire graph logic as specified by the set of rules in `RuleDSL`,\n",
    "the dispatcher does not need to send every node to the worker, instead only few key nodes\n",
    "are sent in the format of `NodeRef` object. Once a worker receives a job definition in\n",
    "`NodeRef`, it can create any dependent nodes from the common set of rules shared by all\n",
    "workers and the dispatcher. As a result, Julius distribution requires much less communication\n",
    "between the dispathcer and the workers, thus incurring much less overhead.\n",
    "\n",
    "The following cell shows that only 15 nodes need to be sent to the workers instead of all\n",
    "125 nodes in the graph."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gstats = GraphVM.rpccall(gs, GraphIO.graphstats, UInt(1), UInt(1))\n",
    "println(\"length of keyjobs = \", length(keyjobs))\n",
    "println(\"graph node cnt = \", gstats[:cnt])"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "By creating dependent nodes at worker, Julius also achieves much better locality for graph\n",
    "execution, as all the dependent nodes created by the worker are local. Therefore Julius\n",
    "graph distribution and execution are much more efficient than Dask or Dagger.jl. Julius\n",
    "can easily distribute graphs as large as hundreds of millions of nodes, and even in these\n",
    "cases, the number of keyjobs to distribute is rarely more than a few thousands. In comparison,\n",
    "Dask or Dagger.jl suffer from huge overhead when dealing with large graphs because of the need\n",
    "to distribute every node. As a result, developers are advised to \"avoid large graphs\"\n",
    "when using Dask, Julius does not have such limitations."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postremotegraph(gs, remoteport);\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data from the remote cluster are easily accessible using Julius' remote RPC interface."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.rpccall(gs, :onnode, :getdata, UInt[0], hash(mrscore), 1)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The resulting graph uses different colors to indicate the placement of nodes to individual\n",
    "workers instance, where a single color represent a given physical worker instance. There is\n",
    "a network data transfer for every arrow connecting two nodes of different colors. The entire\n",
    "graph distribution is handled by Julius automatically without the need for the developer to\n",
    "change a single line of code in RuleDSL or Atom definitions.\n",
    "\n",
    "Upon close examination, we observe that the resulting MapReduce distribution is optimal in\n",
    "that the work load is evenly distributed amongst 3 workers, and there is no unnecessary data\n",
    "transfer between different physical computers (represented by different colors) at all in\n",
    "the resulting graph distribution.\n",
    "\n",
    "Neither training nor inference input data were ever aggregated onto a single computer, so\n",
    "that we can stay within individual worker's memory limit. Only the realization and inference\n",
    "outputs were aggregated to a single machine, which are only two columns of data, a tiny\n",
    "fraction comparing to the entire inference input data set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Streaming\n",
    "\n",
    "Besides batching, streaming is another effective strategy to minimize memory usage. Instead\n",
    "of processing the entire data set at once, we could break it into multiple batches and\n",
    "process them sequentially in time. Streaming was not implemented in the original Dask blog,\n",
    "as the DAG created by Dask does not support streaming use case.\n",
    "\n",
    "In contrast, any pipeline created in Julius can be easily run in streaming mode with few\n",
    "line of code change. In the streaming mode, the `RuleDSL.fwddata!` method will be called\n",
    "multiple times with different streaming values. Therefore, we can easily implement a few\n",
    "`Atom` types that act as the the source, running average and cumulator, so that we can\n",
    "express rich logic and behaviors in stream processing. These atoms are generic, they can be\n",
    "readily re-used in other streaming use cases."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# records the streaming value x\n",
    "RuleDSL.@datom Record begin\n",
    "    values::Vector = []\n",
    "\n",
    "    function fwddata!(x::Any)\n",
    "        push!(values, x)\n",
    "    end\n",
    "end\n",
    "\n",
    "# computes the running average of all the value being streamed\n",
    "RuleDSL.@datom RunningAverage begin\n",
    "    sum::DataFrame = DataFrame()\n",
    "    cnt::Vector = [0]\n",
    "\n",
    "    function fwddata!(x::DataFrame)\n",
    "        if cnt[1] == 0\n",
    "            append!(sum, x)\n",
    "        else\n",
    "            sum .+= x\n",
    "        end\n",
    "\n",
    "        cnt[1] += 1\n",
    "        [ sum ./ cnt[1] ]\n",
    "    end\n",
    "end\n",
    "\n",
    "# sequentially return values as represented in batchsrc, for each fwddata! call\n",
    "# this only work for a source node, i.e., NodeRef without any further dependencies\n",
    "RuleDSL.@datom StreamSrc begin\n",
    "    config::RuleDSL.Config\n",
    "    batchsrc::Vector{RuleDSL.NodeRef}\n",
    "    idx::Vector = [0]\n",
    "\n",
    "    function fwddata!()\n",
    "        thissrc = batchsrc[idx[1] % length(batchsrc) + 1]\n",
    "        atom = RuleDSL.calcop(config, thissrc)\n",
    "        idx[1] += 1\n",
    "        RuleDSL.fwddata!(atom)\n",
    "    end\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then add a few generic high level rules to connect these Atoms for streaming source,\n",
    "running average and cumulator:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "RuleDSL.@addrules ml begin\n",
    "    streamsrc(refs::Vector{RuleDSL.NodeRef}) = StreamSrc[RuleDSL.@config, refs]()\n",
    "    runningaverage(ref::RuleDSL.NodeRef) = RunningAverage(ref...)\n",
    "    record(ref::RuleDSL.NodeRef) = Record(ref...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "You might be curious of how the ML performance improves as more training data is added in\n",
    "the bagging process. The following is a streaming pipeline that can answer this kind\n",
    "of questions. The streaming ML pipeline receives individual training data: for\n",
    "each of which a new ML model is trained, then its inference is used to compute a running\n",
    "average of model inferences. The ML scores computed from the running average inference are\n",
    "then recorded, which shows how AUC score improves with more training data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "stream_ddf = DataScience.DDataFrame(train_data_file, blocksize=\"2 MB\")\n",
    "stream_batches = stream_ddf.chunks\n",
    "\n",
    "stream_src = RuleDSL.@ref ml.streamsrc(stream_batches)\n",
    "down_stream = RuleDSL.@ref ml.downsample(stream_src, target, sampleratio)\n",
    "\n",
    "# change the mapper to use the streaming data source for training\n",
    "streammapper = RuleDSL.@ref ml.batchpred(model, options, [down_stream], target)\n",
    "\n",
    "# the shuffler/reducer remains the same as before\n",
    "shuffler = RuleDSL.@ref mr.shuffler(first, 3)\n",
    "reducer = RuleDSL.@ref mr.reducer(vcat)\n",
    "\n",
    "streampred = RuleDSL.@ref mr.mapreduce(test_batches, streammapper, shuffler, reducer, valcat)\n",
    "\n",
    "# we use the running average to compute the ML score\n",
    "streamprob = RuleDSL.@ref ml.select(streampred, :proba)\n",
    "streamprobavg = RuleDSL.@ref ml.runningaverage(streamprob)\n",
    "streamscore = RuleDSL.@ref ml.score(RuleDSL.@ref(ml.select(streampred, :isFraud)), streamprobavg)\n",
    "\n",
    "# finally we record all the ML score history\n",
    "streamrecord = RuleDSL.@ref ml.record(streamscore)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "# create a local graph with the default pipeline\n",
    "gs7 = RuleDSL.createlocalgraph(config, RuleDSL.GenericData(), Set([streamrecord]));\n",
    "\n",
    "# this single line of code turns the regular batch pipeline into a streaming pipeline,\n",
    "# by specifying the source and sink of the stream processing\n",
    "RuleDSL.initstream!(gs7, Set(hash(stream_src)), Set(hash(streamrecord)));"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "svg = GraphIO.postlocalgraph(gss, gs7, port, true; key=\"stream\");\n",
    "display(\"image/svg+xml\", svg)\n",
    "\n",
    "# stream all the training batches data through\n",
    "RuleDSL.pushpullcalc!(gs7, length(stream_batches))\n",
    "\n",
    "# stop any further streaming, and persist the state\n",
    "RuleDSL.stopstream!(gs7);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The recorded history of ML score clearly shows the improvements in model inference, it is\n",
    "quite interesting to see how much the quality of inference improves with more training data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.getdata(gs7, hash(streamrecord))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Julius streaming is fully pipelined, in that each node in the graph processes a different\n",
    "input data batch simultaneously, which is a lot faster than the mini-batching approach in\n",
    "Spark. Julius streaming also works for distributed graphs across multiple computers, again\n",
    "without any code changes in RuleDSL or Atoms, the developer just need to make a different\n",
    "API call for distributed streaming."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "gs = GraphVM.RemoteGraphProxy(config, my_domain => remoteport, balancer, GraphVM.GenericData())\n",
    "GraphVM.rpccall(gs, GraphVM.workerstatus)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs begin\n",
    "\n",
    "    RuleDSL.@datom Record begin\n",
    "        values::Vector = []\n",
    "\n",
    "        function fwddata!(x::Any)\n",
    "            push!(values, x)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    RuleDSL.@datom RunningAverage begin\n",
    "        sum::DataFrame = DataFrame()\n",
    "        cnt::Vector = [0]\n",
    "\n",
    "        function fwddata!(x::DataFrame)\n",
    "            if cnt[1] == 0\n",
    "                append!(sum, x)\n",
    "            else\n",
    "                sum .+= x\n",
    "            end\n",
    "\n",
    "            cnt[1] += 1\n",
    "            [ sum ./ cnt[1] ]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    RuleDSL.@datom StreamSrc begin\n",
    "        config::RuleDSL.Config\n",
    "        batchsrc::Vector{RuleDSL.NodeRef}\n",
    "        idx::Vector = [0]\n",
    "\n",
    "        function fwddata!()\n",
    "            thissrc = batchsrc[idx[1] % length(batchsrc) + 1]\n",
    "            atom = RuleDSL.calcop(config, thissrc)\n",
    "            idx[1] += 1\n",
    "            RuleDSL.fwddata!(atom)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs ml begin\n",
    "    streamsrc(refs::Vector{RuleDSL.NodeRef})=StreamSrc[@config, refs]()\n",
    "    runningaverage(ref::RuleDSL.NodeRef)=RunningAverage(ref...)\n",
    "    record(ref::RuleDSL.NodeRef)=Record(ref...)\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "\n",
    "# create a regular batch piepline\n",
    "GraphVM.createremotegraph(gs, Set([streamrecord]), Set([:bagpred, :splitbykey, :reducer]))\n",
    "\n",
    "# turns it into streaming mode by specifying data source and sink\n",
    "GraphVM.initstream!(gs, UInt(0), Set(hash(stream_src)), Set(hash(streamrecord)))\n",
    "\n",
    "# stream data through\n",
    "RuleDSL.pushpullcalc!(gs, Set(UInt(0)), length(stream_batches))\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "# finalize, no longer accept future streaming data\n",
    "RuleDSL.stopstream!(gs);\n",
    "\n",
    "svg = GraphIO.postremotegraph(gs, remoteport, true);\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can retrieve the distributed streaming results:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.rpccall(gs, :onnode, :getdata, UInt[0], hash(streamrecord))"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "This tutorial shows how to productionize a ML model by adding batching and\n",
    "distribution capabilities in Julius step by step. It only takes two additional rules\n",
    "`ml.bagpred`, `ml.batchpred` and one additional function `valcat` to turn a simple ML\n",
    "pipeline into a doubly batched and fully distributed ML pipeline. This is astonishing\n",
    "considering the fact the doubly batched pipeline is quite complex with 125 nodes, which is\n",
    "a big increase from the 13 nodes in the original simple ML pipeline.\n",
    "\n",
    "We explained in section 3 that Julius graph distribution and execution is much more\n",
    "efficient than Dask or Dagger.jl, because Julius only needs to communicate a few key nodes to\n",
    "the workers.\n",
    "\n",
    "Using Julius, it is effortless to move a ML model from development to production, since it\n",
    "requires no code change and distribution is fully automatic. In comparison, to productionize\n",
    "a ML model using Dask, developers have to modify the code base heavily and manually using\n",
    "Dask specific API, then they have to go through extensive testing and performance tuning,\n",
    "as shown in the original Dask blog.\n",
    "\n",
    "Furthermore, Julius' web UI offers much easier and more intuitive visualization and\n",
    "navigation of data, logic and distribution in the distributed graph, with every intermediate\n",
    "results full visible to the user. In comparison, it is quite difficult to access\n",
    "intermediate data and distribution results from Dask, as individual tasks in Dask are\n",
    "transient, their states are not persisted. Yes, one can manually record and\n",
    "persist intermediate results from Dask, but that requires additional coding and efforts.\n",
    "\n",
    "Julius is also flexible, the ML pipeline can run in batch, streaming or distributed modes\n",
    "without additional coding. Dask, on the other hand, only support batch processing, but not\n",
    "the streaming use cases."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
