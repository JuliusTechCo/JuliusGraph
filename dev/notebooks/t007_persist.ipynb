{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tutorial 7: MLOps: ML Experiment Tracking and Persisting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to use this tutorial\n",
    "\n",
    "  * Select \"run all cells\" on this notebook from the Run menu in Jupyter notebook or Jupyter\n",
    "    lab. This step will produce intermediate data output and charts.\n",
    "  * Some cells print out a url, which you can click on and bring up an interactive web UI to\n",
    "    visualize the graph data.\n",
    "  * In the unlikely event that the notebook becomes irresponsive, you can try \"Restart\n",
    "    Kernel\" from the Kernel menu, then run individual cells one by one using `Shift+Enter`.\n",
    "  * Some tutorials use local clusters consisting of multiple processes to mimic the effects\n",
    "    of graph distribution over a remote cluster. By default, these local clusters\n",
    "    automatically stop after idling for 15 minutes to conserve CPU and memory resources. You\n",
    "    will need to rerun the entire notebook if your local cluster stopped due to inactivity.\n",
    "  * Additional resources (video demos & blogs) are available at http://juliustech.co.\n",
    "  * To report any issues, get help or request features, please raise an issue at\n",
    "    https://github.com/JuliusTechCo/JuliusGraph/issues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Introduction\n",
    "\n",
    "ML model experiment tracking is a common challenge for datas scientists and engineers. Once\n",
    "in a while, we hear the following kind of story:\n",
    "\n",
    "”My team spent months to train a massive ML model, we got exellent results at some point. But\n",
    "unfortunately we can't reproduce it any more, a number of things have changed, including\n",
    "data, underlying python library versions and hyperparameters, we are just not sure what\n",
    "combination might have worked ...\".\n",
    "\n",
    "Such story highlights the challenge in ML experment tracking, in order to reproduce a past ML\n",
    "run exactly, three things has to be persisted and recovered,\n",
    "1. runtime environment, including hadware, OS, software libraries etc\n",
    "2. input data\n",
    "3. the entire code, parameters and configurations, to be able to re-build the entire\n",
    "data/analytical pipeline\n",
    "\n",
    "Experiment tracking becomes much more challenging if the model runs on a\n",
    "distributed environment with many computers.  To guarantee reproducibility, each ML experiment\n",
    "should run from a fresh environment, otherwise the data, setting or environment might\n",
    "change between runs. For example, some data files could be added or modified as part of the\n",
    "runs. However, a complete refrehs of a complex distributed data and analytical pipeline\n",
    "is often out of the question in pracitice, as it consists many software components, parameters\n",
    "and configurations. This is why most existing experiment tracking solutions only persist part\n",
    "of the pipeline that are most relevant for the ML models. The downside of this\n",
    "approach is that the stored ML runs might fail to recover the exact results.\n",
    "\n",
    "Leveraging its distributed graph computing engine, Julius offer an experiment tracking solution\n",
    "that can persist and recover an entire distributed data and analytical pipeline, as well as\n",
    "the full runtime data and environment. We belive Julius is the only solution on the market\n",
    "with such capabilities.\n",
    "\n",
    "Julius persists model experiment with its entire data & analytical pipeline in the following\n",
    "simple steps:\n",
    "1. spin up a fresh virtual distributed environment, this only takes a few seconds\n",
    "2. run the ML experiment and then record the entire session on the Julius server side, and\n",
    "persist the recorded session onto long term storage. The recorded session contains the\n",
    "step by step instruction to recreate the entire runtime environment, including the entire\n",
    "data and distributed pipeline to recover the exact state of the experiment.\n",
    "3. the recorded ML experiement can be easily recovered by replaying it on another\n",
    "fresh environment.\n",
    "\n",
    "In this notebook, we follow a typical work flow of a data scientist to show Julius' experiment\n",
    "traking capabilities. We use a ML fraud detection model from a previous tutorial as an example.\n",
    "Readers are referred to the \"bagging\" tutorial for more details on the model itself.\n",
    "\n",
    "## 1. Model Development & Experiment\n",
    "\n",
    "Data scientists usually develop ML models by running experiments interactively\n",
    "in a Jupyter notebook. This section shows the definition and pipeline of a distributed ML\n",
    "model."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "using AtomExt\n",
    "using DataFrames, DataScience, StatsBase, Random\n",
    "\n",
    "newfunctions = quote\n",
    "    function downsample(ycol::Symbol, frac::Float64, df::DataFrame)\n",
    "        positive = DataFrames.filter(row -> isequal(row[ycol], true), df)\n",
    "        negative =  DataFrames.filter(row -> isequal(row[ycol], false), df)\n",
    "        dspositive = positive[sample(1:nrow(positive), round(Int, frac * nrow(positive)), replace=false), :]\n",
    "        dsnegative = negative[sample(1:nrow(negative), round(Int, frac * nrow(negative)), replace=false), :]\n",
    "        merged = vcat(dspositive, dsnegative)\n",
    "        merged[shuffle(1:nrow(merged)), :]\n",
    "    end\n",
    "\n",
    "    function valcat(xs::Vector...)\n",
    "        agg = DataFrame()\n",
    "        for (k, v) in vcat(xs...)\n",
    "            agg = vcat(agg, v)\n",
    "        end\n",
    "        agg\n",
    "    end\n",
    "\n",
    "    function dfmean(dfs::DataFrame...)\n",
    "        df = reduce(.+, dfs)\n",
    "        df ./ (length(dfs))\n",
    "    end\n",
    "end\n",
    "\n",
    "newrules = quote\n",
    "    select(ref::RuleDSL.NodeRef, cols::Any; label=\"$(isa(cols, InvertedIndex) ? \"col != $(cols.skip)\" : \"col == $(cols)\")\") =\n",
    "        DataScience.ApplyFn[x::DataFrame->DataFrames.select(x, cols; copycols=false)](ref...)\n",
    "\n",
    "    classifiertrain(model::Val{:ExtraTreesClassifier}, options::Dict, trainxs::RuleDSL.NodeRef, trainy::RuleDSL.NodeRef; label=\"$model train\")=DataScience.PyTrain[\"sklearn.ensemble.ExtraTreesClassifier\", options](trainxs..., trainy...)\n",
    "    classify(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, testx::RuleDSL.NodeRef; label=\"$model inference\")=begin\n",
    "        train_data_X = RuleDSL.@ref ml.select(train_data, Not(target))\n",
    "        train_data_y = RuleDSL.@ref ml.select(train_data, target)\n",
    "        trained = RuleDSL.@ref ml.classifiertrain(model, options,  train_data_X, train_data_y )\n",
    "        DataScience.PyPredict(trained..., testx...)\n",
    "    end\n",
    "\n",
    "    classifyprob(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, test_data::RuleDSL.NodeRef; label=\"prob\")=begin\n",
    "        testx = RuleDSL.@ref ml.select(test_data, Not(target))\n",
    "        DataScience.ApplyFn[x::DataFrame->DataFrames.select(x, :proba; copycols=false)](classify(train_data, target, model, options, testx))\n",
    "    end\n",
    "\n",
    "    score(realized::RuleDSL.NodeRef, probs::RuleDSL.NodeRef)=DataScience.PyScore(realized..., probs...)\n",
    "\n",
    "    downsample(raw::RuleDSL.NodeRef, ycol::Symbol, frac::Float64)=DataScience.ApplyFn[Main.downsample, ycol, frac](raw...)\n",
    "\n",
    "    bagpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) =\n",
    "        DataScience.ApplyFn[dfmean](RuleDSL.@ref((ml.classifyprob(b, target, model, options, test) for b = train_batches))...)\n",
    "\n",
    "    batchpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) =\n",
    "        DataScience.ApplyFn[(ind, prob)->[hash(test) => hcat(ind, prob)]](select(test, target), bagpred(test, model, options, train_batches, target))\n",
    "end;"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "we use existing rules in ds namespace to read CSV files from a shared drive"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "train_data_file = joinpath(@__DIR__, \"../data/train_fraud.csv\")\n",
    "test_data_file = joinpath(@__DIR__, \"../data/test_fraud.csv\")\n",
    "train_data = RuleDSL.@ref ds.csvsrc(train_data_file, true; label=\"train data\")\n",
    "test_data  = RuleDSL.@ref ds.csvsrc(test_data_file, true; label=\"test data\")\n",
    "\n",
    "target = :isFraud\n",
    "model = Val(:ExtraTreesClassifier)\n",
    "options = Dict(:n_estimators => 10, :min_samples_leaf => 10)\n",
    "\n",
    "test_data_y = RuleDSL.@ref ml.select(test_data, target)\n",
    "\n",
    "sampleratio = 0.05\n",
    "train_ddf = DataScience.DDataFrame(train_data_file, blocksize=\"5 MB\")\n",
    "train_batches = train_ddf.chunks\n",
    "down_batches = RuleDSL.@ref(ml.downsample(b, target, sampleratio) for b in train_batches)\n",
    "\n",
    "test_ddf = DataScience.DDataFrame(test_data_file, blocksize=\"3.5 MB\")\n",
    "test_batches = test_ddf.chunks\n",
    "\n",
    "mapper = RuleDSL.@ref ml.batchpred(model, options, down_batches, target)\n",
    "shuffler = RuleDSL.@ref mr.shuffler(first, 3)\n",
    "reducer = RuleDSL.@ref mr.reducer(vcat)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model now runs with good results on a cluster for model development, as shown below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphEngine: RuleDSL, GraphVM\n",
    "\n",
    "config = RuleDSL.newconfig(RuleDSL.Config(), :project => \"MapReduce\")\n",
    "balancer = GraphVM.GlobalUnique()\n",
    "my_domain = GraphVM.mydomain()\n",
    "remoteport = GraphVM.drawdataport();"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.startlocalmasterservice(remoteport, 3)\n",
    "gs = GraphVM.RemoteGraphProxy(config, my_domain => remoteport, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs begin\n",
    "    using GraphEngine: RuleDSL, GraphVM\n",
    "    using AtomExt, GraphIO\n",
    "    using DataFrames, DataScience, StatsBase, Random\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@remote_eval gs $newfunctions\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs ml $newrules\n",
    "\n",
    "mrpred = RuleDSL.@ref mr.mapreduce(test_batches, mapper, shuffler, reducer, Main.valcat)\n",
    "mrscore = RuleDSL.@ref ml.score(RuleDSL.@ref(ml.select(mrpred, :isFraud)), RuleDSL.@ref(ml.select(mrpred, :proba)))\n",
    "\n",
    "alljobs, ds = RuleDSL.jobdeps(config, [mrscore], Set([:classifiertrain, :splitbykey, :reducer]));\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "GraphVM.initgraph!(gs)\n",
    "GraphVM.dispatchjobs!(gs, alljobs, 1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using GraphIO\n",
    "\n",
    "GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));\n",
    "svg = GraphIO.postremotegraph(gs, remoteport);\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Record a ML Experiment\n",
    "\n",
    "Now the data scientists is happy with the results, and want to persist the experiment so that\n",
    "it can be reproduced later. The data scientits have made a number of choices in this ML model\n",
    "run, including data sources, configurations, choice of model, and hyper parameters etc. Some\n",
    "experiment tracking tools are based on saving notebooks, however that is not an adequate and\n",
    "reliable solution, as some data or variables were not captured by the code in the notebook. For\n",
    "example, the developer might have read data from a local file, or rely upon the state or data\n",
    "of a remote server. Under those circumstances, just saving the notebook is not adequate\n",
    "to recover the ML Run.\n",
    "\n",
    "Julius took a different approach, instead of saving the notebook on the client side, we\n",
    "persist the entire state on the server side. This server side persisting process is easy\n",
    "and seamless. We first start a fresh virtual cluster at a new port, using the same docker image\n",
    "used by the development environment."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "remoteport2 = GraphVM.drawdataport()\n",
    "\n",
    "GraphVM.startlocalmasterservice(remoteport2, 3)\n",
    "gs2 = GraphVM.RemoteGraphProxy(config, my_domain => remoteport2, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs2)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line enables recording on this new cluster, all the subsequent actions will\n",
    "be recorded on the serverside."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.rpccall(gs2, :clearrecording!)\n",
    "GraphVM.rpccall(gs2, :setrecording!, true);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now re-run the same ML model on this fresh server, existing local variables can be re-used\n",
    "without change to recreate the same data/analytica pipeline on the server. Only a\n",
    "few lines of codes are needed for server side recording, as shown below:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.@remote_eval gs2 begin\n",
    "    using GraphEngine: RuleDSL, GraphVM\n",
    "    using AtomExt, GraphIO\n",
    "    using DataFrames, DataScience, StatsBase, Random\n",
    "end\n",
    "\n",
    "GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@remote_eval gs2 $newfunctions\n",
    "GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));\n",
    "\n",
    "GraphVM.@addrules gs2 ml $newrules\n",
    "\n",
    "GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));\n",
    "GraphVM.initgraph!(gs2)\n",
    "GraphVM.dispatchjobs!(gs2, alljobs, 1);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can retrieve the recordeing saved on the server side for this ML run, which is an\n",
    "extremely compact representation of the entire run, including all the data and analytical\n",
    "logic to recreate the distributed pipeline. This recording can be persisted on long term\n",
    "storage like AWS S3. The version of docker container being used can also be persisted\n",
    "along with the recording. The docker container captures the exact and complete run time\n",
    "environment."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));\n",
    "records = GraphVM.rpccall(gs2, :getrecording);\n",
    "\n",
    "# terminate the recording cluster\n",
    "GraphVM.rpccall(gs2, :endcluster);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Reproduce a ML Experiment\n",
    "From the recording, the entire distributed pipeline can be easily recreated at a later time.\n",
    "To do so, we first spin up a fresh cluster using the same version of docker container, we\n",
    "then replay the recording on this new server. It only take a single line of code to recover\n",
    "the stored pipeline:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "remoteport3 = GraphVM.drawdataport()\n",
    "GraphVM.startlocalmasterservice(remoteport3, 3)\n",
    "gs3 = GraphVM.RemoteGraphProxy(config, my_domain => remoteport3, balancer, GraphVM.GenericData())\n",
    "GraphVM.wait4clusterinit(gs3)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.rpccall(gs3, :replayrecording, records)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now the results are ready to be inspected:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "GraphVM.waitcheckstatus(gs3, RuleDSL.getconfig(config, :project));\n",
    "svg = GraphIO.postremotegraph(gs3, remoteport3);\n",
    "display(\"image/svg+xml\", svg)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
