<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>2 Machine Learning · Julius GraphEngine Tutorials</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/theme.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Julius GraphEngine Tutorials</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Introduction</a></li><li><a class="tocitem" href="t001_quickstart.html">1 Quick Start</a></li><li class="is-active"><a class="tocitem" href="t002_titanic.html">2 Machine Learning</a><ul class="internal"><li><a class="tocitem" href="#How-to-use-this-tutorial-1"><span>How to use this tutorial</span></a></li><li><a class="tocitem" href="#.-Introduction-1"><span>0. Introduction</span></a></li><li><a class="tocitem" href="#.-Data-Processing-1"><span>1. Data Processing</span></a></li><li><a class="tocitem" href="#.-Experiment-with-multiple-ML-models-1"><span>2. Experiment with multiple ML models</span></a></li><li><a class="tocitem" href="#.-Hyperparameter-Tuning-1"><span>3. Hyperparameter Tuning</span></a></li><li><a class="tocitem" href="#.-Conclusions-1"><span>4. Conclusions</span></a></li></ul></li><li><a class="tocitem" href="t003_mapreduce.html">3 MapReduce</a></li><li><a class="tocitem" href="t004_bagging.html">4 Distributed Machine Learning Pipeline</a></li><li><a class="tocitem" href="t005_aad.html">5 Adjoint Algorithmic Differentiation (AAD)</a></li><li><a class="tocitem" href="t006_advanced.html">6 Advanced Features</a></li><li><a class="tocitem" href="t007_persist.html">7 MLOps: ML Experiment Tracking and Persisting</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="t002_titanic.html">2 Machine Learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="t002_titanic.html">2 Machine Learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliusTechCo/Tutorials/blob/main/src/titanic.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial-2:-Machine-Learning-1"><a class="docs-heading-anchor" href="#Tutorial-2:-Machine-Learning-1">Tutorial 2: Machine Learning</a><a class="docs-heading-anchor-permalink" href="#Tutorial-2:-Machine-Learning-1" title="Permalink"></a></h1><h2 id="How-to-use-this-tutorial-1"><a class="docs-heading-anchor" href="#How-to-use-this-tutorial-1">How to use this tutorial</a><a class="docs-heading-anchor-permalink" href="#How-to-use-this-tutorial-1" title="Permalink"></a></h2><ul><li>Select &quot;run all cells&quot; on this notebook from the Run menu in Jupyter notebook or Jupyter lab. This step will produce intermediate data output and charts.</li><li>Some cells print out a url, which you can click on and bring up an interactive web UI to visualize the graph data.</li><li>In the unlikely event that the notebook becomes irresponsive, you can try &quot;Restart Kernel&quot; from the Kernel menu, then run individual cells one by one using <code>Shift+Enter</code>.</li><li>Some tutorials use local clusters consisting of multiple processes to mimic the effects of graph distribution over a remote cluster. By default, these local clusters automatically stop after idling for 15 minutes to conserve CPU and memory resources. You will need to rerun the entire notebook if your local cluster stopped due to inactivity.</li><li>Additional resources (video demos &amp; blogs) are available at http://juliustech.co.</li><li>To report any issues, get help or request features, please raise an issue at https://github.com/JuliusTechCo/JuliusGraph/issues.</li></ul><h2 id=".-Introduction-1"><a class="docs-heading-anchor" href="#.-Introduction-1">0. Introduction</a><a class="docs-heading-anchor-permalink" href="#.-Introduction-1" title="Permalink"></a></h2><p>This Tutorial shows how to use Julius Graph Engine to set up the training and validation of a machine learning model. We will compare several different ML models to predict (or postdict) the survival of Titanic passengers using the classic Titanic dataset.</p><p>Julius  distribution includes a <code>DataScience</code> package, which contains a rich set of functionalities for data sourcing, cleansing, and machine learning. In this tutorial, we will show how to use the <code>DataScience</code> package to quickly build a transparent and sophisticated ML pipeline. This tutorial broadly follow the steps of a data scientist when building a new ML model.</p><h2 id=".-Data-Processing-1"><a class="docs-heading-anchor" href="#.-Data-Processing-1">1. Data Processing</a><a class="docs-heading-anchor-permalink" href="#.-Data-Processing-1" title="Permalink"></a></h2><h3 id=".1-Data-Sourcing-and-Visualization-1"><a class="docs-heading-anchor" href="#.1-Data-Sourcing-and-Visualization-1">1.1 Data Sourcing &amp; Visualization</a><a class="docs-heading-anchor-permalink" href="#.1-Data-Sourcing-and-Visualization-1" title="Permalink"></a></h3><p>A data scientist usually starts their project by exploring and visualizing data of various sources. Julius provides a rich set of connectors to multiple data sources and formats, such as CSV, web url, relational Databases, various NoSQL Databases, etc. Julius also offers many data visualization tools in its interactive web UI.</p><p>We start by including necessary Julia and Julius packages and set up some basic configurations.</p><pre><code class="language-julia"># Julia packages
using Base.CoreLogging
using DataFrames, Statistics

# Julius Packages
using GraphEngine: RuleDSL, GraphVM
using DataScience, AtomExt, GraphIO

# turn off informational logging output
disable_logging(CoreLogging.Info)

# extend the number of displayed columns in Jupyter notebooks
ENV[&quot;COLUMNS&quot;] = 100;

# the project is used for web UI display
config = RuleDSL.Config(:project =&gt; &quot;Titanic&quot;);</code></pre><p>The dataset can be loaded from a url or a local CSV file via rules defined in the <code>ds</code> namespace, which is provided in the <code>DataScience</code> package. The line commented out is a rule to load the same data from a URL.</p><pre><code class="language-julia">rawsrc = RuleDSL.@ref ds.csvsrc(&quot;../data/titanic.csv&quot;, true; label=&quot;raw csv&quot;);
# rawsrc = RuleDSL.@ref ds.urlsrc(&quot;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&quot;, true; label=&quot;raw url&quot;)</code></pre><pre><code class="language-none">ds:csvsrc/raw csv</code></pre><p>The very first thing a data scientits do is often to get a summary of the dataset. The follow cell shows how it can be done using the <code>ds.datasummary</code> rule in the <code>DataScience</code> package.</p><pre><code class="language-julia">rawsummary = RuleDSL.@ref ds.datasummary(rawsrc; label=&quot;data summary&quot;)

gs1 = GraphVM.createlocalgraph(config, RuleDSL.GenericData());
GraphVM.calcfwd!(gs1, Set([rawsummary]));</code></pre><p>The data summary results can be retrieved using the <code>GraphVM.getdata</code> method. The data cached in individual graph nodes are all vectors, the last argument <code>1</code> is optional, it selects a given element from the data vector of the node. Without it, the entire vector will be returned.</p><pre><code class="language-julia">RuleDSL.getdata(gs1, rawsummary, 1)</code></pre><div class="data-frame"><p>12 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="Type">Type</th></tr></thead><tbody><tr><th>1</th><td>PassengerId</td><td>446.0</td><td>1</td><td>446.0</td><td>891</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>Name</td><td></td><td>Abbing, Mr. Anthony</td><td></td><td>van Melkebeke, Mr. Philemon</td><td>0</td><td>String</td></tr><tr><th>5</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td><td>0</td><td>String7</td></tr><tr><th>6</th><td>Age</td><td>29.6991</td><td>0.42</td><td>28.0</td><td>80.0</td><td>177</td><td>Union{Missing, Float64}</td></tr><tr><th>7</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>Ticket</td><td></td><td>110152</td><td></td><td>WE/P 5735</td><td>0</td><td>String31</td></tr><tr><th>10</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>Cabin</td><td></td><td>A10</td><td></td><td>T</td><td>687</td><td>Union{Missing, String15}</td></tr><tr><th>12</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td><td>2</td><td>Union{Missing, String1}</td></tr></tbody></table></div><h3 id=".2-Data-Cleansing-and-Imputation-1"><a class="docs-heading-anchor" href="#.2-Data-Cleansing-and-Imputation-1">1.2 Data Cleansing &amp; Imputation</a><a class="docs-heading-anchor-permalink" href="#.2-Data-Cleansing-and-Imputation-1" title="Permalink"></a></h3><p>We observe that some columns in the raw data set have <code>missing</code> values. Data imputation and cleansing is the next step for the data scientists. Julius&#39; <code>DataScience</code> library provides common data imputation methods, which can be easily invoked using the <code>ds.fillmissing</code> rule with the desired imputation method for each missing field, i.e., we use median value of Age of all passenger for any missing Ages, and the mode value (which is true) for any missing Embarked.</p><p>After data imputation, we recompute the data summary, showing all the the <code>missing</code> values for both <code>Age</code> and <code>Embarked</code> features have been populated.</p><pre><code class="language-julia">cleansrc = RuleDSL.@ref ds.fillmissing(
    rawsrc, Dict(:Age =&gt; :median, :Embarked =&gt; :mode); label=&quot;imputation&quot;
);

cleansummary = RuleDSL.@ref ds.datasummary(cleansrc; label=&quot;clean summary&quot;)
GraphVM.calcfwd!(gs1, Set([cleansummary]))
RuleDSL.getdata(gs1, cleansummary, 1)</code></pre><div class="data-frame"><p>12 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="Type">Type</th></tr></thead><tbody><tr><th>1</th><td>PassengerId</td><td>446.0</td><td>1</td><td>446.0</td><td>891</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>Name</td><td></td><td>Abbing, Mr. Anthony</td><td></td><td>van Melkebeke, Mr. Philemon</td><td>0</td><td>String</td></tr><tr><th>5</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td><td>0</td><td>String7</td></tr><tr><th>6</th><td>Age</td><td>29.3616</td><td>0.42</td><td>28.0</td><td>80.0</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>Ticket</td><td></td><td>110152</td><td></td><td>WE/P 5735</td><td>0</td><td>String31</td></tr><tr><th>10</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>Cabin</td><td></td><td>A10</td><td></td><td>T</td><td>687</td><td>Union{Missing, String15}</td></tr><tr><th>12</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td><td>0</td><td>String1</td></tr></tbody></table></div><h3 id=".3-Feature-Engineering-1"><a class="docs-heading-anchor" href="#.3-Feature-Engineering-1">1.3 Feature Engineering</a><a class="docs-heading-anchor-permalink" href="#.3-Feature-Engineering-1" title="Permalink"></a></h3><p>Once the data scientist is happy with the results of data cleansing and imputation, the next step is feature engineering, which is to add or remove columns from the data set.</p><p>In the Titanic data set, we want to drop the columns that should have no correlation to a passenger&#39;s suvival outcome, such as a passenger&#39;s ticket id, name and IDs. Including irrelavant data in the training of a ML model may degrade its performance. The Cabin also has to be dropped because it has too many missing values to be useful.</p><p>We also create two additional features: 1) the z value of the ticket fare, which is the difference of a passenger&#39;s ticket price from the mean price in the unit of standard deviation of the ticket prices; 2) the total number of relatives onboard for a given passenger, which is the sum of the number of siblings (:SibSp) and parents/children (:Parch) onboard.</p><p>Feature engineering is supported generically by a rule <code>ds.coltransform</code> in the <code>DataScience</code> package. The following cell shows is usage. The feature engineering can be easily entered as formulae operating on the columns (named by those variables start with <code>:</code>).</p><pre><code class="language-julia">newfeatures = quote
    :Zfare = (:Fare .- mean(:Fare)) ./ std(:Fare)
    :Relatives = :SibSp .+ :Parch
end

dropfeatures = [:Cabin, :Ticket, :PassengerId, :Name]
features = RuleDSL.@ref ds.coltransform(cleansrc, :feature, newfeatures, dropfeatures; label=&quot;feature eng&quot;)

featuresummary = RuleDSL.@ref ds.datasummary(features; label=&quot;feature summary&quot;)
GraphVM.calcfwd!(gs1, Set([featuresummary]));</code></pre><p>The data summary results after feature engineering is therefore:</p><pre><code class="language-julia">RuleDSL.getdata(gs1, featuresummary, 1)</code></pre><div class="data-frame"><p>10 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="DataType">DataType</th></tr></thead><tbody><tr><th>1</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td><td>0</td><td>String7</td></tr><tr><th>4</th><td>Age</td><td>29.3616</td><td>0.42</td><td>28.0</td><td>80.0</td><td>0</td><td>Float64</td></tr><tr><th>5</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td><td>0</td><td>Int64</td></tr><tr><th>6</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>7</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td><td>0</td><td>Float64</td></tr><tr><th>8</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td><td>0</td><td>String1</td></tr><tr><th>9</th><td>Zfare</td><td>-1.76938e-17</td><td>-0.648058</td><td>-0.35719</td><td>9.66174</td><td>0</td><td>Float64</td></tr><tr><th>10</th><td>Relatives</td><td>0.904602</td><td>0</td><td>0.0</td><td>10</td><td>0</td><td>Int64</td></tr></tbody></table></div><p>The entire data processing steps we performed so far can be visualized interactively in Julius convenient web UI by clicking the link below. All the intermediate data are accessilble from the web UI.</p><pre><code class="language-julia"># start data server for web UI
gss = Dict{String,RuleDSL.AbstractGraphState}()
port = GraphVM.drawdataport()
@async GraphVM.startresponder(gss, port)

svg = GraphIO.postlocalgraph(gss, gs1, port, true; key=&quot;data&quot;);
GraphIO.postsvg(svg, &quot;titanic_1.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7930_data
starting data service at port 7930
</code></pre><p align = "center">
<img src="../assets/titanic_1.svg" alt="" title="Data Cleansing"/>
</p>
<p align = "center">
Figure 1 - Data Cleansing
</p><h2 id=".-Experiment-with-multiple-ML-models-1"><a class="docs-heading-anchor" href="#.-Experiment-with-multiple-ML-models-1">2. Experiment with multiple ML models</a><a class="docs-heading-anchor-permalink" href="#.-Experiment-with-multiple-ML-models-1" title="Permalink"></a></h2><p>Once the data scientist is happy with the results of data cleansing, imputation and feature engineering, the next step is often to try multiple ML models and see how they perform on the data set.</p><p>Julius Graph Engine can interop with existing Python, Java, C++ and R libraries via the generic <code>Atom</code> interface, making tt seamless to access the rich set of ML models in these ecosystems.</p><p>For example, the following rules leverages the Python ML libraries, such as <code>sklearn</code> and <code>xgboost</code>, by using the <code>PyTrain</code> atom provided in the <code>DataScience</code> package. The first parameter of the <code>PyTrain</code> atom is the full name of the Python ML class to use. The second parameter is a Dictionary with the corresponding parameters/options/arguments of that ML class.</p><pre><code class="language-julia">@addrules ds begin
    classifiertrain(model::Val{:SVC}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.svm.SVC&quot;, options](traindat...)
    classifiertrain(model::Val{:DecisionTree}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.tree.DecisionTreeClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:RandomForest}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.ensemble.RandomForestClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:AdaBoost}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.ensemble.AdaBoostClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:MLPC}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.neural_network.MLPClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:GaussianNB}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.naive_bayes.GaussianNB&quot;, options](traindat...)
    classifiertrain(model::Val{:XGBoost}, options::Dict, traindat::NodeRef) = PyTrain[&quot;xgboost.XGBClassifier&quot;, options](traindat...)
    classifiertrain(model::Symbol, options::Dict, traindat::NodeRef; label = &quot;$model-train&quot;) = Alias(classifiertrain(val(model), options, traindat))
end</code></pre><p>We now proceed to train multiple ML models and compare their in-sample and out-sample performance using metrics, such as Gini. The ML models are trained to predict the survival probability of Titanic passengers. We first define the list of models we want to compare and their hyperparameters.</p><pre><code class="language-julia">models = [
    :DecisionTree =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :LogisticRegression =&gt; Dict(:solver =&gt; &quot;saga&quot;, :max_iter =&gt; 200),
    :AdaBoost =&gt; Dict(),
    :XGBoost =&gt; Dict(),
    :GradientBoost =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :RandomForest =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :GaussianNB =&gt; Dict(),
];</code></pre><p>The target variable name for ML prediction is given below, which is the survival outcome of passengers.</p><pre><code class="language-julia">yname = :Survived;</code></pre><p>To divide the input dataset for training and validation, we use the <code>randrowsel</code> rule from the <code>DataScience</code> package, which randomly select a portion of the input data as validation set, and the rest is used for training. The parameter <code>1/3</code> is the fraction of rows that are reserved for validation.</p><pre><code class="language-julia">valind = RuleDSL.@ref ds.randrowsel(cleansrc, 1 / 3);</code></pre><p><code>DataScience.ClassifierSpec</code> is a genereic <code>struct</code> that holds all the configurations for training and validating binary classifiers, such as those we have defined so far. It is more convenient and readable to pass the <code>DataScience.ClassifierSpec</code> object to a rule, than having to pass five separate parameters. The <code>DataScience.ClassifierSpec</code> can be used for any binary classifier problems on data sets. The last parameter to the <code>DataScience.ClassifierSpec</code> constructor is a tuple representing the feature engineering.</p><pre><code class="language-julia">cspec = DataScience.ClassifierSpec(models, cleansrc, yname, valind, (:feature, newfeatures, dropfeatures));</code></pre><p>Now we can proceed and use the <code>ds.classifiermetrics</code> rule, which is also part of <code>DataScience</code>, to compute in-sample and out-of-sample metrics for each model. This rule depends on the <code>ds.classifiertrain</code> rules defined above for accessing the python ML models.</p><pre><code class="language-julia">metrics = [:gini, :roc, :accuracyrate, :accuracygraph]
basem = RuleDSL.@ref ds.classifiermetrics(cspec, metrics)
gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())
@time GraphVM.calcfwd!(gs2, Set([basem]));</code></pre><pre><code class="language-none"> 36.270563 seconds (39.31 M allocations: 2.243 GiB, 2.72% gc time, 28.97% compilation time)
</code></pre><p>We can retrieve in-sample and out-sample performance metrics, for example, the GINIs:</p><pre><code class="language-julia">giniref = RuleDSL.@ref ds.classifiermetric(cspec, :gini)
gini = GraphVM.getdata(gs2, hash(giniref), 1)
ginidf = DataFrame(model=gini[:InSample][!, :Model], InSample_GINI=gini[:InSample][!, 2], OutSample_GINI=gini[:OutSample][!, 2])</code></pre><div class="data-frame"><p>7 rows × 3 columns</p><table class="data-frame"><thead><tr><th></th><th>model</th><th>InSample_GINI</th><th>OutSample_GINI</th></tr><tr><th></th><th title="String">String</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>DecisionTree</td><td>0.718352</td><td>0.643985</td></tr><tr><th>2</th><td>LogisticRegression</td><td>0.503204</td><td>0.4584</td></tr><tr><th>3</th><td>AdaBoost</td><td>0.832608</td><td>0.654584</td></tr><tr><th>4</th><td>XGBoost</td><td>0.992142</td><td>0.69433</td></tr><tr><th>5</th><td>GradientBoost</td><td>0.841746</td><td>0.675305</td></tr><tr><th>6</th><td>RandomForest</td><td>0.704748</td><td>0.629677</td></tr><tr><th>7</th><td>GaussianNB</td><td>0.685388</td><td>0.658135</td></tr></tbody></table></div><p>The entire data and logic from can be visualized by clicking on the URL below.</p><pre><code class="language-julia">svg = GraphIO.postlocalgraph(gss, gs2, port; key=&quot;ml&quot;);
GraphIO.postsvg(svg, &quot;titanic_2.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7930_ml
</code></pre><p align = "center">
<img src="../assets/titanic_2.svg" alt="" title="ML"/>
</p>
<p align = "center">
Figure 2 - Machine Learning
</p><p>The entire ML pipeline includes all the steps we have defined so far, including data sourcing, imputation, feature engineering, training of multiple ML models and the computation and reporting of performance metrics. A data scientists only need to invoke a few rules defined in <code>DataScience</code> package to construct this realistic ML pipeline, the total number of nodes in the graph is 83, as shown below.</p><pre><code class="language-julia">dg = GraphVM.mygraph(gs2)
println(length(dg._items))</code></pre><pre><code class="language-none">83
</code></pre><h2 id=".-Hyperparameter-Tuning-1"><a class="docs-heading-anchor" href="#.-Hyperparameter-Tuning-1">3. Hyperparameter Tuning</a><a class="docs-heading-anchor-permalink" href="#.-Hyperparameter-Tuning-1" title="Permalink"></a></h2><p>Once a data scientist narrowed down the choice of ML models to a few, the next step is to select the optimal hyperparameters for these candidate ML models.</p><p>Julius Graph Engin provides a generic rule <code>hypertune</code> for hyperparameter tuning of any ML model. This shows the power of high level rules, where a single hypertune rule can perform hyperparmeter tuning for any ML model.</p><p>For example, for a given machine learning model, we can select a range for a set of hyperparameters and easily perform a grid search and report the corresponding metric results:</p><pre><code class="language-julia">ht_1 = RuleDSL.@ref ds.hypertune(cspec, :XGBoost,       Dict(), :gini, :n_estimators =&gt; 50:50:200, :learning_rate    =&gt; .05:.05:.2);
ht_2 = RuleDSL.@ref ds.hypertune(cspec, :AdaBoost,      Dict(), :gini, :n_estimators =&gt; 50:50:200, :learning_rate    =&gt; .05:.05:.2);
ht_3 = RuleDSL.@ref ds.hypertune(cspec, :GradientBoost, Dict(), :gini, :n_estimators =&gt; 50:50:200, :min_samples_leaf =&gt; .05:.05:.2);
ht_4 = RuleDSL.@ref ds.hypertune(cspec, :RandomForest,  Dict(), :gini, :n_estimators =&gt; 50:50:200, :min_samples_leaf =&gt; .05:.05:.2);</code></pre><p>Additional search dimensions can be added to the <code>ds.hypertune</code> rule by appending additional pairs of hyperparameter =&gt; searchgrid to the end of rule parameter. We can then wrap all the hyperparameter searches in a single node for convenience by means of the <code>alias</code> rule wich uses the <code>Alias</code> atom:</p><pre><code class="language-julia">tunings = RuleDSL.@ref ds.alias([ht_1, ht_2, ht_3, ht_4]; label=&quot;Hyperparameter Tuning&quot;)</code></pre><pre><code class="language-none">ds:alias/Hyperparameter Tuning</code></pre><p>Now proceed with the computation of all the defined hyperparameter tunings:</p><pre><code class="language-julia">gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData());
@time GraphVM.calcfwd!(gs3, Set([tunings]));</code></pre><pre><code class="language-none"> 28.939420 seconds (11.80 M allocations: 804.371 MiB, 1.08% gc time, 17.86% compilation time)
</code></pre><p>The following cell shows the resulting insample and outsample GINI from the different hyperparametrs for GradientBoost:</p><pre><code class="language-julia">dat = GraphVM.getdata(gs3, hash(ht_3))
df = deepcopy(dat[1][:, 1:2])
df[!, :InSampleGINI] = dat[1][!, 3]
df[!, :OutSampleGINI] = dat[2][!, 3]
df</code></pre><div class="data-frame"><p>16 rows × 4 columns</p><table class="data-frame"><thead><tr><th></th><th>n_estimators</th><th>min_samples_leaf</th><th>InSampleGINI</th><th>OutSampleGINI</th></tr><tr><th></th><th title="Int64">Int64</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>50</td><td>0.05</td><td>0.828853</td><td>0.706074</td></tr><tr><th>2</th><td>50</td><td>0.1</td><td>0.791264</td><td>0.679376</td></tr><tr><th>3</th><td>50</td><td>0.15</td><td>0.768111</td><td>0.681152</td></tr><tr><th>4</th><td>50</td><td>0.2</td><td>0.728196</td><td>0.68072</td></tr><tr><th>5</th><td>200</td><td>0.05</td><td>0.902352</td><td>0.707851</td></tr><tr><th>6</th><td>200</td><td>0.1</td><td>0.868768</td><td>0.69892</td></tr><tr><th>7</th><td>200</td><td>0.15</td><td>0.838649</td><td>0.691573</td></tr><tr><th>8</th><td>200</td><td>0.2</td><td>0.783123</td><td>0.697623</td></tr><tr><th>9</th><td>100</td><td>0.05</td><td>0.868001</td><td>0.709724</td></tr><tr><th>10</th><td>100</td><td>0.1</td><td>0.829944</td><td>0.682545</td></tr><tr><th>11</th><td>100</td><td>0.15</td><td>0.807079</td><td>0.680624</td></tr><tr><th>12</th><td>100</td><td>0.2</td><td>0.751241</td><td>0.683553</td></tr><tr><th>13</th><td>150</td><td>0.05</td><td>0.89053</td><td>0.713902</td></tr><tr><th>14</th><td>150</td><td>0.1</td><td>0.853157</td><td>0.693157</td></tr><tr><th>15</th><td>150</td><td>0.15</td><td>0.825064</td><td>0.686963</td></tr><tr><th>16</th><td>150</td><td>0.2</td><td>0.766301</td><td>0.69407</td></tr></tbody></table></div><p>A data scientist has to exercise sound judgement in selecting the optimal hyperparameter set, which may have to balance multiple objectives. The parameter set with the maximum out-sample gini may not be the best choice. Often, it is better to choose the parameter set with similar in-sample and out-of-sample gini to minimize the chance of overfitting.</p><p>The details of hyperparameter search can be visualized by clicking the url below.</p><pre><code class="language-julia">svg = GraphIO.postlocalgraph(gss, gs3, port; key=&quot;hyper&quot;);
GraphIO.postsvg(svg, &quot;titanic_3.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7930_hyper
</code></pre><p align = "center">
<img src="../assets/titanic_3.svg" alt="" title="ML"/>
</p>
<p align = "center">
Figure 2 - Machine Learning
</p><h2 id=".-Conclusions-1"><a class="docs-heading-anchor" href="#.-Conclusions-1">4. Conclusions</a><a class="docs-heading-anchor-permalink" href="#.-Conclusions-1" title="Permalink"></a></h2><p>It only takes a few lines of code in Julius to build a sophisticated and ML pipeline, by leveraging the existing rules and atoms provided by the <code>DataScience</code> package.  Even though the titanic data set is quite small, the ML pipeline built in this tutorial are quite representative; it has all the essential elements of a real world ML piepline such as data cleansing, imputation, feature engineering, model performance monitoring and hyper parameter tuning.</p><p>The ML pipeline built by Julius offers full transparency and data lineage, allows data scientists to easily visualze and explore data in every intermediate step, all from Julius&#39; web UI. Julius also offers full data lineage and explanability, a data scientist can easily query and trace how a piece of data is sourced, modified and used throughout the entire ML pipeline, as every intermediate results are automatically cached by Julius Graph Engine.</p><p>In a next tutorial &quot;distributed ML pipeline&quot;, we will show how to deal with very large data set that does not fit into memory.</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="t001_quickstart.html">« 1 Quick Start</a><a class="docs-footer-nextpage" href="t003_mapreduce.html">3 MapReduce »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 29 March 2022 15:50">Tuesday 29 March 2022</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
