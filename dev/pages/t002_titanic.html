<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>2 Machine Learning · Julius GraphEngine Tutorials</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/theme.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Julius GraphEngine Tutorials</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Introduction</a></li><li><a class="tocitem" href="t001_quickstart.html">1 Quick Start</a></li><li class="is-active"><a class="tocitem" href="t002_titanic.html">2 Machine Learning</a><ul class="internal"><li><a class="tocitem" href="#How-to-use-this-tutorial-1"><span>How to use this tutorial</span></a></li><li><a class="tocitem" href="#Introduction-1"><span>Introduction</span></a></li><li><a class="tocitem" href="#Data-Exploration,-Visualization-and-Cleaning-1"><span>Data Exploration, Visualization and Cleaning</span></a></li><li><a class="tocitem" href="#Hyperparameter-Tuning-1"><span>Hyperparameter Tuning</span></a></li></ul></li><li><a class="tocitem" href="t003_mapreduce.html">3 MapReduce</a></li><li><a class="tocitem" href="t004_bagging.html">4 Distributed Machine Learning Pipeline</a></li><li><a class="tocitem" href="t005_aad.html">5 Adjoint Algorithmic Differentiation (AAD)</a></li><li><a class="tocitem" href="t006_advanced.html">6 Advanced Features</a></li><li><a class="tocitem" href="t007_persist.html">7 MLOps: ML Experiment Tracking and Persiting</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="t002_titanic.html">2 Machine Learning</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="t002_titanic.html">2 Machine Learning</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliusTechCo/Tutorials/blob/main/src/titanic.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial-2:-Machine-Learning-1"><a class="docs-heading-anchor" href="#Tutorial-2:-Machine-Learning-1">Tutorial 2: Machine Learning</a><a class="docs-heading-anchor-permalink" href="#Tutorial-2:-Machine-Learning-1" title="Permalink"></a></h1><h2 id="How-to-use-this-tutorial-1"><a class="docs-heading-anchor" href="#How-to-use-this-tutorial-1">How to use this tutorial</a><a class="docs-heading-anchor-permalink" href="#How-to-use-this-tutorial-1" title="Permalink"></a></h2><ul><li>Select &quot;run all cells&quot; on this notebook from the Run menu in Jupyter notebook or Jupyter</li></ul><p>lab. This step will produce intermediate data output and charts.</p><ul><li>Some cells print out a url, which you can click on and bring up an interactive web UI to visualize the graph data.</li><li>In the unlikely event that the notebook becomes irresponsive, you can try &quot;Restart Kernel&quot; from the Kernel menu, then run individual cells one by one using <code>Shift+Enter</code>.</li><li>Some tutorials use local clusters consisting of multiple processes to mimic the effects</li></ul><p>of graph distribution over a remote clusters. By default, these local clusters   automatically stop after idling for 15min to conserve CPU and memory resources. You will   need to rerun the entire notebook if your local cluster stopped due to inactivity.</p><ul><li>Additional resources (video demos &amp; blogs) are available at http://juliustech.co</li><li>To report any issues, get help or request features, please raise an issue at:</li></ul><p>https://github.com/JuliusTechCo/JuliusGraph/issues</p><h2 id="Introduction-1"><a class="docs-heading-anchor" href="#Introduction-1">Introduction</a><a class="docs-heading-anchor-permalink" href="#Introduction-1" title="Permalink"></a></h2><p>This Tutorial shows how to use Julius Graph Engine to set up the training and validation of a machine learning model. We implement the classic example of using several different ML models to predict (or postdict) the survival of Titanic passengers by using the well-known Titanic dataset.</p><h2 id="Data-Exploration,-Visualization-and-Cleaning-1"><a class="docs-heading-anchor" href="#Data-Exploration,-Visualization-and-Cleaning-1">Data Exploration, Visualization and Cleaning</a><a class="docs-heading-anchor-permalink" href="#Data-Exploration,-Visualization-and-Cleaning-1" title="Permalink"></a></h2><p>Data scientits can use Julius Graph Engine to quickly explore and visualize data from different sources. Julius provides connections to many data sources and formats, such as CSV, web url, relational Databases, various NoSQL Databases, etc.</p><p><code>DataScience</code> package is part of Julius distribution, it provides a rich set of rules and atoms for data sourcing, cleansing, and machine learning. In this notebook, we will show how the rules and atoms defined in <code>DataScience</code> to quickly build the ML pipeline for the Titanic data set.</p><p>The relevant rules from <code>DataScience</code> package for this tutorial are listed below for your reference.</p><pre><code class="language-julia">@addrules ds begin
    dfsrc(df::DataFrame) = DFSrc[df]()
    colsel(r::NodeRef, idx::Vector{Int}) = Index[idx](r...)
    csvsrc(filen::String, hashdr::Bool) = Alias(dfsrc(CSV.read(filen, DataFrame, header = hashdr)))
    colsplit(ref::NodeRef, idx::Int, yhdr::Set{Symbol}, ex::Set{Symbol}) = ColSplit[idx, yhdr, ex](ref...)
    trainvalsplit(ref::NodeRef, pctval::Float64) = RandRowSplit[pctval](ref...)
    fillmissing(ref::NodeRef, methods::Dict{Symbol,Symbol}) = FillMissing[methods](ref...)
    numerize(ref::NodeRef) = Numerize(ref...)
end</code></pre><p>We first include the dependent Julia and Julius packages and set up some basic configurations.</p><pre><code class="language-julia">using GraphEngine: RuleDSL, GraphVM
using Base.CoreLogging
using DataScience, AtomExt, GraphIO
using DataFrames

# turn off informational logging output
disable_logging(CoreLogging.Info)

# extend the number of displayed columns in Jupyter notebooks
ENV[&quot;COLUMNS&quot;] = 100</code></pre><pre><code class="language-none">100</code></pre><pre><code class="language-julia"># the project is used for web UI display
config = RuleDSL.Config(:project =&gt; &quot;Titanic&quot;);

# start data server for web UI
gss = Dict{String,RuleDSL.AbstractGraphState}()
port = GraphVM.drawdataport()
@async GraphVM.startresponder(gss, port)</code></pre><pre><code class="language-none">Task (runnable) @0x00007f8e63e70d30</code></pre><p>The dataset can be loaded by either directly using a url or by providing a CSV source file via rules defined in the <code>ds</code> namespace, which are defined in the <code>DataScience</code> package. The line commented out is a rule to load the same data from a URL.</p><pre><code class="language-julia">rawsrc = RuleDSL.@ref ds.csvsrc(&quot;../data/titanic.csv&quot;, true; label=&quot;raw csv&quot;)
# rawsrc = RuleDSL.@ref ds.urlsrc(&quot;https://raw.githubusercontent.com/agconti/kaggle-titanic/master/data/train.csv&quot;, true; label=&quot;raw url&quot;)</code></pre><pre><code class="language-none">ds:csvsrc/raw csv</code></pre><p>The very first thing a data scientits may want to do is to get a summary of the dataset. The follow cell shows how it can be done using the <code>ds.datasummary</code> rule in the <code>DataScience</code> package.</p><pre><code class="language-julia">rawsummary = RuleDSL.@ref ds.datasummary(rawsrc; label=&quot;data summary&quot;)

gs = GraphVM.createlocalgraph(config, RuleDSL.GenericData());
GraphVM.calcfwd!(gs, Set([rawsummary]))</code></pre><pre><code class="language-none">0</code></pre><p>The data summary results can be retrieved using the <code>GraphVM.getdata</code> method. The data cached in individual graph nodes are all vectors, the last argument <code>1</code> just refers to the first and only element of the data summary. Some nodes will have multiple entries in the data vector.</p><pre><code class="language-julia">RuleDSL.getdata(gs, rawsummary, 1)</code></pre><div class="data-frame"><p>12 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="Type">Type</th></tr></thead><tbody><tr><th>1</th><td>PassengerId</td><td>446.0</td><td>1</td><td>446.0</td><td>891</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>Name</td><td></td><td>Abbing, Mr. Anthony</td><td></td><td>van Melkebeke, Mr. Philemon</td><td>0</td><td>String</td></tr><tr><th>5</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td><td>0</td><td>String7</td></tr><tr><th>6</th><td>Age</td><td>29.6991</td><td>0.42</td><td>28.0</td><td>80.0</td><td>177</td><td>Union{Missing, Float64}</td></tr><tr><th>7</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>Ticket</td><td></td><td>110152</td><td></td><td>WE/P 5735</td><td>0</td><td>String31</td></tr><tr><th>10</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>Cabin</td><td></td><td>A10</td><td></td><td>T</td><td>687</td><td>Union{Missing, String15}</td></tr><tr><th>12</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td><td>2</td><td>Union{Missing, String1}</td></tr></tbody></table></div><p>We observe that some columns in the raw data set have <code>missing</code> values. Data imputation and cleansing is a common task for building ML models. Julius&#39; <code>DataScience</code> library has provided data imputation methods, which can be easily used by calling the <code>fillmissing</code> rule with the desired imputation method for a each field, i.e., we use median value of Age of all passenger for any missing Age, and use the mode value (which is true) for any missing Embarked.</p><pre><code class="language-julia">cleansrc = RuleDSL.@ref ds.fillmissing(
    rawsrc, Dict(:Age =&gt; :median, :Embarked =&gt; :mode); label=&quot;impute missing&quot;
)</code></pre><pre><code class="language-none">ds:fillmissing/impute missing</code></pre><p>After data imputation, we recompute the data summary, showing all the the <code>missing</code> values for both <code>Age</code> and <code>Embarked</code> features have been populated:</p><pre><code class="language-julia">cleansummary = RuleDSL.@ref ds.datasummary(cleansrc; label=&quot;clean summary&quot;)
GraphVM.calcfwd!(gs, Set([cleansummary]))
RuleDSL.getdata(gs, cleansummary, 1)</code></pre><div class="data-frame"><p>12 rows × 7 columns</p><table class="data-frame"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th title="Symbol">Symbol</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Union{Nothing, Float64}">Union…</th><th title="Any">Any</th><th title="Int64">Int64</th><th title="Type">Type</th></tr></thead><tbody><tr><th>1</th><td>PassengerId</td><td>446.0</td><td>1</td><td>446.0</td><td>891</td><td>0</td><td>Int64</td></tr><tr><th>2</th><td>Survived</td><td>0.383838</td><td>0</td><td>0.0</td><td>1</td><td>0</td><td>Int64</td></tr><tr><th>3</th><td>Pclass</td><td>2.30864</td><td>1</td><td>3.0</td><td>3</td><td>0</td><td>Int64</td></tr><tr><th>4</th><td>Name</td><td></td><td>Abbing, Mr. Anthony</td><td></td><td>van Melkebeke, Mr. Philemon</td><td>0</td><td>String</td></tr><tr><th>5</th><td>Sex</td><td></td><td>female</td><td></td><td>male</td><td>0</td><td>String7</td></tr><tr><th>6</th><td>Age</td><td>29.3616</td><td>0.42</td><td>28.0</td><td>80.0</td><td>0</td><td>Float64</td></tr><tr><th>7</th><td>SibSp</td><td>0.523008</td><td>0</td><td>0.0</td><td>8</td><td>0</td><td>Int64</td></tr><tr><th>8</th><td>Parch</td><td>0.381594</td><td>0</td><td>0.0</td><td>6</td><td>0</td><td>Int64</td></tr><tr><th>9</th><td>Ticket</td><td></td><td>110152</td><td></td><td>WE/P 5735</td><td>0</td><td>String31</td></tr><tr><th>10</th><td>Fare</td><td>32.2042</td><td>0.0</td><td>14.4542</td><td>512.329</td><td>0</td><td>Float64</td></tr><tr><th>11</th><td>Cabin</td><td></td><td>A10</td><td></td><td>T</td><td>687</td><td>Union{Missing, String15}</td></tr><tr><th>12</th><td>Embarked</td><td></td><td>C</td><td></td><td>S</td><td>0</td><td>String1</td></tr></tbody></table></div><p>The entire data sourcing and cleansing can be visualized interactively in Julius convenient web UI by clicking the link below.</p><pre><code class="language-julia">svg = GraphIO.postlocalgraph(gss, gs, port; key=&quot;data&quot;);
GraphIO.postsvg(svg, &quot;titanic_1.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7577_data
</code></pre><p align = "center">
<img src="../assets/titanic_1.svg" alt="" title="Data Cleansing"/>
</p>
<p align = "center">
Figure 1 - Data Cleansing
</p><h3 id=".-Experiment-with-multiple-Machine-Learning-models-1"><a class="docs-heading-anchor" href="#.-Experiment-with-multiple-Machine-Learning-models-1">2. Experiment with multiple Machine Learning models</a><a class="docs-heading-anchor-permalink" href="#.-Experiment-with-multiple-Machine-Learning-models-1" title="Permalink"></a></h3><p>Julius Graph Engine can easily interop with existing Python, Java, C++ and R libraries via the generic <code>Atom</code> interface</p><p>For example, the following rules show how easy it is to access Python ML libraries, such as <code>sklearn</code>, by using the <code>PyTrain</code> atom provided in the <code>DataScience</code> package. The first parameter of the <code>PyTrain</code> atom is the full name of the Python ML class to use. The second parameter is a Dictionary with the corresponding parameters/options/arguments of that ML class.</p><pre><code class="language-julia">@addrules ds begin
    classifiertrain(model::Val{:SVC}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.svm.SVC&quot;, options](traindat...)
    classifiertrain(model::Val{:DecisionTree}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.tree.DecisionTreeClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:RandomForest}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.ensemble.RandomForestClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:AdaBoost}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.ensemble.AdaBoostClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:MLPC}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.neural_network.MLPClassifier&quot;, options](traindat...)
    classifiertrain(model::Val{:GaussianNB}, options::Dict, traindat::NodeRef) = PyTrain[&quot;sklearn.naive_bayes.GaussianNB&quot;, options](traindat...)
    classifiertrain(model::Val{:XGBoost}, options::Dict, traindat::NodeRef) = PyTrain[&quot;xgboost.XGBClassifier&quot;, options](traindat...)
    classifiertrain(model::Symbol, options::Dict, traindat::NodeRef; label = &quot;$model-train&quot;) = Alias(classifiertrain(val(model), options, traindat))
end</code></pre><p>We now proceed to train multiple ML models and compare their in-sample and out-sample performance using metrics, such as Gini. The ML models are trained to predict the survival probability of Titanic passengers. We first define the list of models we want to compare and their hyperparameters.</p><pre><code class="language-julia">models = [
    :DecisionTree =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :LogisticRegression =&gt; Dict(:solver =&gt; &quot;saga&quot;, :max_iter =&gt; 200),
    :AdaBoost =&gt; Dict(),
    :XGBoost =&gt; Dict(),
    :GradientBoost =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :RandomForest =&gt; Dict(:min_samples_leaf =&gt; 0.1),
    :GaussianNB =&gt; Dict(),
];</code></pre><p>The target variable name for ML prediction is given below.</p><pre><code class="language-julia">yname = :Survived;</code></pre><p>To divide the input dataset for training and validation, we use the <code>randrowsel</code> rule from the <code>DataScience</code> package. The resulting node <code>valind</code> contains a <code>DataFrame</code> with a validation index column of <code>Vector{Bool}</code> type that randomly indicates wether a row is used for training or validation. The <code>1/3</code> is the percentage of rows that will be reserved for validation.</p><pre><code class="language-julia">valind = RuleDSL.@ref ds.randrowsel(cleansrc, 1 / 3);</code></pre><p>Feature engineering is also supported generically by a rule in <code>DataScience</code> package. A feature set is defined by a 3-element <code>Tuple</code>, including the name of the feature set, a Julia <code>Expr</code> providing the transformations for additional features and lastly, the columns to be dropped  from the feature set. In this particular case, we only drop some columns that should have  no association with a passenger&#39;s suvival probability such as Ticket Id and passenger&#39; name  and Ids. The Cabin is dropped from the feature set because it has too many missing values.</p><pre><code class="language-julia"># definition of feature engineering: (name, transformations, drop columns)
features = (
    :Original,
    quote end,
    [:Cabin, :Ticket, :PassengerId, :Name],
);</code></pre><p>Note that <code>quote end</code> is just an empty <code>Expr</code> that does not create additional features from the original data set.</p><p><code>DataScience.ClassifierSpec</code> is a <code>struct</code> that contains all the information required for training and validating multiple binary classifiers, it includes the key configurations we have defined so far. It is more convenient and readable to pass a single <code>DataScience.ClassifierSpec</code> object than five separate parameters. The <code>DataScience.ClassifierSpec</code> is generic, it can be used for any binary classifier problems and data sets.</p><pre><code class="language-julia">cspec = DataScience.ClassifierSpec(models, cleansrc, yname, valind, features);</code></pre><p>Now we can proceed and use the <code>classifiermetrics</code> rule to compute in-sample and out-of-sample metrics for each model. Internally, this rule calls many other rules for each model and metric, eventually invoking the <code>classifiertrain</code> rules mentioned at the beginning of the section:</p><pre><code class="language-julia">metrics = [:gini, :roc, :accuracyrate, :accuracygraph]
basem = RuleDSL.@ref ds.classifiermetrics(cspec, metrics)
gs2 = GraphVM.createlocalgraph(config, RuleDSL.GenericData())
@time GraphVM.calcfwd!(gs2, Set([basem]));</code></pre><pre><code class="language-none"> 27.997593 seconds (40.23 M allocations: 2.295 GiB, 2.82% gc time, 27.62% compilation time)
</code></pre><p>We can retrieve in-sample and out-sample performance metrics, for example, the GINIs:</p><pre><code class="language-julia">giniref = RuleDSL.@ref ds.classifiermetric(cspec, :gini)
gini = GraphVM.getdata(gs2, hash(giniref), 1)
ginidf = DataFrame(model=gini[:InSample][!, :Model], InSample_GINI=gini[:InSample][!, 2], OutSample_GINI=gini[:OutSample][!, 2])</code></pre><div class="data-frame"><p>7 rows × 3 columns</p><table class="data-frame"><thead><tr><th></th><th>model</th><th>InSample_GINI</th><th>OutSample_GINI</th></tr><tr><th></th><th title="String">String</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>DecisionTree</td><td>0.726202</td><td>0.636718</td></tr><tr><th>2</th><td>LogisticRegression</td><td>0.488286</td><td>0.480253</td></tr><tr><th>3</th><td>AdaBoost</td><td>0.828838</td><td>0.682299</td></tr><tr><th>4</th><td>XGBoost</td><td>0.99447</td><td>0.665693</td></tr><tr><th>5</th><td>GradientBoost</td><td>0.838257</td><td>0.691454</td></tr><tr><th>6</th><td>RandomForest</td><td>0.699895</td><td>0.667981</td></tr><tr><th>7</th><td>GaussianNB</td><td>0.689239</td><td>0.633114</td></tr></tbody></table></div><p>The entire data and logic from can be visualized by clicking on the URL below.</p><pre><code class="language-julia">svg = GraphIO.postlocalgraph(gss, gs2, port; key=&quot;ml&quot;);
GraphIO.postsvg(svg, &quot;titanic_2.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7577_ml
</code></pre><p align = "center">
<img src="../assets/titanic_2.svg" alt="" title="ML"/>
</p>
<p align = "center">
Figure 2 - Machine Learning
</p><h2 id="Hyperparameter-Tuning-1"><a class="docs-heading-anchor" href="#Hyperparameter-Tuning-1">Hyperparameter Tuning</a><a class="docs-heading-anchor-permalink" href="#Hyperparameter-Tuning-1" title="Permalink"></a></h2><p>Our Julius Graph Engine also provides a generic rule <code>hypertune</code> for hyperparameter tuning of any ML model. This illustrates the power of rule composition, where a single hypertune rule can leverage existing rules to search for hyperparmeters of any ML model.</p><p>For example, for a given machine learning model, we can select a range for a set of hyperparameters and easily perform a grid search with respect to a metric:</p><pre><code class="language-julia">ht_1 = RuleDSL.@ref ds.hypertune(cspec, :XGBoost,       Dict(), :gini, :n_estimators =&gt; 50:50:200, :learning_rate    =&gt; .05:.05:.2);
ht_2 = RuleDSL.@ref ds.hypertune(cspec, :AdaBoost,      Dict(), :gini, :n_estimators =&gt; 50:50:200, :learning_rate    =&gt; .05:.05:.2);
ht_3 = RuleDSL.@ref ds.hypertune(cspec, :GradientBoost, Dict(), :gini, :n_estimators =&gt; 50:50:200, :min_samples_leaf =&gt; .05:.05:.2);
ht_4 = RuleDSL.@ref ds.hypertune(cspec, :RandomForest,  Dict(), :gini, :n_estimators =&gt; 50:50:200, :min_samples_leaf =&gt; .05:.05:.2);</code></pre><p>Additional search dimensions can be added to the <code>ds.hypertune</code> rule by appending additioanl pairs of hyperparameter =&gt; searchgrid to the end of rule parameter. We can then wrap the previous nodes in a single node for convenience by means of the <code>alias</code> rule wich uses the <code>Alias</code> atom:</p><pre><code class="language-julia">tunings = RuleDSL.@ref ds.alias([ht_1, ht_2, ht_3, ht_4]; label=&quot;Hyperparameter Tuning&quot;)</code></pre><pre><code class="language-none">ds:alias/Hyperparameter Tuning</code></pre><p>Now proceed with the computation of all the defined hyperparameter tunings:</p><pre><code class="language-julia">gs3 = GraphVM.createlocalgraph(config, RuleDSL.GenericData());
@time GraphVM.calcfwd!(gs3, Set([tunings]));</code></pre><pre><code class="language-none"> 21.444195 seconds (11.79 M allocations: 802.595 MiB, 1.25% gc time, 18.49% compilation time)
</code></pre><p>The following cell shows the resulting insample and outsample GINI from the different hyperparametrs for GradientBoost:</p><pre><code class="language-julia">dat = GraphVM.getdata(gs3, hash(ht_3))
df = deepcopy(dat[1][:, 1:2])
df[!, :InSampleGINI] = dat[1][!, 3]
df[!, :OutSampleGINI] = dat[2][!, 3]
df</code></pre><div class="data-frame"><p>16 rows × 4 columns</p><table class="data-frame"><thead><tr><th></th><th>n_estimators</th><th>min_samples_leaf</th><th>InSampleGINI</th><th>OutSampleGINI</th></tr><tr><th></th><th title="Int64">Int64</th><th title="Float64">Float64</th><th title="Float64">Float64</th><th title="Float64">Float64</th></tr></thead><tbody><tr><th>1</th><td>50</td><td>0.05</td><td>0.830292</td><td>0.708162</td></tr><tr><th>2</th><td>50</td><td>0.1</td><td>0.791017</td><td>0.713298</td></tr><tr><th>3</th><td>50</td><td>0.15</td><td>0.755973</td><td>0.709992</td></tr><tr><th>4</th><td>50</td><td>0.2</td><td>0.705137</td><td>0.729621</td></tr><tr><th>5</th><td>200</td><td>0.05</td><td>0.916393</td><td>0.699364</td></tr><tr><th>6</th><td>200</td><td>0.1</td><td>0.869205</td><td>0.709535</td></tr><tr><th>7</th><td>200</td><td>0.15</td><td>0.838939</td><td>0.714874</td></tr><tr><th>8</th><td>200</td><td>0.2</td><td>0.775377</td><td>0.726367</td></tr><tr><th>9</th><td>100</td><td>0.05</td><td>0.871326</td><td>0.708263</td></tr><tr><th>10</th><td>100</td><td>0.1</td><td>0.831655</td><td>0.707399</td></tr><tr><th>11</th><td>100</td><td>0.15</td><td>0.795702</td><td>0.708772</td></tr><tr><th>12</th><td>100</td><td>0.2</td><td>0.740216</td><td>0.728096</td></tr><tr><th>13</th><td>150</td><td>0.05</td><td>0.900193</td><td>0.699975</td></tr><tr><th>14</th><td>150</td><td>0.1</td><td>0.854054</td><td>0.705975</td></tr><tr><th>15</th><td>150</td><td>0.15</td><td>0.823299</td><td>0.720773</td></tr><tr><th>16</th><td>150</td><td>0.2</td><td>0.761951</td><td>0.724333</td></tr></tbody></table></div><p>Please note that the developer is in charge of selecting the optimal hyperparameter set, as this may involve multiple objectives, the parameter set producing the maximum out-sample gini may not be the best choice. Often, it is better to choose the parameter set with similar in-sample and out-of-sample gini to minimize the chance of overfitting.</p><pre><code class="language-julia">svg = GraphIO.postlocalgraph(gss, gs3, port; key=&quot;hyper&quot;);
GraphIO.postsvg(svg, &quot;titanic_3.svg&quot;)</code></pre><pre><code class="language-none">view graph data at http://127.0.0.1:8080/depgraph.html?dataurl=127.0.0.1:7577_hyper
</code></pre><p align = "center">
<img src="../assets/titanic_3.svg" alt="" title="ML"/>
</p>
<p align = "center">
Figure 2 - Machine Learning
</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="t001_quickstart.html">« 1 Quick Start</a><a class="docs-footer-nextpage" href="t003_mapreduce.html">3 MapReduce »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 28 March 2022 12:31">Monday 28 March 2022</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
