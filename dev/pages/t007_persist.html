<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>7 MLOps: ML Experiment Tracking and Persiting · Julius GraphEngine Tutorials</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/theme.css" rel="stylesheet" type="text/css"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Julius GraphEngine Tutorials</span></div><form class="docs-search" action="../search.html"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../index.html">Introduction</a></li><li><a class="tocitem" href="t001_quickstart.html">1 Quick Start</a></li><li><a class="tocitem" href="t002_titanic.html">2 Machine Learning</a></li><li><a class="tocitem" href="t003_mapreduce.html">3 MapReduce</a></li><li><a class="tocitem" href="t004_bagging.html">4 Distributed Machine Learning Pipeline</a></li><li><a class="tocitem" href="t005_aad.html">5 Adjoint Algorithmic Differentiation (AAD)</a></li><li><a class="tocitem" href="t006_advanced.html">6 Advanced Features</a></li><li class="is-active"><a class="tocitem" href="t007_persist.html">7 MLOps: ML Experiment Tracking and Persiting</a><ul class="internal"><li><a class="tocitem" href="#How-to-use-this-tutorial-1"><span>How to use this tutorial</span></a></li><li><a class="tocitem" href="#Introduction-1"><span>Introduction</span></a></li><li><a class="tocitem" href="#Model-Development-and-Experiment-1"><span>Model Development &amp; Experiment</span></a></li><li><a class="tocitem" href="#Record-a-Model-Experiment-1"><span>Record a Model Experiment</span></a></li><li><a class="tocitem" href="#Reproduce-the-Model-Experiment-1"><span>Reproduce the Model Experiment</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href="t007_persist.html">7 MLOps: ML Experiment Tracking and Persiting</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href="t007_persist.html">7 MLOps: ML Experiment Tracking and Persiting</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliusTechCo/Tutorials/blob/main/src/persist.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Tutorial-7:-MLOps:-ML-Experiment-Tracking-and-Persiting-1"><a class="docs-heading-anchor" href="#Tutorial-7:-MLOps:-ML-Experiment-Tracking-and-Persiting-1">Tutorial 7: MLOps: ML Experiment Tracking and Persiting</a><a class="docs-heading-anchor-permalink" href="#Tutorial-7:-MLOps:-ML-Experiment-Tracking-and-Persiting-1" title="Permalink"></a></h1><h2 id="How-to-use-this-tutorial-1"><a class="docs-heading-anchor" href="#How-to-use-this-tutorial-1">How to use this tutorial</a><a class="docs-heading-anchor-permalink" href="#How-to-use-this-tutorial-1" title="Permalink"></a></h2><ul><li>Select &quot;run all cells&quot; on this notebook from the Run menu in Jupyter notebook or Jupyter</li></ul><p>lab. This step will produce intermediate data output and charts.</p><ul><li>Some cells print out a url, which you can click on and bring up an interactive web UI to visualize the graph data.</li><li>In the unlikely event that the notebook becomes irresponsive, you can try &quot;Restart Kernel&quot; from the Kernel menu, then run individual cells one by one using <code>Shift+Enter</code>.</li><li>Some tutorials use local clusters consisting of multiple processes to mimic the effects</li></ul><p>of graph distribution over a remote clusters. By default, these local clusters   automatically stop after idling for 15min to conserve CPU and memory resources. You will   need to rerun the entire notebook if your local cluster stopped due to inactivity.</p><ul><li>Additional resources (video demos &amp; blogs) are available at http://juliustech.co</li><li>To report any issues, get help or request features, please raise an issue at:</li></ul><p>https://github.com/JuliusTechCo/JuliusGraph/issues</p><h2 id="Introduction-1"><a class="docs-heading-anchor" href="#Introduction-1">Introduction</a><a class="docs-heading-anchor-permalink" href="#Introduction-1" title="Permalink"></a></h2><p>ML model experiment tracking is a common challenge for datas scientists and engineers. Once in a while, we hear the following kind of story:</p><p>”My team spent months to train a massive ML model, we got exellent results at some point. But unfortunately we can&#39;t reproduce it any more, a number of things have changed, including data, underlying python library versions and hyperparameters, we are just not sure what combination might have worked ...&quot;.</p><p>Such story highlights the challenge in ML experment tracking, in order to reproduce a past ML run exactly, three things has to be persisted and recovered,</p><ol><li>runtime environment, including hadware, OS, software libraries etc</li><li>input data</li><li>the entire code, parameters and configurations, to be able to re-build the entire</li></ol><p>data/analytical pipeline</p><p>Experiment tracking becomes much more challenging if the model runs on a distributed environment with many computers.  To guarantee reproducibility, each ML experiment should run from a fresh environment, otherwise the data, setting or environment might change between runs. For example, some data files could be added or modified as part of the runs. However, a complete refrehs of a complex distributed data and analytical pipeline is often out of the question in pracitice, as it consists many software components, parameters and configurations. This is why most existing experiment tracking solutions only persist part of the pipeline that are most relevant for the ML models. The downside of this approach is that the stored ML runs might fail to recover the exact results.</p><p>Leveraging its distributed graph computing engine, Julius offer an experiment tracking solution that can persist and recover an entire distributed data and analytical pipeline, as well as the full runtime data and environment. We belive Julius is the only solution on the market with such capabilities.</p><p>Julius persists model experiment with its entire data &amp; analytical pipeline in the following simple steps:</p><ol><li>spin up a fresh virtual distributed environment, this only takes a few seconds</li><li>run the ML experiment and then record the entire session on the Julius server side, and</li></ol><p>persist the recorded session onto long term storage. The recorded session contains the step by step instruction to recreate the entire runtime environment, including the entire data and distributed pipeline to recover the exact state of the experiment.</p><ol><li>the recorded ML experiement can be easily recovered by replaying it on another</li></ol><p>fresh environment.</p><p>In this notebook, we follow a typical work flow of a data scientist to show Julius&#39; experiment traking capabilities. We use a ML fraud detection model from a previous tutorial as an example. Readers are referred to the &quot;bagging&quot; tutorial for more details on the model itself.</p><h2 id="Model-Development-and-Experiment-1"><a class="docs-heading-anchor" href="#Model-Development-and-Experiment-1">Model Development &amp; Experiment</a><a class="docs-heading-anchor-permalink" href="#Model-Development-and-Experiment-1" title="Permalink"></a></h2><p>Data scientists usually develop ML models by running experiments interactively in a Jupyter notebook. This section shows the definition and pipeline of a distributed ML model.</p><pre><code class="language-julia">using GraphEngine: RuleDSL, GraphVM
using AtomExt
using DataFrames, DataScience, StatsBase, Random

newfunctions = quote
    function downsample(ycol::Symbol, frac::Float64, df::DataFrame)
        positive = DataFrames.filter(row -&gt; isequal(row[ycol], true), df)
        negative =  DataFrames.filter(row -&gt; isequal(row[ycol], false), df)
        dspositive = positive[sample(1:nrow(positive), round(Int, frac * nrow(positive)), replace=false), :]
        dsnegative = negative[sample(1:nrow(negative), round(Int, frac * nrow(negative)), replace=false), :]
        merged = vcat(dspositive, dsnegative)
        merged[shuffle(1:nrow(merged)), :]
    end

    function valcat(xs::Vector...)
        agg = DataFrame()
        for (k, v) in vcat(xs...)
            agg = vcat(agg, v)
        end
        agg
    end

    function dfmean(dfs::DataFrame...)
        df = reduce(.+, dfs)
        df ./ (length(dfs))
    end
end

newrules = quote
    select(ref::RuleDSL.NodeRef, cols::Any; label=&quot;$(isa(cols, InvertedIndex) ? &quot;col != $(cols.skip)&quot; : &quot;col == $(cols)&quot;)&quot;) =
        DataScience.ApplyFn[x::DataFrame-&gt;DataFrames.select(x, cols; copycols=false)](ref...)

    classifiertrain(model::Val{:ExtraTreesClassifier}, options::Dict, trainxs::RuleDSL.NodeRef, trainy::RuleDSL.NodeRef; label=&quot;$model train&quot;)=DataScience.PyTrain[&quot;sklearn.ensemble.ExtraTreesClassifier&quot;, options](trainxs..., trainy...)
    classify(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, testx::RuleDSL.NodeRef; label=&quot;$model inference&quot;)=begin
        train_data_X = RuleDSL.@ref ml.select(train_data, Not(target))
        train_data_y = RuleDSL.@ref ml.select(train_data, target)
        trained = RuleDSL.@ref ml.classifiertrain(model, options,  train_data_X, train_data_y )
        DataScience.PyPredict(trained..., testx...)
    end

    classifyprob(train_data::RuleDSL.NodeRef, target::Symbol, model::Val, options::Dict, test_data::RuleDSL.NodeRef; label=&quot;prob&quot;)=begin
        testx = RuleDSL.@ref ml.select(test_data, Not(target))
        DataScience.ApplyFn[x::DataFrame-&gt;DataFrames.select(x, :proba; copycols=false)](classify(train_data, target, model, options, testx))
    end

    score(realized::RuleDSL.NodeRef, probs::RuleDSL.NodeRef)=DataScience.PyScore(realized..., probs...)

    downsample(raw::RuleDSL.NodeRef, ycol::Symbol, frac::Float64)=DataScience.ApplyFn[Main.downsample, ycol, frac](raw...)

    bagpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) =
        DataScience.ApplyFn[dfmean](RuleDSL.@ref((ml.classifyprob(b, target, model, options, test) for b = train_batches))...)

    batchpred(test::RuleDSL.NodeRef, model::Val, options::Dict, train_batches::Vector{RuleDSL.NodeRef}, target::Symbol) =
        DataScience.ApplyFn[(ind, prob)-&gt;[hash(test) =&gt; hcat(ind, prob)]](select(test, target), bagpred(test, model, options, train_batches, target))
end;</code></pre><p>we use existing rules in ds namespace to read CSV files from a shared drive</p><pre><code class="language-julia">train_data_file = joinpath(@__DIR__, &quot;../data/train_fraud.csv&quot;)
test_data_file = joinpath(@__DIR__, &quot;../data/test_fraud.csv&quot;)
train_data = RuleDSL.@ref ds.csvsrc(train_data_file, true; label=&quot;train data&quot;)
test_data  = RuleDSL.@ref ds.csvsrc(test_data_file, true; label=&quot;test data&quot;)

target = :isFraud
model = Val(:ExtraTreesClassifier)
options = Dict(:n_estimators =&gt; 10, :min_samples_leaf =&gt; 10)

test_data_y = RuleDSL.@ref ml.select(test_data, target)

sampleratio = 0.05
train_ddf = DataScience.DDataFrame(train_data_file, blocksize=&quot;5 MB&quot;)
train_batches = train_ddf.chunks
down_batches = RuleDSL.@ref(ml.downsample(b, target, sampleratio) for b in train_batches)

test_ddf = DataScience.DDataFrame(test_data_file, blocksize=&quot;3.5 MB&quot;)
test_batches = test_ddf.chunks

mapper = RuleDSL.@ref ml.batchpred(model, options, down_batches, target)
shuffler = RuleDSL.@ref mr.shuffler(first, 3)
reducer = RuleDSL.@ref mr.reducer(vcat)</code></pre><pre><code class="language-none">mr:reducer/typeof(vcat):7555</code></pre><p>The model now runs with good results on a cluster for model development, as shown below:</p><pre><code class="language-julia">using GraphEngine: RuleDSL, GraphVM

config = RuleDSL.newconfig(RuleDSL.Config(), :project =&gt; &quot;MapReduce&quot;)
balancer = GraphVM.GlobalUnique()
my_domain = GraphVM.mydomain()
remoteport = GraphVM.drawdataport();</code></pre><pre><code class="language-julia">gs0 = GraphVM.RemoteGraphProxy(my_domain =&gt; 7225)
GraphVM.rpccall(gs0, :startlocalmasterservice, remoteport, 4)
gs = GraphVM.RemoteGraphProxy(config, my_domain =&gt; remoteport, balancer, GraphVM.GenericData())
GraphVM.wait4clusterinit(gs)</code></pre><pre><code class="language-none">Dict{UInt64, Pair{Float64, GraphEngine.GraphVM.WorkerStatus}} with 4 entries:
  0x0f2847bdc16ab03e =&gt; 1.64847e9=&gt;Ready
  0xcfc6b616c2a782f5 =&gt; 1.64847e9=&gt;Ready
  0x312a9f14056ee53a =&gt; 1.64847e9=&gt;Ready
  0x0821bef1b35d62f2 =&gt; 1.64847e9=&gt;Ready</code></pre><pre><code class="language-julia">GraphVM.@remote_eval gs begin
    using GraphEngine: RuleDSL, GraphVM
    using AtomExt, GraphIO
    using DataFrames, DataScience, StatsBase, Random
end

GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));

GraphVM.@remote_eval gs $newfunctions
GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));

GraphVM.@addrules gs ml $newrules

mrpred = RuleDSL.@ref mr.mapreduce(test_batches, mapper, shuffler, reducer, Main.valcat)
mrscore = RuleDSL.@ref ml.score(RuleDSL.@ref(ml.select(mrpred, :isFraud)), RuleDSL.@ref(ml.select(mrpred, :proba)))

alljobs, ds = RuleDSL.jobdeps(config, [mrscore], Set([:classifiertrain, :splitbykey, :reducer]));

GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));
GraphVM.initgraph!(gs)
GraphVM.dispatchjobs!(gs, alljobs, 2);</code></pre><pre><code class="language-julia">using GraphIO

GraphVM.waitcheckstatus(gs, RuleDSL.getconfig(config, :project));
svg = GraphIO.postremotegraph(gs, remoteport);
GraphIO.postsvg(svg, &quot;ml_persist_1.svg&quot;)
GraphVM.rpccall(gs, :endcluster);</code></pre><p align = "center">
<img src="../assets/ml_persist_1.svg" alt="" title="ml persist"/>
</p>
<p align = "center">
Figure 1 - Original Distributed ML Pipeline.
</p><h2 id="Record-a-Model-Experiment-1"><a class="docs-heading-anchor" href="#Record-a-Model-Experiment-1">Record a Model Experiment</a><a class="docs-heading-anchor-permalink" href="#Record-a-Model-Experiment-1" title="Permalink"></a></h2><p>Now the data scientists is happy with the results, and want to persist the experiment so that it can be reproduced later. The data scientits have made a number of choices in this ML model run, including data sources, configurations, choice of model, and hyper parameters etc. Some experiment tracking tools are based on saving notebooks, however that is not an adequate and reliable solution, as some data or variables were not captured by the code in the notebook. For example, the developer might have read data from a local file, or rely upon the state or data of a remote server. Under those circumstances, just saving the notebook is not adequate to recover the ML Run.</p><p>Julius took a different approach, instead of saving the notebook on the client side, we persist the entire state on the server side. This server side persisting process is easy and seamless. We first start a fresh virtual cluster at a new port, using the same docker image used by the development environment.</p><pre><code class="language-julia">remoteport2 = GraphVM.drawdataport()

GraphVM.rpccall(gs0, :startlocalmasterservice, remoteport2, 4)
gs2 = GraphVM.RemoteGraphProxy(config, my_domain =&gt; remoteport2, balancer, GraphVM.GenericData())
GraphVM.wait4clusterinit(gs2)</code></pre><pre><code class="language-none">Dict{UInt64, Pair{Float64, GraphEngine.GraphVM.WorkerStatus}} with 4 entries:
  0xfa09eaae972e0ff8 =&gt; 1.64847e9=&gt;Ready
  0xe7973ecc7248f7ac =&gt; 1.64847e9=&gt;Ready
  0x6be430dbfae2a741 =&gt; 1.64847e9=&gt;Ready
  0x42da11d8a8ce8d6b =&gt; 1.64847e9=&gt;Ready</code></pre><p>The following line enables recording on this new cluster, all the subsequent actions will be recorded on the serverside.</p><pre><code class="language-julia">GraphVM.rpccall(gs2, :clearrecording!)
GraphVM.rpccall(gs2, :setrecording!, true);</code></pre><p>We now re-run the same ML model on this fresh server, existing local variables can be re-used without change to recreate the same data/analytica pipeline on the server. Only a few lines of codes are needed for server side recording, as shown below:</p><pre><code class="language-julia">GraphVM.@remote_eval gs2 begin
    using GraphEngine: RuleDSL, GraphVM
    using AtomExt, GraphIO
    using DataFrames, DataScience, StatsBase, Random
end

GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));

GraphVM.@remote_eval gs2 $newfunctions
GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));

GraphVM.@addrules gs2 ml $newrules

GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));
GraphVM.initgraph!(gs2)
GraphVM.dispatchjobs!(gs2, alljobs, 2);</code></pre><p>Now we can retrieve the recordeing saved on the server side for this ML run, which is an extremely compact representation of the entire run, including all the data and analytical logic to recreate the distributed pipeline. This recording can be persisted on long term storage like AWS S3. The version of docker container being used can also be persisted along with the recording. The docker container captures the exact and complete run time environment.</p><pre><code class="language-julia">GraphVM.waitcheckstatus(gs2, RuleDSL.getconfig(config, :project));
records = GraphVM.rpccall(gs2, :getrecording);

# terminate the recording cluster
GraphVM.rpccall(gs2, :endcluster);</code></pre><h2 id="Reproduce-the-Model-Experiment-1"><a class="docs-heading-anchor" href="#Reproduce-the-Model-Experiment-1">Reproduce the Model Experiment</a><a class="docs-heading-anchor-permalink" href="#Reproduce-the-Model-Experiment-1" title="Permalink"></a></h2><p>From the recording, the entire distributed pipeline can be easily recreated at a later time. To do so, we first spin up a fresh cluster using the same version of docker container, we then replay the recording on this new server. It only take a single line of code to recover the stored pipeline:</p><pre><code class="language-julia">remoteport3 = GraphVM.drawdataport()
GraphVM.rpccall(gs0, :startlocalmasterservice, remoteport3, 4)
gs3 = GraphVM.RemoteGraphProxy(config, my_domain =&gt; remoteport3, balancer, GraphVM.GenericData())
GraphVM.wait4clusterinit(gs3)</code></pre><pre><code class="language-none">Dict{UInt64, Pair{Float64, GraphEngine.GraphVM.WorkerStatus}} with 4 entries:
  0x8ecd021fc0b3ca34 =&gt; 1.64847e9=&gt;Ready
  0xc2c51f6128a445ee =&gt; 1.64847e9=&gt;Ready
  0x1e7096185ffb0a6f =&gt; 1.64847e9=&gt;Ready
  0xa5a331e6ee6039aa =&gt; 1.64847e9=&gt;Ready</code></pre><pre><code class="language-julia">GraphVM.rpccall(gs3, :replayrecording, records)</code></pre><pre><code class="language-none">11</code></pre><p>Now the results are ready to be inspected:</p><pre><code class="language-julia">GraphVM.waitcheckstatus(gs3, RuleDSL.getconfig(config, :project));
svg = GraphIO.postremotegraph(gs3, remoteport3);
GraphVM.rpccall(gs3, :endcluster)
GraphIO.postsvg(svg, &quot;ml_persist_2.svg&quot;)</code></pre><p align = "center">
<img src="../assets/ml_persist_2.svg" alt="" title="ml persist"/>
</p>
<p align = "center">
Figure 2 - Recreate a Distributed ML Pipeline.
</p><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="t006_advanced.html">« 6 Advanced Features</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 28 March 2022 12:31">Monday 28 March 2022</span>. Using Julia version 1.6.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
